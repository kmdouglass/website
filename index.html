<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Optics, programming, and biophysics">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Kyle M. Douglass</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="rss.xml">
<link rel="canonical" href="https://kylemdouglass.com/">
<link rel="next" href="index-1.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/why-is-camera-read-noise-gaussian-distributed/" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark
bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href=".">

            <span id="blog-title">Kyle M. Douglass</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="rss.xml" class="nav-link">RSS feed</a>
                </li>
<li class="nav-item">
<a href="https://kmdouglass.github.io/" class="nav-link">Previous</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/why-is-camera-read-noise-gaussian-distributed/" class="u-url">Why is Camera Read Noise Gaussian Distributed?</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/why-is-camera-read-noise-gaussian-distributed/" rel="bookmark">
            <time class="published dt-published" datetime="2025-06-19T10:40:12+02:00" itemprop="datePublished" title="2025-06-19 10:40">2025-06-19 10:40</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/why-is-camera-read-noise-gaussian-distributed/#disqus_thread" data-disqus-identifier="cache/posts/why-is-camera-read-noise-gaussian-distributed.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>As a microscopist I work with very weak light signals, often just tens of photons per camera pixel. As a result, the images I record are noisy<sup id="fnref:1"><a class="footnote-ref" href="posts/why-is-camera-read-noise-gaussian-distributed/#fn:1">1</a></sup>. To a good approximation, the value of a pixel is a sum of two random variables describing two different physical processes:</p>
<ol>
<li>photon shot noise, which is described by a Poisson probability mass function, and</li>
<li>camera read noise, which is described by a Gaussian probability density function.</li>
</ol>
<p>Read noise has units of electrons, which must be discrete, positive integers. So why is it modeled as a continuous probability density function<sup id="fnref:2"><a class="footnote-ref" href="posts/why-is-camera-read-noise-gaussian-distributed/#fn:2">2</a></sup>?</p>
<h2>The Source(s) of Read Noise</h2>
<p>Janesick<sup id="fnref:3"><a class="footnote-ref" href="posts/why-is-camera-read-noise-gaussian-distributed/#fn:3">3</a></sup> defines read noise as "any noise source that is not a function of signal." This means that there is not necessarily one single source of read noise. It is commonly understood that it comes from somewhere in the camera electronics, but "somewhere" need not imply that it is isolated to one location.</p>
<p>The signal from a camera pixel is the number of photoelectrons that were generated inside the pixel. I imagine readout of this signal as a linear path consisting of many steps. The signal might change form along this path, such as going from number of electrons to a voltage. At each step, there is a small probability that some small error is added to (or maybe also removed from?) the signal. The final result is a value that differs randomly from the original signal.</p>
<p>Importantly, I do not think that it matters which physical process each step actually represents; rather there just has to be many of them for this abstraction to be valid.</p>
<h2>Read Noise is Gaussian because of the Central Limit Theorem</h2>
<p>The reason for my conclusion that we can ignore the details so long as there are many steps is the following:</p>
<p>We can model the error introduced by each step as a random variable. Let's assume that each step is independent of the others. The result of camera readout is a sum of a large number of independent random variables. And of course the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a> states that the distribution of the sum of random variables tends towards a normal distribution, i.e. Gaussian, as the number of random variables tends towards infinity. This happens regardless of the distributions of the underlying random variables.</p>
<p>So read noise can appear to be effectively Gaussian so long as there are many steps along the path of conversion from photoelectrons to pixel values and each step has a chance of introducing an error.</p>
<h3>Sums of Discrete Random Variables</h3>
<p>I encountered one conceptual difficulty here: the sum of discrete random variables is still discrete. If I have several variables that produce only integers, their sum is still an integer. I cannot get, say, 3.14159 as a result. Does the Gaussian approximation, which is for continuous random variables, still apply in this case?</p>
<p>Let's say that I have a discrete random variable that can assume values of 0 or 1, and the probability that the value is 1 is denoted \( p \). This is known as a Bernoulli trial. Now let's say that I have a large number \( n \) of Bernoulli trials. But the sum of \( n \) Bernoulli trials has a distribution that is binomial, and this is well-known to be approximated as a Gaussian when certain conditions are met, including large \( n \)<sup id="fnref:4"><a class="footnote-ref" href="posts/why-is-camera-read-noise-gaussian-distributed/#fn:4">4</a></sup>. So a sum of a large number of discrete random variables can have a probability distribution function that is approximated as a Gaussian. </p>
<p><strong>This does not mean that the sum of discrete random variables can take on continuous values.</strong> Rather, the probability associated with any one output value is approximately given by a Gaussian probability density function sampled at the same value<sup id="fnref:5"><a class="footnote-ref" href="posts/why-is-camera-read-noise-gaussian-distributed/#fn:5">5</a></sup>.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>I wrote a blog post about this a while back: <a href="https://kmdouglass.github.io/posts/modeling-noise-for-image-simulations/">https://kmdouglass.github.io/posts/modeling-noise-for-image-simulations/</a> <a class="footnote-backref" href="posts/why-is-camera-read-noise-gaussian-distributed/#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p>This is often asserted without justification. See for example Janesick, Photon Transfer, page 34. <a class="footnote-backref" href="posts/why-is-camera-read-noise-gaussian-distributed/#fnref:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:3">
<p><a href="https://doi.org/10.1117/3.725073">https://doi.org/10.1117/3.725073</a> <a class="footnote-backref" href="posts/why-is-camera-read-noise-gaussian-distributed/#fnref:3" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:4">
<p><a href="https://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation">https://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation</a> <a class="footnote-backref" href="posts/why-is-camera-read-noise-gaussian-distributed/#fnref:4" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
<li id="fn:5">
<p>Technically, you need to apply what is known as a continuity correction to ensure that the set of possible discrete outcomes sample the Gaussian at the positions that produce the best approximation to the true probabilities. This is like choosing to sample the Gaussian at the midpoints between integers instead of at the discrete integer values themselves because the result is closer to the true value. <a href="https://en.wikipedia.org/wiki/Continuity_correction#Binomial">https://en.wikipedia.org/wiki/Continuity_correction#Binomial</a> <a class="footnote-backref" href="posts/why-is-camera-read-noise-gaussian-distributed/#fnref:5" title="Jump back to footnote 5 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/3d-sequential-optical-system-layouts/" class="u-url">3D Sequential Optical System Layouts</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/3d-sequential-optical-system-layouts/" rel="bookmark">
            <time class="published dt-published" datetime="2025-06-05T11:26:25+02:00" itemprop="datePublished" title="2025-06-05 11:26">2025-06-05 11:26</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/3d-sequential-optical-system-layouts/#disqus_thread" data-disqus-identifier="cache/posts/3d-sequential-optical-system-layouts.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>I am working on a new feature in my <a href="https://github.com/kmdouglass/cherry">ray tracer</a> that will allow users to lay out sequential optical systems in 3D. This is forcing me to think carefully about 3D rigid body transformations in a level of detail that I have never before considered.</p>
<p>In this post I walk through the mathematics for modeling a pair of flat mirrors that are oriented at different angles. Strictly speaking, the layout can be represented more easily in 2D, but I will treat the problem as if it were the more general 3D case. Emphasis will be placed on specifying rotations in an intuitive manner, which will mean rotations about the optical axis, rather than about a fixed axis in a global reference frame.</p>
<h2>The Problem</h2>
<p>The problem that I will consider is depicted as follows:</p>
<p><img alt="A system of two flat mirrors whose optical axis forms the figure Z." src="images/sequential-layout-problem-statement.png"></p>
<p>The system consists of two flat mirrors whose optical axis forms a "figure Z." The normal of the first mirror is at 30 degrees to the axis, and likewise for the second. The optical axis emerges from the second mirror parallel to the first.</p>
<p>The questions are:</p>
<ol>
<li>How do I construct the system without requiring the user to specify the absolute coordinates of the mirror surfaces?</li>
<li>How do I represent the local coordinate reference frames for each mirror surface?</li>
<li>How do I handle transformations between frames?</li>
</ol>
<h3>Ray Tracing Review</h3>
<p>As a quick review, the ray tracing algorithm that I implemented was described by Spencer and Murty<sup id="fnref:1"><a class="footnote-ref" href="posts/3d-sequential-optical-system-layouts/#fn:1">1</a></sup>. It is loosely follows this pseudo-code:</p>
<div class="code"><pre class="code literal-block"><span class="k">for</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">surface</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">system</span>:
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">ray</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">ray</span><span class="w"> </span><span class="nv">bundle</span>:
<span class="w">        </span><span class="mi">1</span>.<span class="w"> </span><span class="nv">transform</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ray</span><span class="w"> </span><span class="nv">coordinates</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">rotating</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">reference</span><span class="w"> </span><span class="nv">frame</span><span class="w"> </span><span class="nv">into</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">local</span><span class="w"> </span><span class="nv">surface</span><span class="w"> </span><span class="nv">frame</span>
<span class="w">        </span><span class="mi">2</span>.<span class="w"> </span><span class="nv">find</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ray</span><span class="o">/</span><span class="nv">surface</span><span class="w"> </span><span class="nv">intersection</span><span class="w"> </span><span class="nv">point</span>
<span class="w">        </span><span class="mi">3</span>.<span class="w"> </span><span class="nv">propagate</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ray</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">intersection</span><span class="w"> </span><span class="nv">point</span>
<span class="w">        </span><span class="mi">4</span>.<span class="w"> </span><span class="nv">perform</span><span class="w"> </span><span class="nv">bounds</span><span class="w"> </span><span class="nv">checking</span><span class="w"> </span><span class="nv">against</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">surface</span>
<span class="w">        </span><span class="mi">5</span>.<span class="w"> </span><span class="nv">redirect</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ray</span><span class="w"> </span><span class="nv">according</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">laws</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">refraction</span><span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">reflection</span>
<span class="w">        </span><span class="mi">6</span>.<span class="w"> </span><span class="nv">transform</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ray</span><span class="w"> </span><span class="nv">coordinates</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">rotating</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">reference</span><span class="w"> </span><span class="nv">frame</span><span class="w"> </span><span class="nv">back</span><span class="w"> </span><span class="nv">into</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">global</span><span class="w"> </span><span class="nv">frame</span>
</pre></div>

<p>Rotations are performed using 3x3 rotation matrices. Ray/surface intersections are found numerically using the Newton-Raphson method, even for spherical surfaces<sup id="fnref:2"><a class="footnote-ref" href="posts/3d-sequential-optical-system-layouts/#fn:2">2</a></sup>. I computed the expressions for the surface sag and normal vectors for conics and flat surfaces by hand and hard-coded them as functions of the intersection point in the local surface reference frame to avoid having to compute them on-the-fly.</p>
<p>Looking at the ray trace algorithm, I see three things that are relevant to this discussion:</p>
<ol>
<li>There are both global and local reference frames</li>
<li>Surfaces are iterated over sequentially</li>
<li>There are rotations, one at the beginning of each loop iteration and one at the end</li>
</ol>
<p>Let's explore each one individually, starting with the global and local reference frames.</p>
<h2>Reference Frames</h2>
<p>I will use only right-handed reference frames where positive rotations are in the counterclockwise direction.</p>
<h3>The Global Reference Frame</h3>
<p>The global reference frame \( \mathbf{G} \) remains fixed. Sometimes it's called the world frame. I denote the coordinate axes of the global frame using \( x \), \( y \), and \( z \).</p>
<p><img alt="The global reference frame." src="images/sequential-layout-global-reference-frame.png"></p>
<p>By convention, I put its origin at the first non-object surface; this would be at the first mirror in the system of two mirrors I described above<sup id="fnref:3"><a class="footnote-ref" href="posts/3d-sequential-optical-system-layouts/#fn:3">3</a></sup>. I also establish the convention that the optical axis between the object and the first surface is parallel to the global z-axis.</p>
<p>The global frame is important because the orthonormal vectors defining the local and cursor frames (to be explained later) are expressed relative to it.</p>
<h3>Local Reference Frames</h3>
<p>Each surface \( i \) has a local reference frame \( \mathbf{L}_i \) whose origin lies at the vertex of the surface. Its coordinate axes are denoted \( x_i^{\prime} \), \( y_i^{\prime} \), and \( z_i^{\prime} \). For flat surfaces, I set the \( z_i^{\prime} \) axis perpendicular to the surface.</p>
<p><img alt="The local reference frames of the two mirrors." src="images/sequential-layout-local-reference-frames.png"></p>
<p>Notice that the \( x^{\prime} \) axes flip directions when going from mirror 1 to mirror 2. This is done to preserve the right-handedness of the reference frames. More about this will be explained in the next section.</p>
<h2>Sequential System Models</h2>
<p>Ray tracing programs for optical design are often divided into two categories: sequential and nonsequential. In sequential ray tracers, rays are traced from one surface to another in the sequence for which they are defined. This means that a ray could pass right through a surface if it is not the next surface in the model sequence.</p>
<p>Nonsequential ray tracers do not take account of the order in which surfaces are defined. Rays are fired into the world and the intersect whatever the closest object is on their path. Illumination optics often use nonsequential ray tracing, as do rendering engines for cinema.</p>
<p>My ray tracer is a sequential ray tracer because sequential ray tracing is easier to implement and can be applied to nearly all the use cases that I encounter in the lab.</p>
<h3>3D Layouts of Sequential Surfaces</h3>
<p>One possibility to layout sequential surfaces in 3D is to specify the coordinates and orientations of each surface relative to the global frame. This is how one adds surfaces in 3D in the open source Python library <a href="https://github.com/HarrisonKramer/optiland">Optiland</a>, for example. In practice, I found that I need to have a piece of paper by my side to work out the positions of each surface independently. This option provides maximum flexibility in surface placement.</p>
<p>The other possibility that I considered is to leverage the fact that the surfaces are an ordered sequence, and position them in 3D space along the optical axis. The axis can reflect from reflecting surfaces using the law of reflection. Furthermore, any tilt or decenter could be specified relative to this axis. I ultimately chose this solution because I felt that it better matches my mental model of sequential optical systems. It also seems to follow more closely what I do in the lab when I build a system, i.e. add components along an axis that bends through 3D space.</p>
<h3>The Cursor</h3>
<p>I created the idea of the cursor to position sequential surfaces in 3D space. A cursor has a 3D position, \( \vec{ t } \left( s \right) \) that is parameterized over the track length \( s \). \( s \) is negative for the object surface, \( s = 0 \) at the first non-object surface, and achieves its greatest value at the final image plane.</p>
<p>In addition, the cursor has a reference frame attached to it that I denote \( \mathbf{C} \left( s \right) \). The axes of the cursor frame are \( r \), \( u \)  and \( f \), which stand for right, up, and forward, respectively. This nearly matches the <a href="https://dev.epicgames.com/documentation/en-us/uefn/forwardrightup-coordinate-system-in-unreal-editor-for-fortnite">FRU</a> coordinate system in game engines such as Unreal, except I take the forward direction to represent the optical axis because I would say that this convention is universal in optical design.</p>
<p><img alt="The cursor frames at three different positions along the optical axis." src="images/sequential-layout-cursor-frames.png"></p>
<p>Above I show the cursor frame at three different positions along the optical axis \( s_1 &lt; 0 &lt; s_2 &lt; s_3 \). Refracting surfaces will not change the orientation of the cursor frame, but reflecting surfaces will.</p>
<p>Finally, when \( s \) is exactly equal to a reflecting surface position, I take the orientation of the cursor frame to be the one <strong>before</strong> reflection. An infinitesimal distance later, the frame reorients by reflecting about the surface normal at the vertex of the surface in its local frame.</p>
<h4>Convention for Maintaining Right Handedness upon Reflection</h4>
<p>There is an ambiguity that arises in the cursor frame upon reflection that is best illustrated in the example below:</p>
<p><img alt="Ambiguity in the cursor frame upon reflection." src="images/sequential-layout-ambiguity.png"></p>
<p>In panel a, the cursor is incident upon a mirror with its frame's forward direction antiparallel to the mirror's normal vector. There are two equally valid choices when defining the cursor frame after reflection. In panel b, the cursor frame is rotated about the up direction, whereas in panel c it is rotated about the right direction. This means that there is no fundamentally correct way to position the cursor frame after reflection. We must choose a convention and stick with it.</p>
<p>Reflections of the cursor frame are handled in two steps:</p>
<ol>
<li>Reflect the frame</li>
<li>Adjust the results to maintain right handedness and address the ambiguity illustrated above</li>
</ol>
<p>The vector law of reflection is used to compute the new \( \hat{ r } \), \( \hat{ u } \), and \( \hat{ f } \) unit vectors for any general angle of incidence of the cursor frame upon a reflecting surface:</p>
<p>$$\begin{equation}
\hat{ f }^{\prime} = \hat{ f } - 2 \left( \hat{ f } \cdot \hat{ n } \right) \hat{ n }
\end{equation}$$</p>
<p>where \( \hat{ n } \) is the surface's unit normal vector. The same applies for the right and up unit vectors.</p>
<p>After reflection, I perform a check for right handedness. By convention, I maintain the direction of the up unit vector because many optical systems are laid out in 2D and their elements are rotated about this direction. This convention means that the right unit vector must be flipped:</p>
<div class="code"><pre class="code literal-block"><span class="k">if</span><span class="w"> </span><span class="nv">cross</span><span class="ss">(</span><span class="nv">right</span>,<span class="w"> </span><span class="nv">up</span><span class="ss">)</span><span class="w"> </span>·<span class="w"> </span><span class="nv">forward</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">0</span>:
<span class="w">    </span><span class="nv">right</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="nv">right</span>
</pre></div>

<p>The cross product between the right and up directions must point in the forward direction if the system is right handed. "Pointing in the forward direction" means that the dot product of the result with the forward unit vector must be greater than zero. The conditonal in the pseudocode above checks whether this is not indeed the case and flips the right unit vector if necessary.</p>
<h2>Transformations between Reference Frames</h2>
<p>There are two different transformations required by the ray trace algorithm:</p>
<ol>
<li>From the global frame to a surface local frame</li>
<li>From a surface local frame to the global frame</li>
</ol>
<p>Because the system is laid out relative to the cursor frame, I need to chain together two rotations, one from the global to the cursor frame, and one from the cursor to the local frame.</p>
<h3>Example</h3>
<p>Let's say that the second mirror has a diameter of 25.4 mm, and that the mirrors are separated by \( \| \vec{ t } \left( s_2 \right) \| = 100 \, mm \). I want to find the transformation from the global frame coordinates at a point on the bottom edge of the mirror to the local frame coordinates, which is \( y_2^{\prime} = -12.7 \, mm \). The image below illustrates the geometry that will be used for this example.</p>
<p><img alt="Example geometry for transforming from the global frame to the local frame via the cursor frame." src="images/sequential-layout-example-geometry.png"></p>
<p>From relatively straightforward trigonmetry<sup id="fnref:4"><a class="footnote-ref" href="posts/3d-sequential-optical-system-layouts/#fn:4">4</a></sup> we get the global frame coordinates of both \( {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \) and the point we are trying to find, \( {}^{\mathbf{G}}\vec{ p } \). (Vectors preceded by superscripts with reference frame names indicate the coordinate system they are being referred to.)</p>
<p>$$\begin{eqnarray}
 {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) = \left(
  \begin{array}{c}
    0 \\
    50 \sqrt{ 3 } \\
    -50
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>$$\begin{eqnarray}
 {}^{\mathbf{G}}\vec{ p } = \left(
  \begin{array}{c}
    0 \\
    43.65 \sqrt{ 3 } \\
    -56.35
  \end{array}
\right)
\end{eqnarray}$$</p>
<h4>Step 1: Translate from the Global Origin to the Cursor Frame</h4>
<p>The first step in computing \( {}^{\mathbf{ C }} \vec{ p } \) is to translate from the origin of the global frame to the position of cursor.</p>
<p>$$ {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right)  =  (0, -6.35 \sqrt{3}, -6.35)^{ \mathrm{ T }}$$</p>
<h4>Step 2: Rotate into the Cursor Frame</h4>
<p>A rotation from the global frame into the cursor frame can be achieved by taking the \( \hat{ r } \), \( \hat{ u } \), and \( \hat{ f } \) unit vectors that define the cursor frame in the global coordinate system and making them the columns of a \( 3 \times 3 \) rotation matrix. At the second mirror, this matrix is:</p>
<p>$$\begin{eqnarray}
R_{GC} \left( \theta = 30^{ \circ } \right) = \left(
  \begin{array}{ccc}
    -1 &amp; 0 &amp; 0 \\
    0 &amp; 1 / 2 &amp; \sqrt{ 3 } / 2 \\
    0 &amp; \sqrt{ 3 } / 2 &amp; -1 / 2
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>If this is not clear, consider that the columns of a rotation matrix represent the basis vectors of the coordinate system after rotation, but expressed in the original (global) frame's coordinate system. Also, from the diagram above, \( \hat{ u } \left( s_2 \right) = ( 0, 1 / 2, \sqrt{ 3 } / 2)^{ \mathrm{ T }} \) and \( \hat{ f } \left( s_2 \right) = ( 0, \sqrt{ 3 } / 2, - 1 / 2)^{ \mathrm{ T }}\), which are the second and third columns of the matrix.</p>
<p>The rotation into the cursor frame is the product between the rotation matrix and the difference \( {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \):</p>
<p>$$\begin{eqnarray}
{}^{\mathbf{ C }} \vec{ p } = R_{GC} \left[ {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \right] = \left(
  \begin{array}{ccc}
    -1 &amp; 0 &amp; 0 \\
    0 &amp; 1 / 2 &amp; \sqrt{ 3 } / 2 \\
    0 &amp; \sqrt{ 3 } / 2 &amp; -1 / 2
  \end{array}
\right) \left(
  \begin{array}{c}
    0 \\
    -6.35 \sqrt{ 3 } \\
    -6.35
  \end{array}
\right) = \left(
  \begin{array}{c}
    0 \\
    -6.35 \sqrt{ 3 } \\
    -6.35
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>At first, I thought I had made a mistake when I did this calculation because the vector is unchanged after rotation. However, as illustrated below, you can see that the relative lengths of the projections of \( {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \) onto the \( u \) and \( f \) axes make sense.</p>
<p><img alt="A simplified schematic showing the projection of difference vector onto the -u and -f axes." src="images/sequential-layout-example-global-to-cursor.png"></p>
<p>As it turns out, I inadvertently chose an eigenvector of the rotation matrix as an example; any general point will in fact change its coordinates when moving from the global to the cursor frame. For example, if we try to rotate a vector that is antiparallel to the global z-axis, i.e. \( {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) = ( 0, 0, -1 )^{ \mathrm{ T } }\), then it will become</p>
<p>$$\begin{eqnarray}
R_{GC} \left[ {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \right] = \left(
  \begin{array}{ccc}
    -1 &amp; 0 &amp; 0 \\
    0 &amp; 1 / 2 &amp; \sqrt{ 3 } / 2 \\
    0 &amp; \sqrt{ 3 } / 2 &amp; -1 / 2
  \end{array}
\right) \left(
  \begin{array}{c}
    0 \\
    0 \\
    -1
  \end{array}
\right) = \left(
  \begin{array}{c}
    0 \\
    -0.8660 \\
    -0.5
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>in the cursor frame.</p>
<h4>Step 3: Rotate into the Surface Local Frame</h4>
<p>For the final step, I need to compose a rotation matrix from a sequence of three rotations. To do this well, I need to be very clear about what types of rotations I am performing and their sequence.</p>
<h5>Active vs. Passive Rotations</h5>
<p>The difference between active and passive rotations are illustrated below for a 45 degree rotation about the right axis.</p>
<p><img alt="Active vs. passive rotations" src="images/sequential-layout-active-vs-passive.png"></p>
<p>Active rotations specify the rotation of a point relative to a fixed reference frame; passive rotations specify the rotation of a reference frame, keeping the point fixed. And pay attention here: the right axis points into the screen, so a positive rotation would be clockwise when viewed from the perspective drawn above. </p>
<p>What are the corresponding rotation matrices? Here, I found that the internet is absolutely littered with wrong answers, including on sites like Wikipedia. I even get different answers from LLMs depending on when I ask. Therefore, I am including them here as a gift to my future self.</p>
<p>The <strong>active</strong> rotation matrices about the x (right), y (up), and z (forward) axes are:</p>
<p>$$\begin{eqnarray}
R_x \left( \theta \right) = \left(
  \begin{array}{ccc}
    1 &amp; 0 &amp; 0 \\
    0 &amp; \cos \theta &amp; - \sin \theta \\
    0 &amp; \sin \theta &amp; \cos \theta
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>$$\begin{eqnarray}
R_y \left( \psi \right) = \left(
  \begin{array}{ccc}
    \cos \psi &amp; 0 &amp; \sin \psi \\
    0 &amp; 1 &amp; 0 \\
    - \sin \psi &amp; 0 &amp; \cos \psi
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>$$\begin{eqnarray}
R_z \left( \phi \right) = \left(
  \begin{array}{ccc}
    \cos \phi &amp; - \sin \phi &amp; 0 \\
    \sin \phi &amp; \cos \phi &amp; 0 \\
    0 &amp; 0 &amp; 1
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>The <strong>passive</strong> rotation matrices about the x (right), y (up), and z (forward) axes are:</p>
<p>$$\begin{eqnarray}
R_x \left( \theta \right) = \left(
  \begin{array}{ccc}
    1 &amp; 0 &amp; 0 \\
    0 &amp; \cos \theta &amp; \sin \theta \\
    0 &amp; - \sin \theta &amp; \cos \theta
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>$$\begin{eqnarray}
R_y \left( \psi \right) = \left(
  \begin{array}{ccc}
    \cos \psi &amp; 0 &amp; - \sin \psi \\
    0 &amp; 1 &amp; 0 \\
    \sin \psi &amp; 0 &amp; \cos \psi
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>$$\begin{eqnarray}
R_z \left( \phi \right) = \left(
  \begin{array}{ccc}
    \cos \phi &amp; \sin \phi &amp; 0 \\
    - \sin \phi &amp; \cos \phi &amp; 0 \\
    0 &amp; 0 &amp; 1
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>Notice that all that changes between these two types of rotations is the location of a negative sign on the \( \sin \) terms.</p>
<p>I found that a useful way to remember whether a matrix represents an active or passive rotation is as follows. Take for example the +45 degree rotation of the vector \( ( 0, 0, 1 )^{ \mathrm{ T } } \) about the right direction illustrated above. You can see that an active rotation should result in a negative \( u \) and a positive \( f \) component. This means<sup id="fnref:6"><a class="footnote-ref" href="posts/3d-sequential-optical-system-layouts/#fn:6">6</a></sup>:</p>
<p>$$\begin{eqnarray}
\left(
  \begin{array}{ccc}
    1 &amp; 0 &amp; 0 \\
    0 &amp; 1 / \sqrt{ 2 } &amp; - 1 / \sqrt{ 2 } \\
    0 &amp; 1 / \sqrt{ 2 } &amp; 1 / \sqrt{ 2 }
  \end{array}
\right) \left(
  \begin{array}{c}
    0 \\
    0 \\
    1
  \end{array}
\right) = \left(
  \begin{array}{c}
    0 \\
    - 1 / \sqrt{ 2 } \\
    1 / \sqrt{ 2 }
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>The passive rotation should result in positive values for both the \( u^{ \prime } \) and \( f^{ \prime } \) components:</p>
<p>$$\begin{eqnarray}
\left(
  \begin{array}{ccc}
    1 &amp; 0 &amp; 0 \\
    0 &amp; 1 / \sqrt{ 2 } &amp; 1 / \sqrt{ 2 } \\
    0 &amp; - 1 / \sqrt{ 2 } &amp; 1 / \sqrt{ 2 }
  \end{array}
\right) \left(
  \begin{array}{c}
    0 \\
    0 \\
    1
  \end{array}
\right) = \left(
  \begin{array}{c}
    0 \\
    1 / \sqrt{ 2 } \\
    1 / \sqrt{ 2 }
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>I can do a similar check for the other directions to verify the other matrices.</p>
<h5>Extrinsic vs. Intrinsic Rotations</h5>
<p>I found these easier to understand than active and passive rotations. Extrinsic rotations are rotations that are always about a fixed global reference frame. On the other hand, intrinsic rotations are about the intermediate frames that result from a single rotation. So if I rotate about the \( f \) axis, then the \( r \) and \( u \) axes will be rotated, resulting in an intermediate \( r^{ \prime }u^{ \prime }f^{ \prime } \) frame. The next rotation will be about one of these intermediate axes.</p>
<p>The confusing thing about these two types of rotations is the order in which the rotation matrices are applied to a vector. An <strong>extrinsic</strong> rotation of a vector \( \vec{ v } \) about \( r \), then \( u \), then \( f \) is written as:</p>
<p>$$ R_f R_u R_r \vec{ v} $$</p>
<p>which follows the usual commutativity rules of matrix multiplication. An <strong>intrinsic</strong> rotation of a vector \( \vec{ v } \) about \( r \), then \( u^{ \prime } \), then \( f^{ \prime \prime } \), on the other hand is written as:</p>
<p>$$ R_r R_u R_f \vec{ v} $$</p>
<p>So even though the rotation about the right direction is performed first, we multiply the vector first by the rotation matrix about the \( f \) direction in the second intermediate frame.</p>
<p>All of this might seem confusing and lead one to wonder why they would want to use intrinsic rotations, but actually they are much more intuitive than extrinsic rotations and make a lot of sense when laying out an optical system. For example, if I have a two-axis mirror mount and I rotate the mirror about the vertical axis, a horizontal rotation that follows will be about the axis in the newly rotated frame, not the global laboratory frame. In any case, a sequence of three extrinsic rotations and three intrinsic rotations through the same angles will produce the same result so long as the order of the rotation matrices is correct.</p>
<h5>Euler Angles and Rotation Sequences</h5>
<p>The most important thing I learned about Euler angles is that they are completely meaningless unless you also specify a rotation sequence. Additionally, the internet is full of resources about the distinction between proper and improper Euler angles. The gist of what I learned here is that proper Euler angles are really a distraction to scientists and engineers because they rely on rotation sequences in which one of the axes is used twice. More useful are what aerospace engineers sometimes refer to as the Tait-Bryan angles, which are the rotation angles associated with sequences like \( z-y^{ \prime }-x^{ \prime \prime } \) or \( x-y-z \).</p>
<p>Now, there is one point here that is worth making and that is relevant to optical system layout: <strong>rotations about \( f \), the forward direction, into the local frame are best performed last in the sequence.</strong> To understand why, consider a cylindrical lens with an axis parallel to the local \( z' \) direction. If we perform an intrinsic rotation about the cursor's \( f \) direction first and then try to adjust its tip or tilt, we will be doing so about axes that are rotated such that its tip and tilt become coupled with respect to the global frame. When aligning such systems, no one expects that rotation of a cylindrical lens about its axis will change the way that the tip and tilt adjustors on a lens mount work.</p>
<p>For all these reasons, I choose an intrinsic sequence \(r - u^{ \prime } - f^{{ \prime \prime} } \) of passive rotations with Euler angles \( \theta \), \( \psi \), and \( \phi \), respectively. The corresponding rotation matrix is:</p>
<p>$$\begin{eqnarray}
R_{ \mathbf{ CL } } ( \theta, \psi, \phi ) = R_r ( \theta ) R_u ( \psi ) R_f ( \phi ) = \left(
  \begin{array}{ccc}
    \cos \phi \cos \psi &amp; \sin \phi \cos \psi &amp; - \sin \psi \\
    - \sin \phi \cos \theta + \sin \psi \sin \theta \cos \phi &amp; \sin \phi \sin \psi \sin \theta + \cos \phi \cos \theta &amp; \sin \theta \cos \psi \\
    \sin \phi \sin \theta + \sin \psi \cos \phi \cos \theta &amp; \sin \phi \sin \psi \cos \theta - \sin \theta \cos \phi &amp; \cos \psi \cos \theta 
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>And finally, the transformation of the point on the mirror from the global to the surface local frame is:</p>
<p>$$ {}^{ \mathbf{ L }}\vec{p} = R_{ CL }R_{ GC } \left[ {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \right] $$</p>
<h5>The Solution to the Example</h5>
<p>Does this give the correct result in the above example? Well, the mirror is rotated +30 degrees about the right direction, so the cursor-to-local rotation matrix is:</p>
<p>$$\begin{eqnarray}
R_{CL} \left( \theta  = 30^{ \circ },  \psi = 0, \phi = 0 \right) = \left(
  \begin{array}{ccc}
    1 &amp; 0 &amp; 0 \\
    0 &amp; \sqrt{ 3 } / 2 &amp; 1 / 2 \\
    0 &amp; - 1 / 2 &amp; \sqrt{ 3 } / 2
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>From earlier, the vector representing the point in the cursor frame is \( ( 0, -6.35 \sqrt{ 3 }, -6.35 )^{ \mathrm{ T } } \). Their product gives the final answer:</p>
<p>$$\begin{eqnarray}
{}^{ \mathbf{ L } }\vec{ p } = \left(
  \begin{array}{ccc}
    1 &amp; 0 &amp; 0 \\
    0 &amp; \sqrt{ 3 } / 2 &amp; 1 / 2 \\
    0 &amp; - 1 / 2 &amp; \sqrt{ 3 } / 2
  \end{array}
\right) \left(
  \begin{array}{c}
    0 \\
    -6.35 \sqrt{ 3 } \\
    -6.35
  \end{array}
\right) = \left(
\begin{array}{c}
    0 \\
    -12.7 \\
    0
  \end{array}
\right)
\end{eqnarray}$$</p>
<p>This is exactly as expected, as we wanted to get a point on the bottom of the 25.4 mm diameter mirror in its local frame.</p>
<h4>Step 4: Rotate back from the Surface Local to the Global Frame</h4>
<p>I need to go back to the global frame at the end of each iteration for a ray trace. Fortunately, it's easy to undo a rotation because the inverse of a rotation matrix is just its transpose. I also need to swap the order of the matrices when taking the inverse, and add back the offset from the origin of the global system. This means:</p>
<p>$${}^{\mathbf{ G } }\vec{ p } = R_{GC}^{ \mathrm{ T} } R_{CL}^{ \mathrm{ T} } {}^{\mathbf{ L } }\vec{ p } + {}^{\mathbf{ G }} \vec{ t } (s_2) $$</p>
<p>I plugged in the numbers in Python and verified that I get the original point back.</p>
<p>This should be all I need to know to implement 3D sequential optical system layouts in my ray tracer.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>G. H. Spencer and M. V. R. K. Murty, "General Ray-Tracing Procedure," J. Opt. Soc. Am. 52, 672-678 (1962). <a href="https://doi.org/10.1364/JOSA.52.000672">https://doi.org/10.1364/JOSA.52.000672</a>. <a class="footnote-backref" href="posts/3d-sequential-optical-system-layouts/#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p>Ray/surface intersections with spherical surfaces can be found analytically using the quadratic equation with only minor caveats considering stability issues due to floating point arithmetic. This would likely be faster than using Newton-Raphson. However, a general system contains both spherical and non-spherical surfaces, and I was concerned that checking each surface type would result in a performance hit due to branch prediction failures by the processor. I could probably have found a way around this by deciding ahead of time which algorithm to use to determine the intersection for each surface before entering the main ray tracing loop, but during initial development I decided to just use Newton-Raphson for everything because doing so resulted in very simple code. (Thanks to Andy York for telling me about the numerical instabilities when using the quadratic equation. See Chapter 7 here: <a href="https://www.realtimerendering.com/raytracinggems/rtg/index.html">https://www.realtimerendering.com/raytracinggems/rtg/index.html</a>.) <a class="footnote-backref" href="posts/3d-sequential-optical-system-layouts/#fnref:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:3">
<p>The object plane is the flat surface perpendicular to the optical axis in which the object lies. It is always at surface index 0 in my convention. <a class="footnote-backref" href="posts/3d-sequential-optical-system-layouts/#fnref:3" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:4">
<p>Since the only angles involved are \( 30^{ \circ} \) and \( 60^{ \circ } \), I used a 30-60-90 triangle of lengths 1, \( \sqrt{ 3 } \), and 2, respectively to compute the cosines and sines. <a class="footnote-backref" href="posts/3d-sequential-optical-system-layouts/#fnref:4" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
<li id="fn:5">
<p>Passive rotations result in a rotation of the coordinate axes, keeping a point fixed; active rotations rotate a point about a set of axes. <a class="footnote-backref" href="posts/3d-sequential-optical-system-layouts/#fnref:5" title="Jump back to footnote 5 in the text">↩</a></p>
</li>
<li id="fn:6">
<p>The cosine and sine of 45 degrees are both \( 1 / \sqrt{ 2 } \). <a class="footnote-backref" href="posts/3d-sequential-optical-system-layouts/#fnref:6" title="Jump back to footnote 6 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/a-very-brief-summary-of-the-analytic-signal-in-fourier-optics/" class="u-url">A Very Brief Summary of The Analytic Signal in Fourier Optics</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/a-very-brief-summary-of-the-analytic-signal-in-fourier-optics/" rel="bookmark">
            <time class="published dt-published" datetime="2025-04-01T15:53:21+02:00" itemprop="datePublished" title="2025-04-01 15:53">2025-04-01 15:53</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/a-very-brief-summary-of-the-analytic-signal-in-fourier-optics/#disqus_thread" data-disqus-identifier="cache/posts/a-very-brief-summary-of-the-analytic-signal-in-fourier-optics.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <h2>The Analytic Signal Representation of a Monochromatic Wave</h2>
<h3>Monochromatic Scalar Waves</h3>
<p>A monochromatic, scalar waveform is described by the expression:</p>
<p>$$ u \left( \mathbf{r}, t\right) = A ( \mathbf{r} ) \cos \left[2 \pi f_0 t + \phi \left( \mathbf{r} \right) \right] $$</p>
<ul>
<li>The signal is real-valued</li>
<li>The signal has a known phase for all \( t \)</li>
</ul>
<h3>The Analytic Signal</h3>
<p>An analytic signal is a generalization of a phasor. It is used to represent a real-valued signal as a complex exponential or a sum of complex exponentials. When Goodman<sup id="fnref:1"><a class="footnote-ref" href="posts/a-very-brief-summary-of-the-analytic-signal-in-fourier-optics/#fn:1">1</a></sup> refers to a phasor, he often means the analytic signal. This is made clear in Chapter 6 where he describes the construction of the phasor for a narrowband signal as follows:</p>
<ol>
<li>compute its Fourier transform</li>
<li>set the positive frequency components to zero</li>
<li>double the amplitudes of the negative frequency components</li>
<li>inverse Fourier transform the resulting one-sided spectrum</li>
</ol>
<p>Strictly speaking, the analytic signal is obtained by setting the negative frequencies to zero and doubling by application of the Hilbert transform. However, many engineering fields have adopted the convention of setting the positive frequencies to zero instead. The results will be the same, except that the direction of power flow will be reversed (if I recall correctly).</p>
<p>The Fourier transform of \( u \left( \mathbf{r}, t\right) \) is:</p>
<p>$$ \mathcal{F} \left\{ u \left( \mathbf{r}, t\right) \right\} = \frac{A ( \mathbf{r} ) }{2} \left[ e^{j \phi \left( \mathbf{r} \right) } \delta \left( f - f_0 \right) + e^{-j \phi \left( \mathbf{r} \right) } \delta \left( f + f_0 \right) \right] $$</p>
<p>Drop the positive frequency term \( e^{j \phi \left( \mathbf{r} \right) } \delta \left( f - f_0 \right) \) and double the result. This produces:</p>
<p>$$ A ( \mathbf{r} ) e^{-j \phi \left( \mathbf{r} \right) } \delta \left( f + f_0 \right) $$</p>
<p>Let \( U ( \mathbf{r} ) := A ( \mathbf{r} ) e^{-j \phi \left( \mathbf{r} \right) } \). The inverse Fourier transform of this signal is:</p>
<p>$$ \mathcal{F}^{-1} \left\{ U ( \mathbf{r} ) \delta \left( f + f_0 \right) \right\} = U ( \mathbf{r} ) e^{-j 2 \pi f_0 t } $$</p>
<p>We can recover the original field by taking the real part of this expression, which is equivalent to applying Euler's identity and dropping the imaginary part:</p>
<p>$$ u \left( \mathbf{r}, t \right) = \Re \left[ U ( \mathbf{r} ) e^{-j 2 \pi f t} \right] $$</p>
<h2>Polychromatic Scalar Waves</h2>
<p>To model a polychromatic wave, we integrate over the analytic signals of each spectral component and take the real part of the result:</p>
<p>$$ u \left( \mathbf{r}, t\right) = \Re \left[ \int_{-\infty}^{\infty} \tilde{U} \left( \mathbf{r}, f \right) e^{-j 2 \pi f t} \,df \right] $$</p>
<h3>The Narrowband Assumption</h3>
<p>We get a useful representation to the expression above if we assume that the bandwidth of the signal is much smaller than its center frequency \( \Delta f \ll f_0 \):</p>
<p>$$ \int_{-\infty}^{\infty} \tilde{U} \left( \mathbf{r}, f \right) e^{-j 2 \pi f t} \,df = U \left( \mathbf{r}, t \right) e^{-j 2 \pi f_0 t} $$</p>
<p>To better understand the meaning of this assumption, make the substitution \( \nu = f - f_0 \) into the expression on the left hand side:</p>
<p>$$\begin{eqnarray}
\int_{-\infty}^{\infty} \tilde{U} \left( \mathbf{r}, f \right) e^{-j 2 \pi f t} \,df &amp;=&amp; \int_{-\infty}^{\infty} \tilde{U} \left( \mathbf{r}, \nu + f0 \right) e^{-j 2 \pi \left( \nu + f_0 \right) t} \,d\nu \\
&amp;=&amp; e^{-j 2 \pi f_0 t} \int_{-\infty}^{\infty} \tilde{U} \left( \mathbf{r}, \nu + f0 \right) e^{-j 2 \pi \nu t} \,d\nu
\end{eqnarray}$$</p>
<p>Under the narrowband assumption, the integration in the expression above is constrained to small values around \( \nu = 0 \) that are much less than the phasor term that is oscillating at frequency \( f_0 \). If we define the following function:</p>
<p>$$ U \left( \mathbf{r}, t \right) := \int_{-\infty}^{\infty} \tilde{U} \left( \mathbf{r}, \nu + f0 \right) e^{-j 2 \pi \nu t} \,d\nu $$ </p>
<p>then it will vary slowly with respect to the carrier frequency \( f_0 \).</p>
<p>As a result, under the assumptions of narrowbandedness, we can interpret the complex function \( U \left( \mathbf{r}, t \right) \) as an "envelope" modulating the amplitude of the fast oscillating carrier wave. If the assumption is not valid, then this interpretation fails.</p>
<h3>The Slowly Varying Envelope Assumption</h3>
<p>It is instructive to reverse our reasoning and see why a slowly-varying envelope implies a narrowband signal. Compute the Fourier transforms of the narrowband waveform, along with the Fourier transform of the derivative of \( U \left( \mathbf{r}, t \right) \).</p>
<p>The Fourier transform of the analytic signal:</p>
<p>$$ \int_{-\infty}^{\infty} \left[ U \left( \mathbf{r}, t \right) e^{-j 2 \pi f_0 t} \right] e^{-j 2 \pi f t} \,dt = \tilde{U} \left( \mathbf{r}, f + f_0 \right) $$</p>
<p>The Fourier transform of the derivative of \( U \):</p>
<p>$$\begin{eqnarray}
\int_{-\infty}^{\infty} \frac{d}{dt} \left[ U \left( \mathbf{r}, t \right) \right] e^{-j 2 \pi f t} \,dt &amp;=&amp; j 2 \pi f \tilde{U} \left( \mathbf{r}, f \right)
\end{eqnarray}$$</p>
<p>Now, apply the <a href="https://en.wikipedia.org/wiki/Slowly_varying_envelope_approximation">slowly varying envelope approximation (SVEA)</a> by asserting that the rate of change of \( U \) with respect to time is much less than the value of \( U \) multiplied by the center frequency, or \( \left| \frac{d}{dt} U \left( \mathbf{r}, t\right) \right| \ll \left| 2 \pi f_0 U \left( \mathbf{r, t} \right) \right| \)</p>
<p>$$\begin{eqnarray}
\left| j 2 \pi f \tilde{U} \left( \mathbf{r}, f \right) \right| &amp;=&amp; \left| \int_{-\infty}^{\infty} \frac{d}{dt} \left[ U \left( \mathbf{r}, t \right) \right] e^{-j 2 \pi f t} \,dt \right| \\
&amp;\ll&amp; \left| \int_{-\infty}^{\infty} 2 \pi f_0 U \left( \mathbf{r}, t \right) e^{-j 2 \pi f t} \,dt \right| \\
&amp;\ll&amp; 2 \pi f_0 \left| \tilde{U} \left( \mathbf{r}, f \right) \right|
\end{eqnarray}$$</p>
<p>This expression means that the appreciable frequency components of \( U \left( \mathbf{r} , t \right) \) are much less than the frequency \( f_0 \)<sup id="fnref:2"><a class="footnote-ref" href="posts/a-very-brief-summary-of-the-analytic-signal-in-fourier-optics/#fn:2">2</a></sup>. And when we consider the spectrum of \( U \left( \mathbf{r} , t \right) \) centered around \( f_0 \), we find that the bandwidth \( \Delta f \) is small with respect to \( f_0 \).</p>
<h3>Assumptions, not Approximations!</h3>
<p>The narrowband and slowly varying envelope assumptions are usually referred to as approximations. This is misleading! The resulting expression for the field is not an approximation at all; instead, under the assumptions of narrowbandedness, we can interpret the complex function \( U \left( \mathbf{r}, t \right) \) an "envelope" modulating the amplitude of the fast oscillating carrier wave. If the assumption is not valid, then this interpretation is not correct.</p>
<h3>Narrowband Polychromatic Waves</h3>
<p>In summary, narrowband polychromatic waves with a center frequency \( f_0 \) are modeled as the product of a fast rotating phasor and slowly varying envelope:</p>
<p>$$ u \left( \mathbf{r}, t \right) = \Re \left[ U \left( \mathbf{r}, t \right) e^{-j 2 \pi f_0 t} \right] $$</p>
<p>The amplitude and the phase of the envelope are the amplitude and phase of the real optical wave.</p>
<h2>Coherence</h2>
<p>While the expression for the analytic signal \( U \left( \mathbf{r}, t \right) \) as an integral over frequency components appears deterministic, the phase relationships between the spectral components are often unknown and vary randomly in time. As a result, the envelope of the optical wave will vary unpredictably and must be analyzed in terms of its statistical properties.</p>
<h3>Monochromatic Light is Coherent</h3>
<p>Since monochromatic light has only one spectral component by definition, it is completely coherent.</p>
<ul>
<li>I mean monochromatic in the ideal sense, not like how we sometimes describe lasers.</li>
<li>Monochromatic waves, like plane waves, cannot exist in real life. The uncertainy principle requires that a monochromatic wave exist for an infinite duration.</li>
</ul>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company publishers (2005). ISBN 978-0974707723. <a class="footnote-backref" href="posts/a-very-brief-summary-of-the-analytic-signal-in-fourier-optics/#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p><a href="https://physics.stackexchange.com/questions/451239/slowly-varying-envelope-approximation-what-does-it-imply">https://physics.stackexchange.com/questions/451239/slowly-varying-envelope-approximation-what-does-it-imply</a> <a class="footnote-backref" href="posts/a-very-brief-summary-of-the-analytic-signal-in-fourier-optics/#fnref:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/a-very-brief-summary-of-fresnel-and-fraunhofer-diffraction-integrals/" class="u-url">A Very Brief Summary of Fresnel and Fraunhofer Diffraction Integrals</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/a-very-brief-summary-of-fresnel-and-fraunhofer-diffraction-integrals/" rel="bookmark">
            <time class="published dt-published" datetime="2025-03-28T09:06:03+01:00" itemprop="datePublished" title="2025-03-28 09:06">2025-03-28 09:06</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/a-very-brief-summary-of-fresnel-and-fraunhofer-diffraction-integrals/#disqus_thread" data-disqus-identifier="cache/posts/a-very-brief-summary-of-fresnel-and-fraunhofer-diffraction-integrals.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>Fourier Optics is complicated, and though I have internalized its concepts over the years, I often still need to review the specifics of its mathematical models. Unfortunately, my go-to resource for this, Goodman's Fourier Optics<sup id="fnref:1"><a class="footnote-ref" href="posts/a-very-brief-summary-of-fresnel-and-fraunhofer-diffraction-integrals/#fn:1">1</a></sup>, tends to disperse information across chapters and homework problems. This makes quick review difficult.</p>
<p>Here I condense what I think are the essentials of Fresnel and Fraunhofer diffraction into one blog post.</p>
<h2>Starting Point: the Huygens-Fresnel Principle</h2>
<p>Ignore Chapter 3 of Goodman; it's largely irrelevant for practical work. The Huygens-Fresnel principle itself is a good intuitive model to start with.</p>
<h3>The Model</h3>
<p>An opaque screen with a clear aperture \( \Sigma \)  is located in the \( z = 0 \) plane with transverse coordinates \( \left( \xi , \eta \right ) \). It is illuminated by a complex-valued scalar field \( U \left( \xi, \eta \right) \). Let \( \vec{r_0} = \left( \xi, \eta, 0 \right) \) be a point in the plane of the aperture and \( \vec{r_1} = \left( x, y, z \right) \) be a point in the observation plane. The Huygens-Fresnel Principle provides the following formula for the diffracted field \( U \left( x, y \right) \) in the plane \( z \):</p>
<p>$$ U \left( x, y; z \right) = \frac{z}{j \lambda} \iint_{\Sigma} U \left( \xi , \eta \right) \frac{\exp \left( j k r_{01} \right)}{r_{01}^2} \, d\xi d\eta $$</p>
<p>with the distance \( r_{01}^2 = \left( x - \xi \right)^2 + \left( y - \eta \right)^2 + z^2 \).</p>
<ul>
<li>We assumed an obliquity factor \( cos \, \theta = z / r_{01}\). The choice of obliquity factor depends on the boundary conditions discussed in Chapter 3, but again this isn't terribly important for practical work.</li>
<li>The integral is a sum over secondary spherical wavelets emitted by each point in the aperture and weighted by the incident field and the obliquity factor.</li>
<li>The factor \( 1 / j \) means that each secondary wavelet from a point \( \left( \xi, \eta \right) \) is 90 degrees out-of-phase with the incident field at that point.</li>
</ul>
<h4>Approximations used in the Huygens-Fresnel Principle</h4>
<ol>
<li>The electromagnetic field can be approximated as a complex-valued scalar field.</li>
<li>\( r_{01} \gg \lambda \), or the observation screen is many multiples of the wavelength away from the aperture.</li>
</ol>
<h2>The Fresnel Diffraction Integral</h2>
<h3>The Fresnel Approximation</h3>
<p>Rewrite \( r_{01} \) as:</p>
<p>$$ r_{01} = z \sqrt{ 1 + \frac{\left( x - \xi \right)^2 + \left( y - \eta \right)^2}{z^2} } $$</p>
<p>Apply the binomial approximation:</p>
<p>$$ r_{01} \approx z + \frac{\left( x - \xi \right)^2 + \left( y - \eta \right)^2}{2z} $$</p>
<p>In the Huygens-Fresnel diffraction integral, replace:</p>
<ol>
<li>\(r_{01}^2 \) in the denominator with \( z^2 \)</li>
<li>\(r_{01}\) in the argument of the exponential with \( z + \frac{\left( x - \xi \right)^2 + \left( y - \eta \right)^2}{2z} \)</li>
</ol>
<h4>The Diffraction Integral: Form 1</h4>
<p>Perform the substitutions for \( r_{01} \) into the Huygens-Fresnel formula that were mentioned above to get the first form of the Fresnel diffraction integral:</p>
<p>$$ U \left( x, y; z \right) = \frac{ e^{jkz} }{j \lambda z} \iint_{-\infty}^{\infty} U \left( \xi , \eta \right) \exp \left\{ \frac{jk}{2z} \left[ \left( x - \xi \right)^2 + \left( y - \eta \right)^2 \right] \right\}  \,d\xi \,d\eta $$</p>
<ul>
<li>It is space invariant, i.e. it depends only on the differences in coordinates \( \left( x - \xi \right) \) and \( \left( y - \eta \right) \).</li>
<li>It represents a convolution of the input field with the kernel \( h \left( x, y \right) = \frac{e^{j k z}}{j \lambda z} \exp \left[ \frac{j k}{2 z} \left( x^2 + y^2 \right) \right] \).</li>
</ul>
<h4>The Diffraction Integral: Form 2</h4>
<p>Expand the squared quantities inside the parantheses of Form 1 to get the second from of the integral:</p>
<p>$$ U \left( x, y; z \right) = \frac{ e^{jkz} }{j \lambda z} e^{\frac{j k}{2 z} \left( x^2 + y^2 \right)} \iint_{-\infty}^{\infty} \left[ U \left( \xi , \eta \right) e^{\frac{j k}{2 z} \left( \xi^2 + \eta^2 \right)} \right] e^{-j \frac{2 \pi }{\lambda z} \left( x \xi + y \eta \right) }  \,d\xi \,d\eta $$</p>
<ul>
<li>It is proportional to the Fourier transform of the product of the incident field and a parabolic phase curvature \( e^{\frac{j k}{2 z} \left( \xi^2 + \eta^2 \right)} \).</li>
</ul>
<h2>Phasor Conventions</h2>
<p>Section 4.2.1 of Goodman is an interesting practical aside about how to identify whether a spherical or parabolic wavefront is converging or diverging based on the sign of its phasor. It is useful for solving the important homework problem 4.16 which concerns the diffraction pattern from an aperture that is illuminated by a converging spherical wave.</p>
<p>Unfortunately, Figure 4.2 does not align well with its description in the text about negative z-values, and it's not clear how the interpretations change for point sources not at \( z = 0 \). I address this below.</p>
<ul>
<li>Let the point of convergence (or center of divergence) of a spherical wave sit on the z-axis at \( z = Z \).</li>
<li>The phasor describing the time-dependent part of the field in Goodman's notation is \( e^{-j 2 \pi f t} \).</li>
<li>If we move away from the center of the wave such that \( z - Z \) is positive and we encounter wavefronts emitted earlier in time, then \( t \) is decreasing and the argument to the phasor is increasing. The wave is therefore diverging if the argument is positive.</li>
<li>If we move away from the center of the wave  such that \( z - Z \) is negative and we encounter wavefronts emitted earlier in time, then \( t \) is decreasing and the argument to the phasor is increasing. However, a negative \( z - Z \) makes the phasor negative again so that it is in fact decreasing. The wave is therefore diverging if the argument is negative.</li>
<li>Likewise for converging waves.</li>
</ul>
<p>To summarize:</p>
<table border="1">
<thead><tr>
<th>Phasor</th>
      <th> \( \left( z - Z \right) \) positive </th>
      <th> \( \left( z - Z \right) \) negative </th>
    </tr></thead>
<tbody>
<tr>
<td>\( e^{ j k r} \)</td>
      <td>diverging</td>
      <td>converging</td>
    </tr>
<tr>
<td>\( e^{ -j k r} \)</td>
      <td>converging</td>
      <td>diverging</td>
    </tr>
</tbody>
</table>
<h2>The Fraunhofer Diffraction Integral</h2>
<h3>The Fraunhofer Approximation</h3>
<p>Assume we are so far from the screen that the quadratic phasor inside the diffraction integral is effectively flat. This means: </p>
<p>$$ z \gg \frac{k \left( \xi^2 + \eta^2 \right)_{\text{max}}}{2} $$</p>
<h3>The Diffraction Integral</h3>
<p>Applying the approximation above allows us to drop the quadratic phasor inside the Fresnel diffraction integral because it is effectively 1:</p>
<p>$$ U \left( x, y; z \right) = \frac{ e^{jkz} }{j \lambda z} e^{\frac{j k}{2 z} \left( x^2 + y^2 \right)} \iint_{-\infty}^{\infty} U \left( \xi , \eta \right) e^{-j \frac{2 \pi }{\lambda z} \left( x \xi + y \eta \right) }  \,d\xi \,d\eta $$</p>
<ul>
<li>Apart from the phase term that depends on \( z \), this expression represents a Fourier transform of the incident field.</li>
<li>It appears to break spatial invariance because we no longer depend on differences of coordinates, e.g. \( x - \xi \). However, we can still use the Fresnel transfer function (the Fourier transform of the Fresnel convolution kernel) as the transfer function for Fraunhofer diffraction because if the Fraunhofer approximation is valid, then so is the Fresnel approximation.</li>
</ul>
<h2>Solution to Homework Problem 4.16</h2>
<p>Problem 4.16 is important because it is a basis for the development of the frequency analysis of image-forming systems in later chapters of Goodman.</p>
<p>The purpose of 4.16 is to show that the diffraction pattern of an aperture that is illuminated by a spherical converging wave in the Fresnel regime is the Fraunhofer diffraction pattern of the aperture.</p>
<h3>Part a: Quadratic phase approximation to the incident wave</h3>
<p>Let \( z = 0 \) be the plane of the aperture and \( z = Z \) be the observation plane. Additionally, let \( \left( \xi, \eta \right) \) represent the coordinates in the plane of the aperture, and \( \left( x, y \right) \) the coordinates in the observation plane. The spherical wave that illuminates the aperture is convering to a point \( \vec{r}_P = Y \hat{ \jmath} + Z \hat{k} \) in the observation plane.</p>
<p>To find a quadratic phase approximation for the incident wave, start with its representation as a time-harmonic spherical wave of amplitude \( A \):</p>
<p>$$ U \left( x, y, z \right) = A \frac{e^{j k |\vec{r} - \vec{r}_P|}}{|\vec{r} - \vec{r}_P|} $$</p>
<p>Note that \( \vec{r} - \vec{r}_P = x \hat{\imath} + \left( y - Y \right) \hat{\jmath} + \left( z - Z \right) \hat{k} \). Its magnitude is</p>
<p>$$\begin{eqnarray}
| \vec{r} - \vec{r}_P | &amp;=&amp; \sqrt{x^2 + \left( y - Y \right)^2 + \left( z - Z \right)^2} \\
&amp;=&amp; \left( z - Z \right) \sqrt{1 + \frac{x^2 + \left( y - Y \right)^2}{\left( z - Z \right)^2} } \\
&amp;\approx&amp; \left( z - Z \right) + \frac{ x^2 + \left( y - Y \right)^2 }{2 \left( z - Z \right)}
\end{eqnarray}$$</p>
<p>At first glance, there's a problem here because allowing \( \left( z - Z \right) \) to be negative will result in a negative value for the magnitude of the vector \( \left( \hat{r} - \hat{r}_P \right) \). However, if we use the above table for selecting \( e^{j k r} \) as the phasor for a converging wave when \( \left( z - Z \right) \) is negative, then we will have the correct sign of the argument to the phasor. We do however need to take the absolute value of the \( z - Z \) term in the denominator of the expression of the spherical wave.</p>
<p>Replacing the distance in the phasor's argument with the two lowest order terms in the binomial expansion and the lowest order term in the denominator:</p>
<p>$$ U \left( x, y, z \right) \approx A \frac{e^{j k \left(z - Z \right)} e^{j k \left[ x^2 + \left( y - Y \right)^2 \right] / 2 \left(z - Z \right) }}{\left|z - Z \right|} $$</p>
<p>In the \( z = 0 \) plane, this becomes:</p>
<p>$$ U \left( x, y; z = 0 \right) \approx A \left(x, y \right) \frac{e^{-j k Z} e^{-j k \left[ x^2 + \left( y - Y \right)^2 \right] / 2 Z }}{Z} $$</p>
<p>I moved the finite extent of the aperture into a new function for the amplitude \( A \) above. This function is zero outside the aperture and a constant \( A \) inside it.</p>
<h3>Part b: Diffraction pattern at the point \( P \)</h3>
<p>Use the second form of the Fresnel diffraction integral to compute the diffraction pattern at \( P \):</p>
<p>$$\begin{eqnarray}
U \left( x = 0, y = Y, z = Z \right) &amp;=&amp; \frac{ e^{jkZ} }{j \lambda Z} e^{\frac{j k Y^2}{2 Z}} \iint_{-\infty}^{\infty} \left[ U \left( \xi , \eta ; z = 0 \right) e^{\frac{j k}{2 Z} \left( \xi^2 + \eta^2 \right)} \right] e^{-j \frac{2 \pi }{\lambda Z} y \eta }  \,d\xi \,d\eta \\
&amp;\approx&amp; \frac{ e^{jkZ} e^{-jkZ} }{j \lambda Z^2} e^{\frac{j k Y^2}{2 Z}} \iint_{-\infty}^{\infty} A \left(\xi, \eta \right) \left[ e^{-\frac{j k}{2Z} \left[ \xi^2 + \left( \eta - Y \right)^2 \right]} e^{\frac{j k}{2 Z} \left( \xi^2 + \eta^2 \right)} \right] e^{-j \frac{2 \pi }{\lambda Z} y \eta }  \,d\xi \,d\eta \\
&amp;\approx&amp; \frac{1}{j \lambda Z^2} e^{\frac{j k Y^2}{2 Z} } \iint_{-\infty}^{\infty} A \left(\xi, \eta \right) \left[ e^{-\frac{j k}{2Z} \left( \xi^2 + \eta^2 - 2 \eta Y + Y^2 \right)} e^{\frac{j k}{2 Z} \left( \xi^2 + \eta^2 \right)} \right] e^{-j \frac{2 \pi }{\lambda Z} y \eta }  \,d\xi \,d\eta \\
&amp;\approx&amp; \frac{1}{j \lambda Z^2} \iint_{-\infty}^{\infty} A \left(\xi, \eta \right) e^{\frac{j k \eta Y}{Z}} e^{-j \frac{2 \pi}{\lambda Z} y \eta }  \,d\xi \,d\eta \\
&amp;\approx&amp; \frac{1}{j \lambda Z^2} \iint_{-\infty}^{\infty} A \left(\xi, \eta \right) e^{-j \frac{2 \pi }{\lambda Z} \left(\eta - Y \right) }  \,d\xi \,d\eta
\end{eqnarray}$$</p>
<p>The final expression above is proportional to the Fraunhofer diffraction pattern of the aperture. The reason that the Fraunhofer diffraction pattern appears as the result is that the converging spherical wavefronts exactly cancel the diverging quadratic phase term inside the Fresnel diffraction formula, leaving a simple Fourier transform of the aperture as a result.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company publishers (2005). ISBN 978-0974707723. <a class="footnote-backref" href="posts/a-very-brief-summary-of-fresnel-and-fraunhofer-diffraction-integrals/#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/data-type-alignment-for-ray-tracing-in-rust/" class="u-url">Data Type Alignment for Ray Tracing in Rust</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/data-type-alignment-for-ray-tracing-in-rust/" rel="bookmark">
            <time class="published dt-published" datetime="2025-02-24T08:40:00+01:00" itemprop="datePublished" title="2025-02-24 08:40">2025-02-24 08:40</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/data-type-alignment-for-ray-tracing-in-rust/#disqus_thread" data-disqus-identifier="cache/posts/data-type-alignment-for-ray-tracing-in-rust.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>I would like to clean up my 3D ray trace routines for my <a href="https://www.github.com/kmdouglass/cherry">Rust-based optical design library</a>. The proof of concept (PoC) is finished and I now I need to make the code easier to modify to better support the features that I want to add on the frontend. I suspect that I might be able to make some performance gains as well during refactoring. Towards this end, I want to take a look at my ray data type from the perspective of making it CPU cache friendly.</p>
<p>One of the current obstacles to adding more features to the GUI (for example color selection for different ray bundles) is how I handle individual rays. For the PoC it was fastest to add two additional fields to each ray to track where they come from and whether they are terminated:</p>
<div class="code"><pre class="code literal-block"><span class="k">struct</span><span class="w"> </span><span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">e</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="kt">f64</span><span class="p">;</span><span class="w"> </span><span class="mi">3</span><span class="p">],</span>
<span class="p">}</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">Ray</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">pos</span><span class="p">:</span><span class="w"> </span><span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">dir</span><span class="p">:</span><span class="w"> </span><span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">terminated</span><span class="p">:</span><span class="w"> </span><span class="kt">bool</span><span class="p">,</span>
<span class="w">    </span><span class="n">field_id</span><span class="p">:</span><span class="w"> </span><span class="kt">usize</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>

<p>A ray is just two, 3-element arrays of floats that specify the coordinates of a point on the ray and its direction cosines. I have additionally included a boolean flag to indicate whether the ray has terminated, i.e. gone out-of-bounds of the system or failed to converge during calculation of the intersection point with a surface.</p>
<p>A ray fan is a collection of rays and is specified by a 3-tuple of wavelength, axis, and field; <code>field_id</code> really should not belong to an individual Ray because it can be stored along with the set of all rays for the current ray fan. I probably added it because it was the easiest thing to do at the time to get the application working.</p>
<h2>A deeper look into the Ray struct</h2>
<h3>Size of a ray</h3>
<p>Let's first look to see how much space the Ray struct occupies.</p>
<div class="code"><pre class="code literal-block"><span class="k">use</span><span class="w"> </span><span class="n">std</span><span class="p">::</span><span class="n">mem</span><span class="p">;</span>

<span class="cp">#[derive(Debug)]</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">e</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="kt">f64</span><span class="p">;</span><span class="w"> </span><span class="mi">3</span><span class="p">],</span>
<span class="p">}</span>

<span class="cp">#[derive(Debug)]</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">Ray</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">pos</span><span class="p">:</span><span class="w"> </span><span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">dir</span><span class="p">:</span><span class="w"> </span><span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">terminated</span><span class="p">:</span><span class="w"> </span><span class="kt">bool</span><span class="p">,</span>
<span class="w">    </span><span class="n">field_id</span><span class="p">:</span><span class="w"> </span><span class="kt">usize</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">fn</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Size of ray: {:?}"</span><span class="p">,</span><span class="w"> </span><span class="n">mem</span><span class="p">::</span><span class="n">size_of</span><span class="p">::</span><span class="o">&lt;</span><span class="n">Ray</span><span class="o">&gt;</span><span class="p">());</span>
<span class="p">}</span>
</pre></div>

<p>The <code>Ray</code> struct occupies 64 bytes in memory. Does this make sense?</p>
<p>The sizes of the individual fields are:</p>
<table>
<thead><tr>
<th>Field</th>
<th>Size, bytes</th>
</tr></thead>
<tbody>
<tr>
<td>pos</td>
<td>24</td>
</tr>
<tr>
<td>dir</td>
<td>24</td>
</tr>
<tr>
<td>terminated</td>
<td>1</td>
</tr>
<tr>
<td>field_id</td>
<td>8*</td>
</tr>
</tbody>
</table>
<p><code>pos</code> and <code>dir</code> are each 24 bytes because they are each composed of three 64-bit floats and 8 bits = 1 byte. <code>terminated</code> is only one byte because it is a boolean. <code>field_id</code> is a <a href="https://doc.rust-lang.org/std/primitive.usize.html">usize</a>, which means that it depends on the compilation target. On 64-bit targets, such as x86_64, it is 64 bits = 8 bytes in size.</p>
<p>Adding the sizes in the above table gives 57 bytes, not 64 bytes as was output from the example code. Why is this?</p>
<h3>Alignment and padding</h3>
<p><a href="https://en.wikipedia.org/wiki/Data_structure_alignment">Alignment</a> refers to the layout of a data type in memory and how it is accessed. CPUs read memory in chunks that are equal in size to the <a href="https://en.wikipedia.org/wiki/Word_(computer_architecture)">word size</a>. Misaligned data is inefficient to access because the CPU requires more cycles than is necessary to fetch the data.</p>
<p>Natural alignment refers to the most efficient alignment of a data type for CPU access. To achieve natural alignment, a compiler can introduce padding between fields of a struct so that the memory address of a field or datatype is a multiple of the field's/data type's alignment.</p>
<p>As an example of misalignment, consider a 4-byte integer and that starts at memory address 5. The CPU has 32-bit memory words. To read the data, the CPU must:</p>
<ol>
<li>read bytes 4-7,</li>
<li>read bytes 8-11,</li>
<li>and combine the relevant parts of both reads to get the 4 bytes, i.e. bytes 5 - 8.</li>
</ol>
<p>Notice that we must specify the memory word size to determine whether a data type is misaligned.</p>
<p>Here is an important question: <strong>why can't the CPU just start reading from memory address 5?</strong> The answer, as far as I can tell, is that it just can't. This is not how the CPU, RAM, and memory bus are wired.</p>
<h3>Alignment in Rust</h3>
<p><a href="https://doc.rust-lang.org/reference/type-layout.html#size-and-alignment">Alignment in Rust</a> is defined as follows:</p>
<blockquote>
<p>The alignment of a value specifies what addresses are valid to store the value at. A value of alignment n must only be stored at an address that is a multiple of n.</p>
</blockquote>
<p><a href="https://doc.rust-lang.org/reference/type-layout.html#the-rust-representation">The Rust compiler only guarantees the following</a> when it comes to padding fields in structs:</p>
<blockquote>
<ol>
<li>The fields are properly aligned.</li>
<li>The fields do not overlap.</li>
<li>The alignment of the type is at least the maximum alignment of its fields.</li>
</ol>
</blockquote>
<p>So for my <code>Ray</code> data type, its alignment is 8 because the maximum alignment of its fields is 8 bytes. (<code>pos</code> and <code>dir</code> are composed of 8-byte floating point numbers). The addresses of its fields are:</p>
<div class="code"><pre class="code literal-block"><span class="k">fn</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">ray</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Ray</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">pos</span><span class="p">:</span><span class="w"> </span><span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">e</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">]</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="n">dir</span><span class="p">:</span><span class="w"> </span><span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">e</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">]</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="n">terminated</span><span class="p">:</span><span class="w"> </span><span class="nc">false</span><span class="p">,</span>
<span class="w">        </span><span class="n">field_id</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="k">unsafe</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Address of ray.pos: {:p}"</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">::</span><span class="n">addr_of</span><span class="o">!</span><span class="p">(</span><span class="n">ray</span><span class="p">.</span><span class="n">pos</span><span class="p">));</span>
<span class="w">        </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Address of ray.dir: {:p}"</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">::</span><span class="n">addr_of</span><span class="o">!</span><span class="p">(</span><span class="n">ray</span><span class="p">.</span><span class="n">dir</span><span class="p">));</span>
<span class="w">        </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Address of ray.terminated: {:p}"</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">::</span><span class="n">addr_of</span><span class="o">!</span><span class="p">(</span><span class="n">ray</span><span class="p">.</span><span class="n">terminated</span><span class="p">));</span>
<span class="w">        </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Address of ray.field_id: {:p}"</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span><span class="p">::</span><span class="n">addr_of</span><span class="o">!</span><span class="p">(</span><span class="n">ray</span><span class="p">.</span><span class="n">field_id</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>

<p>I got the following results, which will vary from system-to-system and probably run-to-run:</p>
<div class="code"><pre class="code literal-block"><span class="go">Address of ray.pos: 0x7fff076c6b50</span>
<span class="go">Address of ray.dir: 0x7fff076c6b68</span>
<span class="go">Address of ray.terminated: 0x7fff076c6b88</span>
<span class="go">Address of ray.field_id: 0x7fff076c6b80</span>
</pre></div>

<p>So the <code>pos</code> field comes first at address <code>0x6b50</code> (omitting the most significant hexadecimal digits). Then, 24 bytes later, comes <code>dir</code> at address <code>0x6b68</code>. Note that the difference is hexadecimal 0x18, which is decimal 16 + 8 = 24! So <code>pos</code> really occupies 24 bytes like we previously calculated.</p>
<p>Next comes <code>field_id</code> and not <code>terminated</code>. It is <code>0x6b80 - 0x6b68 = 0x0018</code>, or 24 bytes after <code>dir</code> like before. So far we have no padding, but the compiler did swap the order of the fields. Finally, <code>terminated</code> is 8 bytes after <code>field_id</code> because <code>field_id</code> is 8-byte aligned. This means that the Rust compiler must have placed 7 bytes of padding after the <code>terminated</code> field.</p>
<h2>What makes a good data type?</h2>
<p>As I mentioned, I already know that <code>field_id</code> shouldn't belong to the ray for reasons related to data access by the programmer. So the reason for removing it from the <code>Ray</code> struct is not related to performance. But what about the <code>terminated</code> bool? Well, in this case, it's resulting in 7 extra bytes of padding for each ray!</p>
<div class="code"><pre class="code literal-block"><span class="cp">#[derive(Debug)]</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">e</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="kt">f64</span><span class="p">;</span><span class="w"> </span><span class="mi">3</span><span class="p">],</span>
<span class="p">}</span>

<span class="cp">#[derive(Debug)]</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">Ray</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">pos</span><span class="p">:</span><span class="w"> </span><span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">dir</span><span class="p">:</span><span class="w"> </span><span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">terminated</span><span class="p">:</span><span class="w"> </span><span class="kt">bool</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">fn</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">ray</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Ray</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">pos</span><span class="p">:</span><span class="w"> </span><span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">e</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">]</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="n">dir</span><span class="p">:</span><span class="w"> </span><span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">e</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">]</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="n">terminated</span><span class="p">:</span><span class="w"> </span><span class="nc">false</span><span class="p">,</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Size of ray: {:?}"</span><span class="p">,</span><span class="w"> </span><span class="n">mem</span><span class="p">::</span><span class="n">size_of</span><span class="p">::</span><span class="o">&lt;</span><span class="n">Ray</span><span class="o">&gt;</span><span class="p">());</span>
<span class="p">}</span>
</pre></div>

<p>This program prints <code>Size of ray : 56</code>, but 24 + 24 + 1 = 49. In both versions we waste 7 bytes.</p>
<h3>Fitting a Ray into the CPU cache</h3>
<p>Do I have a good reason to remove <code>terminated</code> from the <code>Ray</code> struct because it wastes space? Consider the following:</p>
<p>We want as many <code>Ray</code> instances as possible to fit within a CPU cache line if we want to maximize performance. (Note that I'm not saying that we necessarily want to maximize performance because that comes with tradeoffs.) Each CPU core on my AMD Ryzen 7 has a 64 kB L1 cache with 64 byte cache lines. This means that I can fit only 1 of the current version of <code>Ray</code> into each cache line for a total of 64 kB / 64 bytes = 1024 rays maximum in the L1 cache of each core. If I remove <code>field_size</code> and <code>terminated</code>, then the size of a ray becomes 48 bytes. Unfortunately, this means that only one <code>Ray</code> instance fits in a cache line, just as before with a 64 byte <code>Ray</code>.</p>
<p>But, if I also reduce my precision to 32-bit floats, then the size of a <code>Ray</code> becomes 6 * 4 = 24 bytes and I have doubled the number of rays that fit in L1 cache.</p>
<p>Now what if I reduced the precision but kept <code>terminated</code>? Then I get 6 * 4 + 8 = 32 bytes per Ray and I still have 2 rays per cache line.</p>
<p>I conclude that there is no reason to remove <code>terminated</code> for performance reasons. Reducing my floating point precision would produce a more noticeable effect on the cache locality of the <code>Ray</code> data type.</p>
<h2>Does all of this matter?</h2>
<p>My Ryzen 7 laptop can trace about 600 rays through 3 surfaces in 380 microseconds with Firefox, Slack, and Outlook running. At this point, I doubt that crafting my data types for cache friendliness is going to offer a significant payoff. Creating data types that are easy to work with is likely more important.</p>
<p>I do think, however, that it's important to understand these concepts. If I do need to tune the performance in the future, then I know where to look.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/an-analog-led-dimmer-circuit/" class="u-url">An Analog LED Dimmer Circuit</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/an-analog-led-dimmer-circuit/" rel="bookmark">
            <time class="published dt-published" datetime="2025-01-17T09:08:27+01:00" itemprop="datePublished" title="2025-01-17 09:08">2025-01-17 09:08</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/an-analog-led-dimmer-circuit/#disqus_thread" data-disqus-identifier="cache/posts/an-analog-led-dimmer-circuit.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>I recently needed to build a circuit to control the brightness of a 4 W LED with a knob. I know basic electronics, and I thought this would be easy. I spoke to a few people whom I know and are knowledgable in electronics. I also asked people on Reddit. A lot of people said it would be easy.</p>
<p>As it turns it, it wasn't easy.</p>
<h2>The Requirements</h2>
<p>My requirments are simple:</p>
<ul>
<li>The brightness should be manually adjustable with a knob from OFF to nearly full ON.</li>
<li>The LED will serve as the light source of a microscope trans-illuminator. It should work across a large range of frame acquisition rates (1 Hz to 1 kHz, or exposure times of 1 ms to 1 s).</li>
<li>The range of brightnesses should be variable across the dynamic range of the camera, which in my case is 35,000:1, or about 90 dB.</li>
</ul>
<p>I don't care about efficiency. I don't care about whether I can use a Raspberry Pi to control it. I don't care whether it can be turned on or off with different logic levels. I just want a knob that I can turn to make the LED brighter or dimmer.</p>
<p>In spite of the insistence of several people that I communicated with on the internet, I decided that the second requirement would preclude using pulse width modulation (PWM) to dim the LED. Even when I could convince others that PWM almost always causes aliasing at high frame rates, they tried to find obscure work arounds so I could still use PWM. I really do appreciate all the feedback I got. But I also learned that PWM is the hammer of the electronics world that makes everything look like a nail<sup id="fnref:1"><a class="footnote-ref" href="posts/an-analog-led-dimmer-circuit/#fn:1">1</a></sup>.</p>
<h2>The Circuit</h2>
<p>I reached out to a friend of mine who's a wizard at analog electronics<sup id="fnref:2"><a class="footnote-ref" href="posts/an-analog-led-dimmer-circuit/#fn:2">2</a></sup>. He suggested to use a MOSFET and to vary the gate-source voltage to control the current through the LED.</p>
<p>After a lot of thinking and reading, I arrived at <a href="https://tinyurl.com/2cnw45fy">the following circuit</a>:</p>
<figure><img src="images/led-dimmer-circuit-v0.png"></figure><p>The LED is an Osram Oslon star LED (LST1-01F05-4070-01) with a maximum current of 1.3 A and a maximum forward voltage of 3.2 V. The MOSFET is an IRF510, whose gate-source threshold voltage is about 3 V.</p>
<p>Here's a brief explanation of what each component does:</p>
<ol>
<li>
<strong>Voltage source</strong> : This is just a 12 V wall wart.</li>
<li>
<strong>200 nF capacitor</strong> : This smooths out any fluctuations from the wall wart.</li>
<li>
<strong>50 kOhm potentiometer</strong> : The "knob." Turning it will vary the gate-source voltage of the MOSFET, which controls how much current flows through the LED.</li>
<li>
<strong>50 kOhm resistor</strong> : This along with the potentiometer forms a voltage divider to keep the minimum voltage at the MOSFET gate close to where the LED turns on. Without it, you need to rotate the potentiometer almost half of its full range for the LED to turn on.</li>
<li>
<strong>300 nF capacitor</strong> : A debounce capacitor that smooths out the mechanical irregularities of the pot when it turns.</li>
<li>
<strong>IRF510 MOSFET</strong> : Basically a valve that I can vary continuously to control the LED current by setting the voltage at the gate.</li>
<li>
<strong>LED</strong> : So pretty.</li>
<li>
<strong>10 Ohm resistor</strong> : This limits the current through the resistor. I calculated its value by dividing the maximum supply voltage minus the maximum forward voltage drop across the LED by the maximum current, then rounded up for safety.</li>
</ol>
<p>$$ R = \frac{V}{I} = \frac{\left( 12 \, V - 3.2 \, V \right) }{1.3 \, A} = 6.8 \Omega $$</p>
<p>The resistor also has to handle a large power dissipation at the maximum current:</p>
<p>$$ P = I^2 R = \left(1.3 \, A \right)^2 \left( 10 \Omega \right) = 16.9 W $$</p>
<p>I decided instead to keep the current to less than 1 Amp so that I could use a 10 Watt resistor that I had.</p>
<p>Power dissipation is also why we don't just use a potentiometer to control the LED current: my pots were only rated up to about 50 mW, whereas I expected that the MOSFET would have handle loads on the order of Watts due to the high current.</p>
<h2>What I Learned</h2>
<h3>I really need to study MOSFETS</h3>
<p>I still don't really know how to solve circuits with MOSFETs. I arrived at the above circuit largely by trial-and-error on a prototype and by performing naive calculations on the voltage divider that turned out to not be entirely correct. I also expected that the LED would turn on once I passed the MOSFET's gate-source voltage threshold, but this turned out to be off by about 2 or 3 V.</p>
<h3>MOSFETs suffer from second order effects</h3>
<p>There is currently a hysteresis in the gate-ground voltage at when the LED turns on and when it turns off by about half a Volt. According to a helpful person on Reddit, this is likely due to a change in both the LEDs forward voltage and the MOSFET's threshold voltage with temperature once the current starts flowing. A possible fix is to swap the order of the LED and the MOSFET so that only the MOSFET will contribute to the hysteresis.</p>
<h3>You can always complicate things to make them better</h3>
<p>The same person on Reddit above also suggested making the circuit robust to temperature variations by adding an opamp to control the MOSFET gate voltage. It would compensate for temperature changes by comparing the potentiometer value to the 10 Ohm resistor in a closed feedback loop.</p>
<h3>Electronics is an art</h3>
<p>Yes, electronics is a science, but I would argue that having to mentally juggle second order effects and the fact that experts seem to make an initial design "by feel" are signatures of an art.</p>
<p>It also struck me how nearly every step of the process forced me to take a detour to address something I hadn't at first considered, such as current limits in the wires and large variations in the MOSFET specs.</p>
<p>The next time I need to do something like this, I will expect the problem to take longer to solve than I first anticipate.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>To my non-native English speaking readers: I mean that people try to use PWM to solve problems where it's not appropriate. <a class="footnote-backref" href="posts/an-analog-led-dimmer-circuit/#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p>And if you play electric guitar, be sure to check out his handmade effects pedals: <a href="https://www.volumeandpower.com/">https://www.volumeandpower.com/</a>. <a class="footnote-backref" href="posts/an-analog-led-dimmer-circuit/#fnref:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/meta-biophysics-of-the-cell/" class="u-url">Meta-Biophysics of the Cell</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/meta-biophysics-of-the-cell/" rel="bookmark">
            <time class="published dt-published" datetime="2025-01-14T17:07:30+01:00" itemprop="datePublished" title="2025-01-14 17:07">2025-01-14 17:07</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/meta-biophysics-of-the-cell/#disqus_thread" data-disqus-identifier="cache/posts/meta-biophysics-of-the-cell.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>I work in a biophysics lab applying microscopy techniques to study cell biology. I am not a biologist, and I was not trained as one. I therefore have had to develop heuristics to help me understand what cell biologists do and to communicate with them effectively.</p>
<p>In this post, I present these heuristics as a sort of model for how I think that cell biologists think. I collectively call them "Meta-Biophysics of the Cell" because:</p>
<ol>
<li>they model the models of cell biologists, hence they are a meta-model,</li>
<li>they are inspired by physics and quantitative modeling, and</li>
<li>I limit myself to cellular processes and not, for example, in vitro or organismal studies.</li>
</ol>
<p>Furthermore, they are heavily biased by my work in microscopy.</p>
<p>These heuristics may very well be wrong. I do not pretend to understand biology nearly as well as a biologist. If you think I am wrong, please do not hesitate to leave a comment and explain why.</p>
<p>They also are certainly not complete. I present here only what I think the most important heuristics are.</p>
<h2>Biologists want to know distributions of proteins across space and time</h2>
<p>Cell biology is concerned with understanding the structure and behavior of cells as complex phenomena that emerge from the interactions of molecules. The types of these molecules may be proteins, nucleic acids, lipids, or some other type. I will use the term "protein" generally because </p>
<ol>
<li>it's easier than always listing all the types of molecules,</li>
<li>it's more precise than just saying "molecule"</li>
<li>there are at least 10,000 different types of proteins in the human cell, making proteins a core building block of the cell, and</li>
<li>these heuristics do not change if you substitute another type of molecule for the word "protein."</li>
</ol>
<p>Within a single cell, we can model the number of proteins at a point in space and a point in time with a function \( N \left( \mathbf{r}, t \right) \). This is a spatiotemporal distribution for protein number density. Its dimensions are in numbers of proteins per unit of volume.</p>
<p>A single protein can be represented as a point in space and time, such as a delta function:</p>
<p>$$ N \left( \mathbf{r}, t \right) = \delta \left( \mathbf{r}, t \right) $$</p>
<p>Of course a real protein occupies some volume and is not a point, but at the level of a cell I think that this is an adequate model for what cell biologists want to know about protein distributions.</p>
<p>But wait. There are more than ten thousand types of proteins within the cell. Some proteins are of high abundance, and some are very rare. So it is not sufficient merely to count proteins in space and time: we also need to identify their types. For this, I assign a unique ID to each type that I call \( s \) for "species". Our model now becomes a function of another variable, one that is categorical rather than continuous:</p>
<p>$$ N \left( \mathbf{r}, t ; s\right) $$</p>
<p>Below I show a simplified schematic of the volume that this model occupies. It is simplified because I show only one spatial dimension (otherwise it would be a five dimensional hypervolume). I saw a figure like this once in a paper about a decade ago, but sadly I cannot find it to credit it. (<em>Update 2025-01-30: The paper referred to is <a href="https://doi.org/10.1016/j.cell.2007.08.031">Megason and Fraser, "Imaging in Systems Biology," Cell 130(5), 784-795 (2007)</a></em>)</p>
<figure><img src="images/protein-distributions.png"><figcaption>A cell can be represented as a "volume" of spatiotemporal distributions where one spatiotemporal slice belongs to each protein species.</figcaption></figure><p>The \( x \) and \( t \) dimensions are continuous; the \( s \) dimension is a discrete, categorical variable. Each \( s \) slice is the spatiotemporal distribution of that protein within a single cell. Each point in the volume is the protein density for that point in space and time and for that species of protein.</p>
<p>Individual proteins might also vary amongst themselves as in, for example, post translation modifications. This is not really a problem for the above model because we can use a different value for the \( s \) of each variant.</p>
<h3>Creation and degradation of a protein</h3>
<p>The appearance and disappearance of a protein is modeled as a non-zero value over a time range from \( t_0 \) to \( t_1 \). For example, a single protein existing over a finite time interval may be expressed as</p>
<p>$$\begin{eqnarray}
 &amp;&amp;\delta \left( \mathbf{r}, t \right), \, t_0 &lt; t &lt; t_1 \\
 &amp;&amp;0, \, \text{otherwise}
\end{eqnarray}$$</p>
<h3>Biologists can only measure slices of protein distributions</h3>
<p>Look again at the figure above. Remember that there are in fact five dimensions. Can any one experimental technique measure the whole hypevolume?</p>
<p>No. Instead, biologists can measure slices from the volume and try to piece together a complete picture of \( N \) from individual measurements.</p>
<p>For example, fluorescence microscopy can measure proteins in space and time. Sometimes it can measure in 3 spatial dimensions, but it is easier to measure in 2. Unfortunately, it can only measure a small of number of protein species relative to all the proteins that are in the cell. These would correspond to the different fluorescence channels of the measurement. Thus, fluorescence microscopy provides a slice of the volume that looks like the following:</p>
<figure><img src="images/protein-distributions-microscopy-slice.png"><figcaption>Fluorescence microscopy measures a small slice of the spatiotemporal protein distribution that represents a cell.</figcaption></figure><p>Other types of measurements, such as those in single cell proteomics, might measure a large number of proteins but cannot resolve them in space and time. They would slice the volume like this:</p>
<figure><img src="images/protein-distributions-proteomics-slice.png"></figure><p>So after their measurements, biologists are always going to be left with less than the total amount of information contained within a single cell because they can only measure slices of \( N \).</p>
<h2>Organelles are mutually exclusive slices of protein distributions in space</h2>
<p>Consider a mitochondrion. It is a membrane-bound organelle. Everything inside the mitochondrion is considered part of the organelle, and everything outside it is not. An organelle is therefore a mutually-exclusive slice through the volume dimensions of \( N \).</p>
<p>The slices are mutually-exclusive because two mitochondria cannot occupy the same volume at the same time and have a distinct identity.</p>
<p>For non-membrane bound organelles, keep reading.</p>
<h3>Organelles contain many types proteins</h3>
<p>If we use the definition of an organelle as a mutually-exclusive slice through the volume dimensions of \( N \), then we can look sideways along the \( s \) dimension to find all the proteins that belong to the organelle. If the distribution for protein \( s_i \) is non-zero inside an organelle's volume at a given time, then it belongs to the organelle. The set of all proteins within the volume slice at a given time constitute the organelle.</p>
<h3>Knowing \( N \) doesn't by itself tell us what is and is not an organelle</h3>
<p>Membrane-bound organelles are easy to identify because of their structure. Other sets of proteins within a given volume may or may not form an organelle. In these situations, we might look at their function instead to decide whether the volume is or is not occupied by an organelle.</p>
<p>For example, organelles like the centrosome have a diffuse, pericentriolar material that surround them. In this case, the border defining what is and isn't inside the centrosome is likely to be somewhat arbitrary.</p>
<h2>Cause and Effect is the probability of one protein distribution given another</h2>
<p>At this time, I am much less certain about how function and causal relationships fit within this model. It is nevertheless important because biologists are deeply interested in the function of proteins and other complexes. To a rough approximation, I would say that cause and effect describes how the spatiotemporal distributions of a subset of protein species can serve as a predictor of another distribution at a later time. In other words, we can assign a probability to a certain distribution given another one.</p>
<p>I would guess that not every possible set proteins is linked by causal relationships. This would mean that the limitations that come from being able to sample slices of the protein distribution hypervolume are not so significant. You would then want choose your measurements so that you slice the volume to include only the species that are causally linked for the phenomenon that you are studying.</p>
<p>As a consequence of this, the causal links between distributions are likely more important than knowing \( N \). I doubt that we can have a satisfactory understanding of the cell if we could exhaustively measure \( N \) for even a single cell.</p>
<h3>Interactions between proteins require spatial colocalization</h3>
<p>Protein-protein interactions occur on length-scales on the order of the size of individual proteins. For two different proteins to "interact" we require that they be colocated less than this distance. Colcalization means that two proteins are located less than the distance required for an interaction to occur.</p>
<p>Furthermore, colocalization is necessary but not sufficient for an interaction. A real interaction involves the chemistry between the two different species.</p>
<h2>Summary</h2>
<p>In summary, my main three heuristics for meta-biophysics of the cell are:</p>
<ol>
<li>Biologists want to know distributions of proteins across space and time</li>
<li>Organelles are mutually exclusive slices of protein distributions in space</li>
<li>Cause and Effect is the probability of one or more protein distributions given another</li>
</ol>
<p>I find that nearly all the problems that the cell biologists that I work with can be reformulated into this language. I emphasize again that the "real" science is being done by the biologists, and I in no way mean to diminish the complexity of their work. These heuristics are merely a tool that I use to understand what they are doing when I myself am unfamiliar with their jargon and mental models.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/coordinate-systems-for-modeling-microscope-objectives/" class="u-url">Coordinate Systems for Modeling Microscope Objectives</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/coordinate-systems-for-modeling-microscope-objectives/" rel="bookmark">
            <time class="published dt-published" datetime="2024-11-21T10:52:48+01:00" itemprop="datePublished" title="2024-11-21 10:52">2024-11-21 10:52</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/coordinate-systems-for-modeling-microscope-objectives/#disqus_thread" data-disqus-identifier="cache/posts/coordinate-systems-for-modeling-microscope-objectives.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>A common model for infinity corrected microscope objectives is that of an aplanatic and telecentric optical system. In many developments of this model, emphasis is placed upon the calculation of the electric field near the focus. However, this has the effect that the definition of the coordinate systems and geometry are conflated with the determination of the fields. In addition, making the model amenable to computation often occurs as an afterthought.</p>
<p>In this post I will explore the geometry of an aplanatic system for modeling high NA objectives with an emphasis on computational implementations. My approach follows Novotny and Hecht<sup id="fnref:1"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:1">1</a></sup> and Herrera and Quinto-Su<sup id="fnref:2"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:2">2</a></sup>.</p>
<h2>The Model Components</h2>
<p>The model system is illustrated below:</p>
<figure><img src="images/aplanatic-telecentric-system.png"><figcaption>A high NA, infinity corrected microscope objective as an aplanatic and telecentric optical system.
</figcaption></figure><p>In this model, we abstract over the details of the objective by representing it as four surfaces:</p>
<ol>
<li>A back focal plane containing an aperture stop</li>
<li>A back principal plane, \( P \)</li>
<li>A front principal surface, \( P' \)</li>
<li>A front focal plane</li>
</ol>
<p>The space to the left of the back principal plane is called the infinity space. The space to the right of the front principal surface is called the sample space.</p>
<p>We let the infinity space refractive index \( n_1 = 1 \) because it is in air. The refractive index \( n_2 \) is the refractive index of the immersion medium.</p>
<p>The unit vectors \( \mathbf{n} \) are not used in this discussion; they are relevant for computing the fields.</p>
<h3>Assumptions</h3>
<p>We make one assumption: the system obeys the sine condition. The meaning of this will be explained later.</p>
<p>An aplanatic system is one that obeys the sine condition.</p>
<p>We will not assume the intensity law to conserve energy because it is only necessary when computing the electric field near the focus.</p>
<h3>The Aperture Stop and Back Focal Plane</h3>
<p>The aperture stop (AS) of an optical system is the element that limits the angle of the marginal ray.</p>
<p>The system is telecentric because the aperture stop is located in the back focal plane (BFP). We can shape the focal field by spatially modulating any of the amplitude, phase, or polarization of the incident light in a plane conjugate to the BFP.</p>
<h3>The Back Principal Plane</h3>
<p>This is the plane in infinity space at which rays appear to refract. It is a plane because rays coming from a point in the front focal plane all emerge into the infinity space in the same direction.</p>
<p>Strictly speaking, focus field calculations require us to propagate the field from the AS to the back principal plane before computing the Debye diffraction integral, but this step is often omitted<sup id="fnref:3"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:3">3</a></sup>. The assumptions of paraxial optics should hold here.</p>
<h3>The Front Principal Surface</h3>
<p>The front principal surface is the surface at which rays appear to refract in the sample space. It is a surface because</p>
<ol>
<li>this is a non-paraxial system, and</li>
<li>we assumed the sine condition.</li>
</ol>
<p>The sine condition states that refraction of a ray coming from an on-axis point in the front focal plane occurs on a spherical cap centered upon the focal point. The distance from the optical axis of the point of intersection of the ray with the surface is proportional to the sine of the angle that the ray makes with the axis.</p>
<p>The principal surface is in the far field of the electric field coming from the focal region. For this reason, we can represent a point on this surface as representing a single ray or a plane wave<sup id="fnref2:1"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:1">1</a></sup>.</p>
<h3>The Front Focal Plane</h3>
<p>This plane is located a distance \( n_2 f \) from the principal surface<sup id="fnref:4"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:4">4</a></sup>. It is not at a distance \( f \) from this surface. This is a result of imaging in an immersion medium.</p>
<h2>Geometry and Coordinate Systems</h2>
<h3>The Aperture Stop Radius</h3>
<p>The aperture stop radius \( R \) corresponds to the distance from the axis to the point where the marginal ray intersects the front prinicpal surface. In the sample space, the marginal ray travels at an angle \( \theta_{max} \) with respect to the axis.</p>
<p>Under the sine condition, this height is</p>
<p>$$ R = n_2 f \sin{ \theta_{max} } = f \, \text{NA} $$</p>
<p>The right-most expression uses the definition of the numerical aperture \( \text{NA} \equiv n \sin{ \theta_{max} } \).</p>
<p>Compare this result to the oft-cited expression for the entrance pupil diameter of an objective lens: \( D = 2 f \, \text{NA} \). They are the same. This makes sense because an entrance pupil is either</p>
<ol>
<li>an image of an aperture stop, or</li>
<li>a physical stop.</li>
</ol>
<h3>The Back Principal Plane</h3>
<p>There are two independent coordinate systems in the back principal plane:</p>
<ol>
<li>the spatial coordinate system defining the far field positions \( \left( x_{\infty} , y_{\infty} \right) \), and</li>
<li>the coordinate system of the angular spectrum of plane waves \( \left( k_x, k_y \right) \).</li>
</ol>
<h4>The Far Field Coordinate System</h4>
<p>The far field coordinate system may be written in Cartesian form as \( \left( x_{\infty} , y_{\infty} \right) \). It also has a cylindrical representation as</p>
<p>$$\begin{eqnarray}
\rho &amp;=&amp; \sqrt{x_{\infty}^2 + y_{\infty}^2} \\
\phi &amp;=&amp; \arctan \left( \frac{y_{\infty}}{x_{\infty}} \right)
\end{eqnarray}$$</p>
<p>The cylindrical representation appears to be preferred in textbook developments of the model. The Cartesian representation is likely preferred for computational models because it works naturally with two-dimensional arrays of numbers, and because beam shaping elements such as spatial light modulators are rectangular arrays of pixels<sup id="fnref2:2"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:2">2</a></sup>.</p>
<h4>The Angular Spectrum Coordinate System</h4>
<p>Each point in the angular spectrum coordinate system represents a plane wave in the sample space that is traveling at an angle \( \theta \) to the axis according to:</p>
<p>$$\begin{eqnarray}
k_x &amp;=&amp; k \sin \theta \cos \phi \\
k_y &amp;=&amp; k \sin \theta \sin \phi \\
k_z &amp;=&amp; k \cos \theta
\end{eqnarray}$$</p>
<p>where \( k = 2 \pi n_2 / \lambda = n_2 k_0 \).</p>
<p>Along the y-axis ( \( x_{\infty} = 0 \) ), the maximum value of \( k_y \) is \(n_2 k_0 \sin \theta_{max} = k_0 \, \text{NA} \).</p>
<p>Substitute in the expression \( \text{NA} = R / f \) and we get \(k_{y, max} = k_0 R / f\). But \( R = y_{\infty, max} \). This (and similar reasoning for the x-axis) implies that:</p>
<p>$$\begin{eqnarray}
k_x &amp;=&amp; k_0 x_{\infty} / f \\
k_y &amp;=&amp; k_0 y_{\infty} / f
\end{eqnarray}$$</p>
<p>The above equations link the angular spectrum coordinate system to the far field coordinate system. They are no longer independent once \( f \) and \( \lambda \) are specified.</p>
<h2>Numerical Meshes</h2>
<p>There are four free parameters for defining the coordinate systems of the numerical meshes:</p>
<ol>
<li>The numerical aperture, \( \text{NA} \)</li>
<li>The wavelength, \( \lambda \)</li>
<li>The focal length, \( f \)</li>
<li>The linear mesh size, \( L \)</li>
</ol>
<p>Below is a figure that illustrates the construction of the meshes. Both the far field and angular spectrum coordinate systems are represented by a \( L \times L \) array. \( L = 16 \) in the figure below. In general the value of \( L \) should be a power of 2 to help ensure the efficiency of the Fast Fourier Transform (FFT). By considering only powers of 2, we need only consider arrays of even size as well.</p>
<figure><img src="images/pupil-function-simulation-mesh.png"><figcaption>A numeric mesh representing the far field and angular spectrum coordinate systems of a microscope objective. Fields are sampled at the center of each mesh pixel.</figcaption></figure><p>The fields are defined on a region of circular support that is centered on this array. The radius of the domain of the far field coordinate system is \( f \text{NA} \); the radius of the domain of the angular spectrum coordinate system is \( k_0 \text{NA} \).</p>
<p>The boxes that are bound by the gray lines indicate the location of each field sample. The \( \left( x_{\infty} , y_{\infty} \right) \) and the \( \left( k_x, k_y \right) \) coordinate systems are sampled at the center of each gray box. The origin is therefore not sampled, which will help avoid division by zero errors when the fields are eventually computed.</p>
<p>The figure suggests that we could create only one mesh and scale it by either \( f \text{NA} \) or \( k_0 \text{NA} \) depending on which coordinate system we are working with. The normalized coordinates become \( \left( x_{\infty} / \left( f \text{NA} \right), y_{\infty} / \left( f \text{NA} \right) \right) \) and \( \left( k_x / \left( k_0 \text{NA} \right), k_y / \left( k_0 \text{NA} \right) \right) \).</p>
<h3>1D Mesh Example</h3>
<p>As an example, let \( L = 16 \). To four decimal places, the normalized coordinates are \( -1.0000, -0.8667, \ldots, -0.0667, 0.0667, \ldots, 0.8667, 1.0000 \).</p>
<p>The spacing between array elements is \( 2 / \left( L - 1 \right) = 0.1333 \). Note that 0 is not included in the 1D mesh as it goes from -0.0667 to 0.0667.</p>
<p>A 2D mesh is easily constructed from the 1D mesh using tools such as NumPy's <a href="https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html">meshgrid</a>.</p>
<h3>Back Principal Plane Mesh Spacings</h3>
<p>In the x-direction, the mesh spacing of the far field coordinate system is</p>
<p>$$ \Delta x_{\infty} = 2 R / \left( L - 1 \right) = 2 f \text{NA} / \left( L - 1 \right) $$</p>
<p>In the \( k_x \)-direction, the mesh spacing of the angular spectrum coordinate system is</p>
<p>$$ \Delta k_x = 2 k_{max} / \left( L - 1 \right) = 2 k_0 \text{NA} / \left( L - 1 \right) $$</p>
<p>Note the symmetry between these two expressions. One scales with \( f \text{NA} \) and the other \( k_0 \text{NA} \). Recall that these are free parameters of the model.</p>
<h3>Sample Space Mesh Spacing</h3>
<p>It is interesting to compute the spacing between mesh elements \( \Delta x \) in the sample space when the fields are eventually computed.</p>
<p>The sampling angular frequency in the sample space is \( k_S = 2 \pi / \Delta x \).</p>
<p>The Nyquist-Shannon sampling theory states that the maximum informative angular frequency is \( k_{max} = k_S / 2 \).</p>
<p>From the previous section, we know that \( k_{max} = \left(L - 1 \right) \Delta k_x / 2 \), and that \( \Delta k_x = 2 k_0 \text{NA} / \left( L - 1 \right) \).</p>
<p>Combining all the previous expressions and simplifying, we get:</p>
<p>$$\begin{eqnarray}
k_S &amp;=&amp; 2 k_{max} \\
2 \pi / \Delta x &amp;=&amp; \left(L - 1 \right) \Delta k_x \\
2 \pi / \Delta x &amp;=&amp; \left(L - 1 \right) \left[ 2 k_0 \text{NA} / \left( L - 1 \right) \right] \\
2 \pi / \Delta x &amp;=&amp; \left(L - 1 \right) \left[ 2 \left(2 \pi / \lambda \right) \text{NA} / \left( L - 1 \right) \right]
\end{eqnarray}$$</p>
<p>Solving the above expression for \( \Delta x \), we arrive at</p>
<p>$$ \Delta x = \frac{\lambda}{2 \text{NA}} $$</p>
<p>which is of course the Abbe diffraction limit.</p>
<h3>Effect of not Sampling the Origin</h3>
<p>Herrera and Quinto-Su<sup id="fnref3:2"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:2">2</a></sup> point out that an error will be introduced if we naively apply the FFT to compute the field components in the \( \left( k_x, k_y \right) \) coordinate system because the origin is not sampled, whereas the FFT assumes that we sample the zero frequency component. The effect is that the result of the FFT has a constant phase error that accounts for a half-pixel shift in each direction of the mesh.</p>
<p>Consider again the 1D mesh example with \(L = 16 \): \( -1.0000, -0.8667, \ldots, -0.0667, 0.0667, \ldots, 0.8667, 1.0000 \)</p>
<p>In Python and other languages that index arrays starting at 0, the origin is located at \(L / 2 - 0.5 \), i.e. halfway between the samples at index 7 and 8. A lateral shift in Fourier space is equivalent to a phase shift in real space:</p>
<p>$$ \phi_{shift} \left(X, Y \right) =  -j 2 \pi \frac{0.5}{L} X - j 2 \pi \frac{0.5}{L} Y $$</p>
<p>where \( X \) and \( Y \) are normalized coordinates.</p>
<p>At this point, I am uncertain whether the phasor with the above argument needs to be multiplied or divided with the result of the FFT because 1. there are a few typos in the signs for the coordinate system bounds in the manuscript of Herrera and Quinto-Su, and 2. the correction was developed for use in MATLAB, which indexes arrays starting at 1. Once the fields are computed, it would be easy to verify the correct sign of the phase terms following the procedure outlined in Figure 3 of Herrera and Quinto-Su's manuscript.</p>
<h3>Structure of the Algorithm</h3>
<p>The algorithm to compute the focus fields will proceed as follows:</p>
<ol>
<li>(optional) Propgate the inputs fields from the AS to the back principal plane using paraxial wave propagation</li>
<li>Input the sampled fields in the back principal plane in the \( \left( x_{\infty}, y_{\infty} \right) \) coordinate system</li>
<li>Transform the fields to the \( \left( k_x, k_y \right) \) coordinate system</li>
<li>Compute the fields in the \( \left(x, y, z \right) \) coordinate system using the FFT</li>
</ol>
<h2>Additional Remarks</h2>
<ul>
<li>Zero padding the mesh will increase the sample space resolution beyond the Abbe limit, but since the fields remain zero outside of the support, no new information is added.</li>
<li>On the other hand, zero padding might be required when computing fields going from the sample space to the back principal plane to faithfully reproduce any evanescent components.</li>
<li>Separating the coordinate system and mesh construction from the calculation of the fields reveals that the two assumptions of the model belong separately to each part. The sine condition is used in the construction of the coordinate systems, whereas energy conservation is used when computing the fields.</li>
<li>This post did not explain how to compute the fields.</li>
<li>Herrera and Quinto-Su (and possibly also Novotny and Hecht) appear to use an "effective" focal length which can be obtained by multiplying the one that I use by the sample space refractive index. I prefer my formulation because it is consistent with geometric optics and the well-known expression for the diameter of an objective's entrance pupil. When the fields are calculated, however, I do not yet know whether the arguments of the phasors of the Debye integral will require modification.</li>
</ul>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>Lukas Novotny and Bert Hecht, "Principles of Nano-Optics," Cambridge University Press (2006). <a href="https://doi.org/10.1017/CBO9780511813535">https://doi.org/10.1017/CBO9780511813535</a> <a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:1" title="Jump back to footnote 1 in the text">↩</a><a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref2:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p>Isael Herrera and Pedro A. Quinto-Su, "Simple computer program to calculate arbitrary tightly focused (propagating and evanescent) vector light fields," arXiv:2211.06725 (2022). <a href="https://doi.org/10.48550/arXiv.2211.06725">https://doi.org/10.48550/arXiv.2211.06725</a> <a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:2" title="Jump back to footnote 2 in the text">↩</a><a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref2:2" title="Jump back to footnote 2 in the text">↩</a><a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref3:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:3">
<p>Marcel Leutenegger, Ramachandra Rao, Rainer A. Leitgeb, and Theo Lasser, "Fast focus field calculations," Opt. Express 14, 11277-11291 (2006). <a href="https://doi.org/10.1364/OE.14.011277">https://doi.org/10.1364/OE.14.011277</a> <a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:3" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:4">
<p>Sun-Uk Hwang and Yong-Gu Lee, "Simulation of an oil immersion objective lens: A simplified ray-optics model considering Abbe’s sine condition," Opt. Express 16, 21170-21183 (2008). <a href="https://doi.org/10.1364/OE.16.021170">https://doi.org/10.1364/OE.16.021170</a> <a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:4" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/github-cli-authorization-with-a-fine-grained-access-token/" class="u-url">GitHub CLI Authorization with a Fine-grained Access Token</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/github-cli-authorization-with-a-fine-grained-access-token/" rel="bookmark">
            <time class="published dt-published" datetime="2024-10-04T14:18:48+02:00" itemprop="datePublished" title="2024-10-04 14:18">2024-10-04 14:18</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/github-cli-authorization-with-a-fine-grained-access-token/#disqus_thread" data-disqus-identifier="cache/posts/github-cli-authorization-with-a-fine-grained-access-token.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>It is a good idea to use fine-grained access tokens for shared PCs in the lab that require access to private GitHub repos so that you can restrict the scope of their use to specific repositories and not use your own personal SSH keys on the shared machines. I am experimenting with the GitHub command line tool <code>gh</code> to authenticate with GitHub using fine-grained access tokens and make common remote operations on repos easier.</p>
<p>Today I encountered a subtle problem in the <code>gh</code> authentication process. If you set the protocol to <code>ssh</code> during login, then you will not have access to the repos that you granted permissions to in the fine-grained access token. This can lead to a lot of head scratching because it's not at all clear which permissions map to which git operations. In other words, what you think is a specific permissions error with the token is actually an authentication error.</p>
<p>To avoid the problem, be sure to specify <code>https</code> and not <code>ssh</code> as the protocol during authentication:</p>
<div class="code"><pre class="code literal-block"><span class="go"> echo "$ACCESS_TOKEN" | gh auth login -p https --with-token</span>
</pre></div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/raspberry-pi-i2c-quickstart/" class="u-url">Raspberry Pi I2C Quickstart</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/raspberry-pi-i2c-quickstart/" rel="bookmark">
            <time class="published dt-published" datetime="2024-09-24T15:01:45+02:00" itemprop="datePublished" title="2024-09-24 15:01">2024-09-24 15:01</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/raspberry-pi-i2c-quickstart/#disqus_thread" data-disqus-identifier="cache/posts/raspberry-pi-i2c-quickstart.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>Below are my notes concerning the control of a <a href="https://www.sparkfun.com/products/12918">Sparkfun MCP4725 12-bit DAC</a> over I2C with a Raspberry Pi.</p>
<h2>Rasbperry Pi Setup</h2>
<ol>
<li>Enable the I2C interface if isn't already with <code>raspi-config</code>. Verify that the I2C device file(s) are present in <code>/dev/</code> with <code>ls /dev | grep i2c</code>. (I had two files: <code>i2c-1</code> and <code>i2c-2</code>.)</li>
<li>Install the <code>i2c-tools</code> package for debugging I2C interfaces.</li>
</ol>
<div class="code"><pre class="code literal-block"><span class="go">sudo apt update &amp;&amp; sudo apt install -y i2c-tools</span>
</pre></div>

<h3>i2cdetect</h3>
<p>Attach the DAC to the Raspberry Pi. The pinout is simple:</p>
<table>
<thead><tr>
<th>Raspberry Pi</th>
<th>MCP4725</th>
</tr></thead>
<tbody>
<tr>
<td>GND</td>
<td>GND</td>
</tr>
<tr>
<td>3.3V</td>
<td>Vcc</td>
</tr>
<tr>
<td>SCL</td>
<td>SCL</td>
</tr>
<tr>
<td>SDA</td>
<td>SDA</td>
</tr>
</tbody>
</table>
<p>Next, run  the command <code>i2cdetect -y 1</code>. This will check for a device on bus 1 (<code>/dev/i2c-1</code>) and automatically accept confirmations:</p>
<div class="code"><pre class="code literal-block"><span class="gp">leb@raspberrypi:~/$ </span>i2cdetect<span class="w"> </span>-y<span class="w"> </span><span class="m">1</span>
<span class="go">     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f</span>
<span class="go">00:                         -- -- -- -- -- -- -- --</span>
<span class="go">10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">60: 60 -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">70: -- -- -- -- -- -- -- --</span>
</pre></div>

<p>Each I2C device must have a unique 7-bit address, i.e. 0x00 to 0x7f. The ranges [0x00, 0x07] and [0x78, 0x7f] are reserved. The above output indicates the DAC is at address 0x60. (Rows are the value of the first hexadecimal number of the address, columns are the second.)</p>
<h3>i2cset</h3>
<p><code>i2cset</code> is a command line tool that is part of <code>i2c-tools</code> and that is used to write data to I2C devices. I can set the voltage output of the DAC to 0 as follows:</p>
<div class="code"><pre class="code literal-block"><span class="go">i2cset -y 1 0x60 0x40 0x00 0x00 i</span>
</pre></div>

<p>The arguments mean the following:</p>
<ul>
<li>
<strong>-y</strong> : Auto-confirm</li>
<li>
<strong>1</strong> : Use the device on bus 1</li>
<li>
<strong>0x60</strong> : Use the device at address <strong>0x60</strong>
</li>
<li>
<strong>0x40</strong> : This is a command byte</li>
<li>
<strong>0x00 0x00</strong> : These two data bytes specify the DAC output level</li>
<li>
<strong>i</strong> : This is the write mode. <code>i</code> means I2C block write: <a href="https://docs.kernel.org/i2c/smbus-protocol.html#i2c-block-write">https://docs.kernel.org/i2c/smbus-protocol.html#i2c-block-write</a>
</li>
</ul>
<h4>Command byte</h4>
<p>The command byte is explained on pages 23 and 25 of the <a href="https://ww1.microchip.com/downloads/en/devicedoc/22039d.pdf">MCP4725 datasheet</a>. From most-significant to least-significant bits, the bits mean:</p>
<ol>
<li>
<strong>C2</strong> : command bit</li>
<li>
<strong>C1</strong> : command bit</li>
<li>
<strong>C0</strong> : command bit </li>
<li>
<strong>X</strong> : unused</li>
<li>
<strong>X</strong> : unused</li>
<li>
<strong>PD1</strong> : Power down select</li>
<li>
<strong>PD0</strong> : Power down select</li>
<li>
<strong>X</strong> : unused</li>
</ol>
<p>According to Table 6-2 and Figure 6-2, <code>C2, C1, C0 = 0, 1, 0</code> identifies the command to write to the DAC register and NOT also to the EEPROM. In normal operation, the power down bits are 0, 0 (page 28).</p>
<p>So, to write to the DAC register, we want to send <code>0b01000000</code> which in hexadecimal is <code>0x40</code>.</p>
<h4>Data bytes to voltage</h4>
<p>The data bytes are explained in Figure 6-2 of the datasheet. The first byte contains bits 11-4, and the second byte bits 3-0 in the most-significant bits:</p>
<p><code>D11 D10 D9 D8   D7 D6 D5 D4 | D3 D2 D1 D0  X X X X</code></p>
<p>12-bits are used because this is a 12-bit DAC. The mapping between bytes and voltage is:</p>
<table>
<thead><tr>
<th>Data bytes, hex</th>
<th>Data bytes, decimal</th>
<th>Voltage</th>
</tr></thead>
<tbody>
<tr>
<td>0x00 0x00</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0xFF 0xF0</td>
<td>65520</td>
<td>V_max</td>
</tr>
</tbody>
</table>
<p>where V_max is the voltage supplied to the chip's Vcc pin (3.3V in my case). The output step size is \( \Delta V = V_{max} / 4096 \) or about 0.8 mV.</p>
<h2>Control via Python</h2>
<p>This is modified from <a href="https://learn.sparkfun.com/tutorials/raspberry-pi-spi-and-i2c-tutorial/all">Sparkfun's tutorial</a> and uses the smbus Python bindings. Be aware that the tutorial example has a bug in how it prepares the list of bytes to send to the DAC.</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span><span class="w"> </span><span class="nn">smbus</span>


<span class="n">OUTPUT_MAX</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4095</span>
<span class="n">V_MAX</span> <span class="o">=</span> <span class="mf">3.3</span>


<span class="k">def</span><span class="w"> </span><span class="nf">send</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">channel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device_address</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mh">0x60</span><span class="p">,</span> <span class="n">command_byte</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mh">0x40</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">output</span> <span class="o">&gt;</span> <span class="mf">0.0</span> <span class="ow">and</span> <span class="n">output</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">"Output voltage must be expressed as fraction of the maximum in the range [0.0, 1.0]"</span>

    <span class="n">bus</span> <span class="o">=</span> <span class="n">smbus</span><span class="o">.</span><span class="n">SMBus</span><span class="p">(</span><span class="n">channel</span><span class="p">)</span>

    <span class="n">output_bytes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">output</span> <span class="o">*</span> <span class="n">OUTPUT_MAX</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xfff</span>
    <span class="n">data_byte_0</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_bytes</span> <span class="o">&amp;</span> <span class="mh">0xff0</span><span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="mi">4</span>  <span class="c1"># First data byte</span>
    <span class="n">data_bytes</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">data_byte_0</span><span class="p">,</span> <span class="p">(</span><span class="n">output_bytes</span> <span class="o">&amp;</span> <span class="mh">0xf</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="mi">4</span><span class="p">]</span>  <span class="c1"># Second data byte</span>

    <span class="n">bus</span><span class="o">.</span><span class="n">write_i2c_block_data</span><span class="p">(</span><span class="n">device_address</span><span class="p">,</span> <span class="n">command_byte</span><span class="p">,</span> <span class="n">data_bytes</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">output</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.42</span>
    <span class="n">send</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Estimated output: </span><span class="si">{</span><span class="n">output</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">V_MAX</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

<h2>Misc.</h2>
<h3>Basic Calculator <code>bc</code>
</h3>
<p>This is a command line calculator and can be used for hexadecimal, binary, and decimal conversions. Install with <code>apt install bc</code>.</p>
<div class="code"><pre class="code literal-block"><span class="gp"># </span>Convert<span class="w"> </span>0x40<span class="w"> </span>to<span class="w"> </span>binary
<span class="go">echo "ibase=16; obase=2; 40" | bc</span>

<span class="gp"># </span>Convert<span class="w"> </span>0x40<span class="w"> </span>to<span class="w"> </span>decimal
<span class="go">echo "ibase=16; 40" | bc</span>
</pre></div>

<p><strong>Note that hexadecimal values must be uppercase, e.g. 0xC7, not 0xc7!</strong></p>
    </div>
    </article>
</div>

        <ul class="pager postindexpager clearfix">
<li class="next"><a href="index-1.html" rel="next">Older posts</a></li>
        </ul>
<script>var disqus_shortname="kyle-m-douglass";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
        </script><!--End of body content--><footer id="footer">
            Contents © 2025         <a href="mailto:kyle.m.douglass@gmail.com">Kyle M. Douglass</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
<a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<img alt="Creative Commons License BY-NC-SA" style="border-width:0; margin-bottom:12px;" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a>
            
            
        </footer>
</div>
</div>


        <script src="assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
