<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Optics, programming, and biophysics">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Kyle M. Douglass</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="rss.xml">
<link rel="canonical" href="https://kylemdouglass.com/">
<link rel="next" href="index-1.html" type="text/html">
<!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]--><link rel="prefetch" href="posts/data-type-alignment-for-ray-tracing-in-rust/" type="text/html">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark
bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href=".">

            <span id="blog-title">Kyle M. Douglass</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="rss.xml" class="nav-link">RSS feed</a>
                </li>
<li class="nav-item">
<a href="https://kmdouglass.github.io/" class="nav-link">Previous</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/data-type-alignment-for-ray-tracing-in-rust/" class="u-url">Data Type Alignment for Ray Tracing in Rust</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/data-type-alignment-for-ray-tracing-in-rust/" rel="bookmark">
            <time class="published dt-published" datetime="2025-02-24T08:40:00+01:00" itemprop="datePublished" title="2025-02-24 08:40">2025-02-24 08:40</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/data-type-alignment-for-ray-tracing-in-rust/#disqus_thread" data-disqus-identifier="cache/posts/data-type-alignment-for-ray-tracing-in-rust.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>I would like to clean up my 3D ray trace routines for my <a href="https://www.github.com/kmdouglass/cherry">Rust-based optical design library</a>. The proof of concept (PoC) is finished and I now I need to make the code easier to modify to better support the features that I want to add on the frontend. I suspect that I might be able to make some performance gains as well during refactoring. Towards this end, I want to take a look at my ray data type from the perspective of making it CPU cache friendly.</p>
<p>One of the current obstacles to adding more features to the GUI (for example color selection for different ray bundles) is how I handle individual rays. For the PoC it was fastest to add two additional fields to each ray to track where they come from and whether they are terminated:</p>
<div class="code"><pre class="code literal-block"><span class="k">struct</span> <span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">e</span>: <span class="p">[</span><span class="kt">f64</span><span class="p">;</span><span class="w"> </span><span class="mi">3</span><span class="p">],</span>
<span class="p">}</span>

<span class="k">struct</span> <span class="nc">Ray</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">pos</span>: <span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">dir</span>: <span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">terminated</span>: <span class="kt">bool</span><span class="p">,</span>
<span class="w">    </span><span class="n">field_id</span>: <span class="kt">usize</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>

<p>A ray is just two, 3-element arrays of floats that specify the coordinates of a point on the ray and its direction cosines. I have additionally included a boolean flag to indicate whether the ray has terminated, i.e. gone out-of-bounds of the system or failed to converge during calculation of the intersection point with a surface.</p>
<p>A ray fan is a collection of rays and is specified by a 3-tuple of wavelength, axis, and field; <code>field_id</code> really should not belong to an individual Ray because it can be stored along with the set of all rays for the current ray fan. I probably added it because it was the easiest thing to do at the time to get the application working.</p>
<h2>A deeper look into the Ray struct</h2>
<h3>Size of a ray</h3>
<p>Let's first look to see how much space the Ray struct occupies.</p>
<div class="code"><pre class="code literal-block"><span class="k">use</span><span class="w"> </span><span class="n">std</span>::<span class="n">mem</span><span class="p">;</span>

<span class="cp">#[derive(Debug)]</span>
<span class="k">struct</span> <span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">e</span>: <span class="p">[</span><span class="kt">f64</span><span class="p">;</span><span class="w"> </span><span class="mi">3</span><span class="p">],</span>
<span class="p">}</span>

<span class="cp">#[derive(Debug)]</span>
<span class="k">struct</span> <span class="nc">Ray</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">pos</span>: <span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">dir</span>: <span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">terminated</span>: <span class="kt">bool</span><span class="p">,</span>
<span class="w">    </span><span class="n">field_id</span>: <span class="kt">usize</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Size of ray: {:?}"</span><span class="p">,</span><span class="w"> </span><span class="n">mem</span>::<span class="n">size_of</span>::<span class="o">&lt;</span><span class="n">Ray</span><span class="o">&gt;</span><span class="p">());</span>
<span class="p">}</span>
</pre></div>

<p>The <code>Ray</code> struct occupies 64 bytes in memory. Does this make sense?</p>
<p>The sizes of the individual fields are:</p>
<table>
<thead><tr>
<th>Field</th>
<th>Size, bytes</th>
</tr></thead>
<tbody>
<tr>
<td>pos</td>
<td>24</td>
</tr>
<tr>
<td>dir</td>
<td>24</td>
</tr>
<tr>
<td>terminated</td>
<td>1</td>
</tr>
<tr>
<td>field_id</td>
<td>8*</td>
</tr>
</tbody>
</table>
<p><code>pos</code> and <code>dir</code> are each 24 bytes because they are each composed of three 64-bit floats and 8 bits = 1 byte. <code>terminated</code> is only one byte because it is a boolean. <code>field_id</code> is a <a href="https://doc.rust-lang.org/std/primitive.usize.html">usize</a>, which means that it depends on the compilation target. On 64-bit targets, such as x86_64, it is 64 bits = 8 bytes in size.</p>
<p>Adding the sizes in the above table gives 57 bytes, not 64 bytes as was output from the example code. Why is this?</p>
<h3>Alignment and padding</h3>
<p><a href="https://en.wikipedia.org/wiki/Data_structure_alignment">Alignment</a> refers to the layout of a data type in memory and how it is accessed. CPUs read memory in chunks that are equal in size to the <a href="https://en.wikipedia.org/wiki/Word_(computer_architecture)">word size</a>. Misaligned data is inefficient to access because the CPU requires more cycles than is necessary to fetch the data.</p>
<p>Natural alignment refers to the most efficient alignment of a data type for CPU access. To achieve natural alignment, a compiler can introduce padding between fields of a struct so that the memory address of a field or datatype is a multiple of the field's/data type's alignment.</p>
<p>As an example of misalignment, consider a 4-byte integer and that starts at memory address 5. The CPU has 32-bit memory words. To read the data, the CPU must:</p>
<ol>
<li>read bytes 4-7,</li>
<li>read bytes 8-11,</li>
<li>and combine the relevant parts of both reads to get the 4 bytes, i.e. bytes 5 - 8.</li>
</ol>
<p>Notice that we must specify the memory word size to determine whether a data type is misaligned.</p>
<p>Here is an important question: <strong>why can't the CPU just start reading from memory address 5?</strong> The answer, as far as I can tell, is that it just can't. This is not how the CPU, RAM, and memory bus are wired.</p>
<h3>Alignment in Rust</h3>
<p><a href="https://doc.rust-lang.org/reference/type-layout.html#size-and-alignment">Alignment in Rust</a> is defined as follows:</p>
<blockquote>
<p>The alignment of a value specifies what addresses are valid to store the value at. A value of alignment n must only be stored at an address that is a multiple of n.</p>
</blockquote>
<p><a href="https://doc.rust-lang.org/reference/type-layout.html#the-rust-representation">The Rust compiler only guarantees the following</a> when it comes to padding fields in structs:</p>
<blockquote>
<ol>
<li>The fields are properly aligned.</li>
<li>The fields do not overlap.</li>
<li>The alignment of the type is at least the maximum alignment of its fields.</li>
</ol>
</blockquote>
<p>So for my <code>Ray</code> data type, its alignment is 8 because the maximum alignment of its fields is 8 bytes. (<code>pos</code> and <code>dir</code> are composed of 8-byte floating point numbers). The addresses of its fields are:</p>
<div class="code"><pre class="code literal-block"><span class="k">fn</span> <span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">ray</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Ray</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">pos</span>: <span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">e</span>: <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">]</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="n">dir</span>: <span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">e</span>: <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">]</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="n">terminated</span>: <span class="nc">false</span><span class="p">,</span>
<span class="w">        </span><span class="n">field_id</span>: <span class="mi">0</span><span class="p">,</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="k">unsafe</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Address of ray.pos: {:p}"</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span>::<span class="n">addr_of</span><span class="o">!</span><span class="p">(</span><span class="n">ray</span><span class="p">.</span><span class="n">pos</span><span class="p">));</span>
<span class="w">        </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Address of ray.dir: {:p}"</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span>::<span class="n">addr_of</span><span class="o">!</span><span class="p">(</span><span class="n">ray</span><span class="p">.</span><span class="n">dir</span><span class="p">));</span>
<span class="w">        </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Address of ray.terminated: {:p}"</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span>::<span class="n">addr_of</span><span class="o">!</span><span class="p">(</span><span class="n">ray</span><span class="p">.</span><span class="n">terminated</span><span class="p">));</span>
<span class="w">        </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Address of ray.field_id: {:p}"</span><span class="p">,</span><span class="w"> </span><span class="n">ptr</span>::<span class="n">addr_of</span><span class="o">!</span><span class="p">(</span><span class="n">ray</span><span class="p">.</span><span class="n">field_id</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>

<p>I got the following results, which will vary from system-to-system and probably run-to-run:</p>
<div class="code"><pre class="code literal-block"><span class="go">Address of ray.pos: 0x7fff076c6b50</span>
<span class="go">Address of ray.dir: 0x7fff076c6b68</span>
<span class="go">Address of ray.terminated: 0x7fff076c6b88</span>
<span class="go">Address of ray.field_id: 0x7fff076c6b80</span>
</pre></div>

<p>So the <code>pos</code> field comes first at address <code>0x6b50</code> (omitting the most significant hexadecimal digits). Then, 24 bytes later, comes <code>dir</code> at address <code>0x6b68</code>. Note that the difference is hexadecimal 0x18, which is decimal 16 + 8 = 24! So <code>pos</code> really occupies 24 bytes like we previously calculated.</p>
<p>Next comes <code>field_id</code> and not <code>terminated</code>. It is <code>0x6b80 - 0x6b68 = 0x0018</code>, or 24 bytes after <code>dir</code> like before. So far we have no padding, but the compiler did swap the order of the fields. Finally, <code>terminated</code> is 8 bytes after <code>field_id</code> because <code>field_id</code> is 8-byte aligned. This means that the Rust compiler must have placed 7 bytes of padding after the <code>terminated</code> field.</p>
<h2>What makes a good data type?</h2>
<p>As I mentioned, I already know that <code>field_id</code> shouldn't belong to the ray for reasons related to data access by the programmer. So the reason for removing it from the <code>Ray</code> struct is not related to performance. But what about the <code>terminated</code> bool? Well, in this case, it's resulting in 7 extra bytes of padding for each ray!</p>
<div class="code"><pre class="code literal-block"><span class="cp">#[derive(Debug)]</span>
<span class="k">struct</span> <span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">e</span>: <span class="p">[</span><span class="kt">f64</span><span class="p">;</span><span class="w"> </span><span class="mi">3</span><span class="p">],</span>
<span class="p">}</span>

<span class="cp">#[derive(Debug)]</span>
<span class="k">struct</span> <span class="nc">Ray</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">pos</span>: <span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">dir</span>: <span class="nc">Vec3</span><span class="p">,</span>
<span class="w">    </span><span class="n">terminated</span>: <span class="kt">bool</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">let</span><span class="w"> </span><span class="n">ray</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Ray</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">pos</span>: <span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">e</span>: <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">]</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="n">dir</span>: <span class="nc">Vec3</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">e</span>: <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">]</span><span class="w"> </span><span class="p">},</span>
<span class="w">        </span><span class="n">terminated</span>: <span class="nc">false</span><span class="p">,</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="fm">println!</span><span class="p">(</span><span class="s">"Size of ray: {:?}"</span><span class="p">,</span><span class="w"> </span><span class="n">mem</span>::<span class="n">size_of</span>::<span class="o">&lt;</span><span class="n">Ray</span><span class="o">&gt;</span><span class="p">());</span>
<span class="p">}</span>
</pre></div>

<p>This program prints <code>Size of ray : 56</code>, but 24 + 24 + 1 = 49. In both versions we waste 7 bytes.</p>
<h3>Fitting a Ray into the CPU cache</h3>
<p>Do I have a good reason to remove <code>terminated</code> from the <code>Ray</code> struct because it wastes space? Consider the following:</p>
<p>We want as many <code>Ray</code> instances as possible to fit within a CPU cache line if we want to maximize performance. (Note that I'm not saying that we necessarily want to maximize performance because that comes with tradeoffs.) Each CPU core on my AMD Ryzen 7 has a 64 kB L1 cache with 64 byte cache lines. This means that I can fit only 1 of the current version of <code>Ray</code> into each cache line for a total of 64 kB / 64 bytes = 1024 rays maximum in the L1 cache of each core. If I remove <code>field_size</code> and <code>terminated</code>, then the size of a ray becomes 48 bytes. Unfortunately, this means that only one <code>Ray</code> instance fits in a cache line, just as before with a 64 byte <code>Ray</code>.</p>
<p>But, if I also reduce my precision to 32-bit floats, then the size of a <code>Ray</code> becomes 6 * 4 = 24 bytes and I have doubled the number of rays that fit in L1 cache.</p>
<p>Now what if I reduced the precision but kept <code>terminated</code>? Then I get 6 * 4 + 8 = 32 bytes per Ray and I still have 2 rays per cache line.</p>
<p>I conclude that there is no reason to remove <code>terminated</code> for performance reasons. Reducing my floating point precision would produce a more noticeable effect on the cache locality of the <code>Ray</code> data type.</p>
<h2>Does all of this matter?</h2>
<p>My Ryzen 7 laptop can trace about 600 rays through 3 surfaces in 3.8 microseconds with Firefox, Slack, and Outlook running. At this point, I doubt that crafting my data types for cache friendliness is going to offer a significant payoff. Creating data types that are easy to work with is likely more important.</p>
<p>I do think, however, that it's important to understand these concepts. If I do need to tune the performance in the future, then I know where to look.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/an-analog-led-dimmer-circuit/" class="u-url">An Analog LED Dimmer Circuit</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/an-analog-led-dimmer-circuit/" rel="bookmark">
            <time class="published dt-published" datetime="2025-01-17T09:08:27+01:00" itemprop="datePublished" title="2025-01-17 09:08">2025-01-17 09:08</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/an-analog-led-dimmer-circuit/#disqus_thread" data-disqus-identifier="cache/posts/an-analog-led-dimmer-circuit.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>I recently needed to build a circuit to control the brightness of a 4 W LED with a knob. I know basic electronics, and I thought this would be easy. I spoke to a few people whom I know and are knowledgable in electronics. I also asked people on Reddit. A lot of people said it would be easy.</p>
<p>As it turns it, it wasn't easy.</p>
<h2>The Requirements</h2>
<p>My requirments are simple:</p>
<ul>
<li>The brightness should be manually adjustable with a knob from OFF to nearly full ON.</li>
<li>The LED will serve as the light source of a microscope trans-illuminator. It should work across a large range of frame acquisition rates (1 Hz to 1 kHz, or exposure times of 1 ms to 1 s).</li>
<li>The range of brightnesses should be variable across the dynamic range of the camera, which in my case is 35,000:1, or about 90 dB.</li>
</ul>
<p>I don't care about efficiency. I don't care about whether I can use a Raspberry Pi to control it. I don't care whether it can be turned on or off with different logic levels. I just want a knob that I can turn to make the LED brighter or dimmer.</p>
<p>In spite of the insistence of several people that I communicated with on the internet, I decided that the second requirement would preclude using pulse width modulation (PWM) to dim the LED. Even when I could convince others that PWM almost always causes aliasing at high frame rates, they tried to find obscure work arounds so I could still use PWM. I really do appreciate all the feedback I got. But I also learned that PWM is the hammer of the electronics world that makes everything look like a nail<sup id="fnref:1"><a class="footnote-ref" href="posts/an-analog-led-dimmer-circuit/#fn:1">1</a></sup>.</p>
<h2>The Circuit</h2>
<p>I reached out to a friend of mine who's a wizard at analog electronics<sup id="fnref:2"><a class="footnote-ref" href="posts/an-analog-led-dimmer-circuit/#fn:2">2</a></sup>. He suggested to use a MOSFET and to vary the gate-source voltage to control the current through the LED.</p>
<p>After a lot of thinking and reading, I arrived at <a href="https://tinyurl.com/2cnw45fy">the following circuit</a>:</p>
<figure><img src="images/led-dimmer-circuit-v0.png"></figure><p>The LED is an Osram Oslon star LED (LST1-01F05-4070-01) with a maximum current of 1.3 A and a maximum forward voltage of 3.2 V. The MOSFET is an IRF510, whose gate-source threshold voltage is about 3 V.</p>
<p>Here's a brief explanation of what each component does:</p>
<ol>
<li>
<strong>Voltage source</strong> : This is just a 12 V wall wart.</li>
<li>
<strong>200 nF capacitor</strong> : This smooths out any fluctuations from the wall wart.</li>
<li>
<strong>50 kOhm potentiometer</strong> : The "knob." Turning it will vary the gate-source voltage of the MOSFET, which controls how much current flows through the LED.</li>
<li>
<strong>50 kOhm resistor</strong> : This along with the potentiometer forms a voltage divider to keep the minimum voltage at the MOSFET gate close to where the LED turns on. Without it, you need to rotate the potentiometer almost half of its full range for the LED to turn on.</li>
<li>
<strong>300 nF capacitor</strong> : A debounce capacitor that smooths out the mechanical irregularities of the pot when it turns.</li>
<li>
<strong>IRF510 MOSFET</strong> : Basically a valve that I can vary continuously to control the LED current by setting the voltage at the gate.</li>
<li>
<strong>LED</strong> : So pretty.</li>
<li>
<strong>10 Ohm resistor</strong> : This limits the current through the resistor. I calculated its value by dividing the maximum supply voltage minus the maximum forward voltage drop across the LED by the maximum current, then rounded up for safety.</li>
</ol>
<p>$$ R = \frac{V}{I} = \frac{\left( 12 \, V - 3.2 \, V \right) }{1.3 \, A} = 6.8 \Omega $$</p>
<p>The resistor also has to handle a large power dissipation at the maximum current:</p>
<p>$$ P = I^2 R = \left(1.3 \, A \right)^2 \left( 10 \Omega \right) = 16.9 W $$</p>
<p>I decided instead to keep the current to less than 1 Amp so that I could use a 10 Watt resistor that I had.</p>
<p>Power dissipation is also why we don't just use a potentiometer to control the LED current: my pots were only rated up to about 50 mW, whereas I expected that the MOSFET would have handle loads on the order of Watts due to the high current.</p>
<h2>What I Learned</h2>
<h3>I really need to study MOSFETS</h3>
<p>I still don't really know how to solve circuits with MOSFETs. I arrived at the above circuit largely by trial-and-error on a prototype and by performing naive calculations on the voltage divider that turned out to not be entirely correct. I also expected that the LED would turn on once I passed the MOSFET's gate-source voltage threshold, but this turned out to be off by about 2 or 3 V.</p>
<h3>MOSFETs suffer from second order effects</h3>
<p>There is currently a hysteresis in the gate-ground voltage at when the LED turns on and when it turns off by about half a Volt. According to a helpful person on Reddit, this is likely due to a change in both the LEDs forward voltage and the MOSFET's threshold voltage with temperature once the current starts flowing. A possible fix is to swap the order of the LED and the MOSFET so that only the MOSFET will contribute to the hysteresis.</p>
<h3>You can always complicate things to make them better</h3>
<p>The same person on Reddit above also suggested making the circuit robust to temperature variations by adding an opamp to control the MOSFET gate voltage. It would compensate for temperature changes by comparing the potentiometer value to the 10 Ohm resistor in a closed feedback loop.</p>
<h3>Electronics is an art</h3>
<p>Yes, electronics is a science, but I would argue that having to mentally juggle second order effects and the fact that experts seem to make an initial design "by feel" are signatures of an art.</p>
<p>It also struck me how nearly every step of the process forced me to take a detour to address something I hadn't at first considered, such as current limits in the wires and large variations in the MOSFET specs.</p>
<p>The next time I need to do something like this, I will expect the problem to take longer to solve than I first anticipate.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>To my non-native English speaking readers: I mean that people try to use PWM to solve problems where it's not appropriate. <a class="footnote-backref" href="posts/an-analog-led-dimmer-circuit/#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p>And if you play electric guitar, be sure to check out his handmade effects pedals: <a href="https://www.volumeandpower.com/">https://www.volumeandpower.com/</a>. <a class="footnote-backref" href="posts/an-analog-led-dimmer-circuit/#fnref:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/meta-biophysics-of-the-cell/" class="u-url">Meta-Biophysics of the Cell</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/meta-biophysics-of-the-cell/" rel="bookmark">
            <time class="published dt-published" datetime="2025-01-14T17:07:30+01:00" itemprop="datePublished" title="2025-01-14 17:07">2025-01-14 17:07</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/meta-biophysics-of-the-cell/#disqus_thread" data-disqus-identifier="cache/posts/meta-biophysics-of-the-cell.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>I work in a biophysics lab applying microscopy techniques to study cell biology. I am not a biologist, and I was not trained as one. I therefore have had to develop heuristics to help me understand what cell biologists do and to communicate with them effectively.</p>
<p>In this post, I present these heuristics as a sort of model for how I think that cell biologists think. I collectively call them "Meta-Biophysics of the Cell" because:</p>
<ol>
<li>they model the models of cell biologists, hence they are a meta-model,</li>
<li>they are inspired by physics and quantitative modeling, and</li>
<li>I limit myself to cellular processes and not, for example, in vitro or organismal studies.</li>
</ol>
<p>Furthermore, they are heavily biased by my work in microscopy.</p>
<p>These heuristics may very well be wrong. I do not pretend to understand biology nearly as well as a biologist. If you think I am wrong, please do not hesitate to leave a comment and explain why.</p>
<p>They also are certainly not complete. I present here only what I think the most important heuristics are.</p>
<h2>Biologists want to know distributions of proteins across space and time</h2>
<p>Cell biology is concerned with understanding the structure and behavior of cells as complex phenomena that emerge from the interactions of molecules. The types of these molecules may be proteins, nucleic acids, lipids, or some other type. I will use the term "protein" generally because </p>
<ol>
<li>it's easier than always listing all the types of molecules,</li>
<li>it's more precise than just saying "molecule"</li>
<li>there are at least 10,000 different types of proteins in the human cell, making proteins a core building block of the cell, and</li>
<li>these heuristics do not change if you substitute another type of molecule for the word "protein."</li>
</ol>
<p>Within a single cell, we can model the number of proteins at a point in space and a point in time with a function \( N \left( \mathbf{r}, t \right) \). This is a spatiotemporal distribution for protein number density. Its dimensions are in numbers of proteins per unit of volume.</p>
<p>A single protein can be represented as a point in space and time, such as a delta function:</p>
<p>$$ N \left( \mathbf{r}, t \right) = \delta \left( \mathbf{r}, t \right) $$</p>
<p>Of course a real protein occupies some volume and is not a point, but at the level of a cell I think that this is an adequate model for what cell biologists want to know about protein distributions.</p>
<p>But wait. There are more than ten thousand types of proteins within the cell. Some proteins are of high abundance, and some are very rare. So it is not sufficient merely to count proteins in space and time: we also need to identify their types. For this, I assign a unique ID to each type that I call \( s \) for "species". Our model now becomes a function of another variable, one that is categorical rather than continuous:</p>
<p>$$ N \left( \mathbf{r}, t ; s\right) $$</p>
<p>Below I show a simplified schematic of the volume that this model occupies. It is simplified because I show only one spatial dimension (otherwise it would be a five dimensional hypervolume). I saw a figure like this once in a paper about a decade ago, but sadly I cannot find it to credit it. (<em>Update 2025-01-30: The paper referred to is <a href="https://doi.org/10.1016/j.cell.2007.08.031">Megason and Fraser, "Imaging in Systems Biology," Cell 130(5), 784-795 (2007)</a></em>)</p>
<figure><img src="images/protein-distributions.png"><figcaption>A cell can be represented as a "volume" of spatiotemporal distributions where one spatiotemporal slice belongs to each protein species.</figcaption></figure><p>The \( x \) and \( t \) dimensions are continuous; the \( s \) dimension is a discrete, categorical variable. Each \( s \) slice is the spatiotemporal distribution of that protein within a single cell. Each point in the volume is the protein density for that point in space and time and for that species of protein.</p>
<p>Individual proteins might also vary amongst themselves as in, for example, post translation modifications. This is not really a problem for the above model because we can use a different value for the \( s \) of each variant.</p>
<h3>Creation and degradation of a protein</h3>
<p>The appearance and disappearance of a protein is modeled as a non-zero value over a time range from \( t_0 \) to \( t_1 \). For example, a single protein existing over a finite time interval may be expressed as</p>
<p>$$\begin{eqnarray}
 &amp;&amp;\delta \left( \mathbf{r}, t \right), \, t_0 &lt; t &lt; t_1 \\
 &amp;&amp;0, \, \text{otherwise}
\end{eqnarray}$$</p>
<h3>Biologists can only measure slices of protein distributions</h3>
<p>Look again at the figure above. Remember that there are in fact five dimensions. Can any one experimental technique measure the whole hypevolume?</p>
<p>No. Instead, biologists can measure slices from the volume and try to piece together a complete picture of \( N \) from individual measurements.</p>
<p>For example, fluorescence microscopy can measure proteins in space and time. Sometimes it can measure in 3 spatial dimensions, but it is easier to measure in 2. Unfortunately, it can only measure a small of number of protein species relative to all the proteins that are in the cell. These would correspond to the different fluorescence channels of the measurement. Thus, fluorescence microscopy provides a slice of the volume that looks like the following:</p>
<figure><img src="images/protein-distributions-microscopy-slice.png"><figcaption>Fluorescence microscopy measures a small slice of the spatiotemporal protein distribution that represents a cell.</figcaption></figure><p>Other types of measurements, such as those in single cell proteomics, might measure a large number of proteins but cannot resolve them in space and time. They would slice the volume like this:</p>
<figure><img src="images/protein-distributions-proteomics-slice.png"></figure><p>So after their measurements, biologists are always going to be left with less than the total amount of information contained within a single cell because they can only measure slices of \( N \).</p>
<h2>Organelles are mutually exclusive slices of protein distributions in space</h2>
<p>Consider a mitochondrion. It is a membrane-bound organelle. Everything inside the mitochondrion is considered part of the organelle, and everything outside it is not. An organelle is therefore a mutually-exclusive slice through the volume dimensions of \( N \).</p>
<p>The slices are mutually-exclusive because two mitochondria cannot occupy the same volume at the same time and have a distinct identity.</p>
<p>For non-membrane bound organelles, keep reading.</p>
<h3>Organelles contain many types proteins</h3>
<p>If we use the definition of an organelle as a mutually-exclusive slice through the volume dimensions of \( N \), then we can look sideways along the \( s \) dimension to find all the proteins that belong to the organelle. If the distribution for protein \( s_i \) is non-zero inside an organelle's volume at a given time, then it belongs to the organelle. The set of all proteins within the volume slice at a given time constitute the organelle.</p>
<h3>Knowing \( N \) doesn't by itself tell us what is and is not an organelle</h3>
<p>Membrane-bound organelles are easy to identify because of their structure. Other sets of proteins within a given volume may or may not form an organelle. In these situations, we might look at their function instead to decide whether the volume is or is not occupied by an organelle.</p>
<p>For example, organelles like the centrosome have a diffuse, pericentriolar material that surround them. In this case, the border defining what is and isn't inside the centrosome is likely to be somewhat arbitrary.</p>
<h2>Cause and Effect is the probability of one protein distribution given another</h2>
<p>At this time, I am much less certain about how function and causal relationships fit within this model. It is nevertheless important because biologists are deeply interested in the function of proteins and other complexes. To a rough approximation, I would say that cause and effect describes how the spatiotemporal distributions of a subset of protein species can serve as a predictor of another distribution at a later time. In other words, we can assign a probability to a certain distribution given another one.</p>
<p>I would guess that not every possible set proteins is linked by causal relationships. This would mean that the limitations that come from being able to sample slices of the protein distribution hypervolume are not so significant. You would then want choose your measurements so that you slice the volume to include only the species that are causally linked for the phenomenon that you are studying.</p>
<p>As a consequence of this, the causal links between distributions are likely more important than knowing \( N \). I doubt that we can have a satisfactory understanding of the cell if we could exhaustively measure \( N \) for even a single cell.</p>
<h3>Interactions between proteins require spatial colocalization</h3>
<p>Protein-protein interactions occur on length-scales on the order of the size of individual proteins. For two different proteins to "interact" we require that they be colocated less than this distance. Colcalization means that two proteins are located less than the distance required for an interaction to occur.</p>
<p>Furthermore, colocalization is necessary but not sufficient for an interaction. A real interaction involves the chemistry between the two different species.</p>
<h2>Summary</h2>
<p>In summary, my main three heuristics for meta-biophysics of the cell are:</p>
<ol>
<li>Biologists want to know distributions of proteins across space and time</li>
<li>Organelles are mutually exclusive slices of protein distributions in space</li>
<li>Cause and Effect is the probability of one or more protein distributions given another</li>
</ol>
<p>I find that nearly all the problems that the cell biologists that I work with can be reformulated into this language. I emphasize again that the "real" science is being done by the biologists, and I in no way mean to diminish the complexity of their work. These heuristics are merely a tool that I use to understand what they are doing when I myself am unfamiliar with their jargon and mental models.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/coordinate-systems-for-modeling-microscope-objectives/" class="u-url">Coordinate Systems for Modeling Microscope Objectives</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/coordinate-systems-for-modeling-microscope-objectives/" rel="bookmark">
            <time class="published dt-published" datetime="2024-11-21T10:52:48+01:00" itemprop="datePublished" title="2024-11-21 10:52">2024-11-21 10:52</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/coordinate-systems-for-modeling-microscope-objectives/#disqus_thread" data-disqus-identifier="cache/posts/coordinate-systems-for-modeling-microscope-objectives.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>A common model for infinity corrected microscope objectives is that of an aplanatic and telecentric optical system. In many developments of this model, emphasis is placed upon the calculation of the electric field near the focus. However, this has the effect that the definition of the coordinate systems and geometry are conflated with the determination of the fields. In addition, making the model amenable to computation often occurs as an afterthought.</p>
<p>In this post I will explore the geometry of an aplanatic system for modeling high NA objectives with an emphasis on computational implementations. My approach follows Novotny and Hecht<sup id="fnref:1"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:1">1</a></sup> and Herrera and Quinto-Su<sup id="fnref:2"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:2">2</a></sup>.</p>
<h2>The Model Components</h2>
<p>The model system is illustrated below:</p>
<figure><img src="images/aplanatic-telecentric-system.png"><figcaption>A high NA, infinity corrected microscope objective as an aplanatic and telecentric optical system.
</figcaption></figure><p>In this model, we abstract over the details of the objective by representing it as four surfaces:</p>
<ol>
<li>A back focal plane containing an aperture stop</li>
<li>A back principal plane, \( P \)</li>
<li>A front principal surface, \( P' \)</li>
<li>A front focal plane</li>
</ol>
<p>The space to the left of the back principal plane is called the infinity space. The space to the right of the front principal surface is called the sample space.</p>
<p>We let the infinity space refractive index \( n_1 = 1 \) because it is in air. The refractive index \( n_2 \) is the refractive index of the immersion medium.</p>
<p>The unit vectors \( \mathbf{n} \) are not used in this discussion; they are relevant for computing the fields.</p>
<h3>Assumptions</h3>
<p>We make one assumption: the system obeys the sine condition. The meaning of this will be explained later.</p>
<p>An aplanatic system is one that obeys the sine condition.</p>
<p>We will not assume the intensity law to conserve energy because it is only necessary when computing the electric field near the focus.</p>
<h3>The Aperture Stop and Back Focal Plane</h3>
<p>The aperture stop (AS) of an optical system is the element that limits the angle of the marginal ray.</p>
<p>The system is telecentric because the aperture stop is located in the back focal plane (BFP). We can shape the focal field by spatially modulating any of the amplitude, phase, or polarization of the incident light in a plane conjugate to the BFP.</p>
<h3>The Back Principal Plane</h3>
<p>This is the plane in infinity space at which rays appear to refract. It is a plane because rays coming from a point in the front focal plane all emerge into the infinity space in the same direction.</p>
<p>Strictly speaking, focus field calculations require us to propagate the field from the AS to the back principal plane before computing the Debye diffraction integral, but this step is often omitted<sup id="fnref:3"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:3">3</a></sup>. The assumptions of paraxial optics should hold here.</p>
<h3>The Front Principal Surface</h3>
<p>The front principal surface is the surface at which rays appear to refract in the sample space. It is a surface because</p>
<ol>
<li>this is a non-paraxial system, and</li>
<li>we assumed the sine condition.</li>
</ol>
<p>The sine condition states that refraction of a ray coming from an on-axis point in the front focal plane occurs on a spherical cap centered upon the focal point. The distance from the optical axis of the point of intersection of the ray with the surface is proportional to the sine of the angle that the ray makes with the axis.</p>
<p>The principal surface is in the far field of the electric field coming from the focal region. For this reason, we can represent a point on this surface as representing a single ray or a plane wave<sup id="fnref2:1"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:1">1</a></sup>.</p>
<h3>The Front Focal Plane</h3>
<p>This plane is located a distance \( n_2 f \) from the principal surface<sup id="fnref:4"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:4">4</a></sup>. It is not at a distance \( f \) from this surface. This is a result of imaging in an immersion medium.</p>
<h2>Geometry and Coordinate Systems</h2>
<h3>The Aperture Stop Radius</h3>
<p>The aperture stop radius \( R \) corresponds to the distance from the axis to the point where the marginal ray intersects the front prinicpal surface. In the sample space, the marginal ray travels at an angle \( \theta_{max} \) with respect to the axis.</p>
<p>Under the sine condition, this height is</p>
<p>$$ R = n_2 f \sin{ \theta_{max} } = f \, \text{NA} $$</p>
<p>The right-most expression uses the definition of the numerical aperture \( \text{NA} \equiv n \sin{ \theta_{max} } \).</p>
<p>Compare this result to the oft-cited expression for the entrance pupil diameter of an objective lens: \( D = 2 f \, \text{NA} \). They are the same. This makes sense because an entrance pupil is either</p>
<ol>
<li>an image of an aperture stop, or</li>
<li>a physical stop.</li>
</ol>
<h3>The Back Principal Plane</h3>
<p>There are two independent coordinate systems in the back principal plane:</p>
<ol>
<li>the spatial coordinate system defining the far field positions \( \left( x_{\infty} , y_{\infty} \right) \), and</li>
<li>the coordinate system of the angular spectrum of plane waves \( \left( k_x, k_y \right) \).</li>
</ol>
<h4>The Far Field Coordinate System</h4>
<p>The far field coordinate system may be written in Cartesian form as \( \left( x_{\infty} , y_{\infty} \right) \). It also has a cylindrical representation as</p>
<p>$$\begin{eqnarray}
\rho &amp;=&amp; \sqrt{x_{\infty}^2 + y_{\infty}^2} \\
\phi &amp;=&amp; \arctan \left( \frac{y_{\infty}}{x_{\infty}} \right)
\end{eqnarray}$$</p>
<p>The cylindrical representation appears to be preferred in textbook developments of the model. The Cartesian representation is likely preferred for computational models because it works naturally with two-dimensional arrays of numbers, and because beam shaping elements such as spatial light modulators are rectangular arrays of pixels<sup id="fnref2:2"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:2">2</a></sup>.</p>
<h4>The Angular Spectrum Coordinate System</h4>
<p>Each point in the angular spectrum coordinate system represents a plane wave in the sample space that is traveling at an angle \( \theta \) to the axis according to:</p>
<p>$$\begin{eqnarray}
k_x &amp;=&amp; k \sin \theta \cos \phi \\
k_y &amp;=&amp; k \sin \theta \sin \phi \\
k_z &amp;=&amp; k \cos \theta
\end{eqnarray}$$</p>
<p>where \( k = 2 \pi n_2 / \lambda = n_2 k_0 \).</p>
<p>Along the y-axis ( \( x_{\infty} = 0 \) ), the maximum value of \( k_y \) is \(n_2 k_0 \sin \theta_{max} = k_0 \, \text{NA} \).</p>
<p>Substitute in the expression \( \text{NA} = R / f \) and we get \(k_{y, max} = k_0 R / f\). But \( R = y_{\infty, max} \). This (and similar reasoning for the x-axis) implies that:</p>
<p>$$\begin{eqnarray}
k_x &amp;=&amp; k_0 x_{\infty} / f \\
k_y &amp;=&amp; k_0 y_{\infty} / f
\end{eqnarray}$$</p>
<p>The above equations link the angular spectrum coordinate system to the far field coordinate system. They are no longer independent once \( f \) and \( \lambda \) are specified.</p>
<h2>Numerical Meshes</h2>
<p>There are four free parameters for defining the coordinate systems of the numerical meshes:</p>
<ol>
<li>The numerical aperture, \( \text{NA} \)</li>
<li>The wavelength, \( \lambda \)</li>
<li>The focal length, \( f \)</li>
<li>The linear mesh size, \( L \)</li>
</ol>
<p>Below is a figure that illustrates the construction of the meshes. Both the far field and angular spectrum coordinate systems are represented by a \( L \times L \) array. \( L = 16 \) in the figure below. In general the value of \( L \) should be a power of 2 to help ensure the efficiency of the Fast Fourier Transform (FFT). By considering only powers of 2, we need only consider arrays of even size as well.</p>
<figure><img src="images/pupil-function-simulation-mesh.png"><figcaption>A numeric mesh representing the far field and angular spectrum coordinate systems of a microscope objective. Fields are sampled at the center of each mesh pixel.</figcaption></figure><p>The fields are defined on a region of circular support that is centered on this array. The radius of the domain of the far field coordinate system is \( f \text{NA} \); the radius of the domain of the angular spectrum coordinate system is \( k_0 \text{NA} \).</p>
<p>The boxes that are bound by the gray lines indicate the location of each field sample. The \( \left( x_{\infty} , y_{\infty} \right) \) and the \( \left( k_x, k_y \right) \) coordinate systems are sampled at the center of each gray box. The origin is therefore not sampled, which will help avoid division by zero errors when the fields are eventually computed.</p>
<p>The figure suggests that we could create only one mesh and scale it by either \( f \text{NA} \) or \( k_0 \text{NA} \) depending on which coordinate system we are working with. The normalized coordinates become \( \left( x_{\infty} / \left( f \text{NA} \right), y_{\infty} / \left( f \text{NA} \right) \right) \) and \( \left( k_x / \left( k_0 \text{NA} \right), k_y / \left( k_0 \text{NA} \right) \right) \).</p>
<h3>1D Mesh Example</h3>
<p>As an example, let \( L = 16 \). To four decimal places, the normalized coordinates are \( -1.0000, -0.8667, \ldots, -0.0667, 0.0667, \ldots, 0.8667, 1.0000 \).</p>
<p>The spacing between array elements is \( 2 / \left( L - 1 \right) = 0.1333 \). Note that 0 is not included in the 1D mesh as it goes from -0.0667 to 0.0667.</p>
<p>A 2D mesh is easily constructed from the 1D mesh using tools such as NumPy's <a href="https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html">meshgrid</a>.</p>
<h3>Back Principal Plane Mesh Spacings</h3>
<p>In the x-direction, the mesh spacing of the far field coordinate system is</p>
<p>$$ \Delta x_{\infty} = 2 R / \left( L - 1 \right) = 2 f \text{NA} / \left( L - 1 \right) $$</p>
<p>In the \( k_x \)-direction, the mesh spacing of the angular spectrum coordinate system is</p>
<p>$$ \Delta k_x = 2 k_{max} / \left( L - 1 \right) = 2 k_0 \text{NA} / \left( L - 1 \right) $$</p>
<p>Note the symmetry between these two expressions. One scales with \( f \text{NA} \) and the other \( k_0 \text{NA} \). Recall that these are free parameters of the model.</p>
<h3>Sample Space Mesh Spacing</h3>
<p>It is interesting to compute the spacing between mesh elements \( \Delta x \) in the sample space when the fields are eventually computed.</p>
<p>The sampling angular frequency in the sample space is \( k_S = 2 \pi / \Delta x \).</p>
<p>The Nyquist-Shannon sampling theory states that the maximum informative angular frequency is \( k_{max} = k_S / 2 \).</p>
<p>From the previous section, we know that \( k_{max} = \left(L - 1 \right) \Delta k_x / 2 \), and that \( \Delta k_x = 2 k_0 \text{NA} / \left( L - 1 \right) \).</p>
<p>Combining all the previous expressions and simplifying, we get:</p>
<p>$$\begin{eqnarray}
k_S &amp;=&amp; 2 k_{max} \\
2 \pi / \Delta x &amp;=&amp; \left(L - 1 \right) \Delta k_x \\
2 \pi / \Delta x &amp;=&amp; \left(L - 1 \right) \left[ 2 k_0 \text{NA} / \left( L - 1 \right) \right] \\
2 \pi / \Delta x &amp;=&amp; \left(L - 1 \right) \left[ 2 \left(2 \pi / \lambda \right) \text{NA} / \left( L - 1 \right) \right]
\end{eqnarray}$$</p>
<p>Solving the above expression for \( \Delta x \), we arrive at</p>
<p>$$ \Delta x = \frac{\lambda}{2 \text{NA}} $$</p>
<p>which is of course the Abbe diffraction limit.</p>
<h3>Effect of not Sampling the Origin</h3>
<p>Herrera and Quinto-Su<sup id="fnref3:2"><a class="footnote-ref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fn:2">2</a></sup> point out that an error will be introduced if we naively apply the FFT to compute the field components in the \( \left( k_x, k_y \right) \) coordinate system because the origin is not sampled, whereas the FFT assumes that we sample the zero frequency component. The effect is that the result of the FFT has a constant phase error that accounts for a half-pixel shift in each direction of the mesh.</p>
<p>Consider again the 1D mesh example with \(L = 16 \): \( -1.0000, -0.8667, \ldots, -0.0667, 0.0667, \ldots, 0.8667, 1.0000 \)</p>
<p>In Python and other languages that index arrays starting at 0, the origin is located at \(L / 2 - 0.5 \), i.e. halfway between the samples at index 7 and 8. A lateral shift in Fourier space is equivalent to a phase shift in real space:</p>
<p>$$ \phi_{shift} \left(X, Y \right) =  -j 2 \pi \frac{0.5}{L} X - j 2 \pi \frac{0.5}{L} Y $$</p>
<p>where \( X \) and \( Y \) are normalized coordinates.</p>
<p>At this point, I am uncertain whether the phasor with the above argument needs to be multiplied or divided with the result of the FFT because 1. there are a few typos in the signs for the coordinate system bounds in the manuscript of Herrera and Quinto-Su, and 2. the correction was developed for use in MATLAB, which indexes arrays starting at 1. Once the fields are computed, it would be easy to verify the correct sign of the phase terms following the procedure outlined in Figure 3 of Herrera and Quinto-Su's manuscript.</p>
<h3>Structure of the Algorithm</h3>
<p>The algorithm to compute the focus fields will proceed as follows:</p>
<ol>
<li>(optional) Propgate the inputs fields from the AS to the back principal plane using paraxial wave propagation</li>
<li>Input the sampled fields in the back principal plane in the \( \left( x_{\infty}, y_{\infty} \right) \) coordinate system</li>
<li>Transform the fields to the \( \left( k_x, k_y \right) \) coordinate system</li>
<li>Compute the fields in the \( \left(x, y, z \right) \) coordinate system using the FFT</li>
</ol>
<h2>Additional Remarks</h2>
<ul>
<li>Zero padding the mesh will increase the sample space resolution beyond the Abbe limit, but since the fields remain zero outside of the support, no new information is added.</li>
<li>On the other hand, zero padding might be required when computing fields going from the sample space to the back principal plane to faithfully reproduce any evanescent components.</li>
<li>Separating the coordinate system and mesh construction from the calculation of the fields reveals that the two assumptions of the model belong separately to each part. The sine condition is used in the construction of the coordinate systems, whereas energy conservation is used when computing the fields.</li>
<li>This post did not explain how to compute the fields.</li>
<li>Herrera and Quinto-Su (and possibly also Novotny and Hecht) appear to use an "effective" focal length which can be obtained by multiplying the one that I use by the sample space refractive index. I prefer my formulation because it is consistent with geometric optics and the well-known expression for the diameter of an objective's entrance pupil. When the fields are calculated, however, I do not yet know whether the arguments of the phasors of the Debye integral will require modification.</li>
</ul>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>Lukas Novotny and Bert Hecht, "Principles of Nano-Optics," Cambridge University Press (2006). <a href="https://doi.org/10.1017/CBO9780511813535">https://doi.org/10.1017/CBO9780511813535</a> <a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:1" title="Jump back to footnote 1 in the text">↩</a><a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref2:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p>Isael Herrera and Pedro A. Quinto-Su, "Simple computer program to calculate arbitrary tightly focused (propagating and evanescent) vector light fields," arXiv:2211.06725 (2022). <a href="https://doi.org/10.48550/arXiv.2211.06725">https://doi.org/10.48550/arXiv.2211.06725</a> <a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:2" title="Jump back to footnote 2 in the text">↩</a><a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref2:2" title="Jump back to footnote 2 in the text">↩</a><a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref3:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:3">
<p>Marcel Leutenegger, Ramachandra Rao, Rainer A. Leitgeb, and Theo Lasser, "Fast focus field calculations," Opt. Express 14, 11277-11291 (2006). <a href="https://doi.org/10.1364/OE.14.011277">https://doi.org/10.1364/OE.14.011277</a> <a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:3" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:4">
<p>Sun-Uk Hwang and Yong-Gu Lee, "Simulation of an oil immersion objective lens: A simplified ray-optics model considering Abbe’s sine condition," Opt. Express 16, 21170-21183 (2008). <a href="https://doi.org/10.1364/OE.16.021170">https://doi.org/10.1364/OE.16.021170</a> <a class="footnote-backref" href="posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:4" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
</ol>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/github-cli-authorization-with-a-fine-grained-access-token/" class="u-url">GitHub CLI Authorization with a Fine-grained Access Token</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/github-cli-authorization-with-a-fine-grained-access-token/" rel="bookmark">
            <time class="published dt-published" datetime="2024-10-04T14:18:48+02:00" itemprop="datePublished" title="2024-10-04 14:18">2024-10-04 14:18</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/github-cli-authorization-with-a-fine-grained-access-token/#disqus_thread" data-disqus-identifier="cache/posts/github-cli-authorization-with-a-fine-grained-access-token.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>It is a good idea to use fine-grained access tokens for shared PCs in the lab that require access to private GitHub repos so that you can restrict the scope of their use to specific repositories and not use your own personal SSH keys on the shared machines. I am experimenting with the GitHub command line tool <code>gh</code> to authenticate with GitHub using fine-grained access tokens and make common remote operations on repos easier.</p>
<p>Today I encountered a subtle problem in the <code>gh</code> authentication process. If you set the protocol to <code>ssh</code> during login, then you will not have access to the repos that you granted permissions to in the fine-grained access token. This can lead to a lot of head scratching because it's not at all clear which permissions map to which git operations. In other words, what you think is a specific permissions error with the token is actually an authentication error.</p>
<p>To avoid the problem, be sure to specify <code>https</code> and not <code>ssh</code> as the protocol during authentication:</p>
<div class="code"><pre class="code literal-block"><span class="go"> echo "$ACCESS_TOKEN" | gh auth login -p https --with-token</span>
</pre></div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/raspberry-pi-i2c-quickstart/" class="u-url">Raspberry Pi I2C Quickstart</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/raspberry-pi-i2c-quickstart/" rel="bookmark">
            <time class="published dt-published" datetime="2024-09-24T15:01:45+02:00" itemprop="datePublished" title="2024-09-24 15:01">2024-09-24 15:01</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/raspberry-pi-i2c-quickstart/#disqus_thread" data-disqus-identifier="cache/posts/raspberry-pi-i2c-quickstart.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>Below are my notes concerning the control of a <a href="https://www.sparkfun.com/products/12918">Sparkfun MCP4725 12-bit DAC</a> over I2C with a Raspberry Pi.</p>
<h2>Rasbperry Pi Setup</h2>
<ol>
<li>Enable the I2C interface if isn't already with <code>raspi-config</code>. Verify that the I2C device file(s) are present in <code>/dev/</code> with <code>ls /dev | grep i2c</code>. (I had two files: <code>i2c-1</code> and <code>i2c-2</code>.)</li>
<li>Install the <code>i2c-tools</code> package for debugging I2C interfaces.</li>
</ol>
<div class="code"><pre class="code literal-block"><span class="go">sudo apt update &amp;&amp; sudo apt install -y i2c-tools</span>
</pre></div>

<h3>i2cdetect</h3>
<p>Attach the DAC to the Raspberry Pi. The pinout is simple:</p>
<table>
<thead><tr>
<th>Raspberry Pi</th>
<th>MCP4725</th>
</tr></thead>
<tbody>
<tr>
<td>GND</td>
<td>GND</td>
</tr>
<tr>
<td>3.3V</td>
<td>Vcc</td>
</tr>
<tr>
<td>SCL</td>
<td>SCL</td>
</tr>
<tr>
<td>SDA</td>
<td>SDA</td>
</tr>
</tbody>
</table>
<p>Next, run  the command <code>i2cdetect -y 1</code>. This will check for a device on bus 1 (<code>/dev/i2c-1</code>) and automatically accept confirmations:</p>
<div class="code"><pre class="code literal-block"><span class="gp">leb@raspberrypi:~/$ </span>i2cdetect<span class="w"> </span>-y<span class="w"> </span><span class="m">1</span>
<span class="go">     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f</span>
<span class="go">00:                         -- -- -- -- -- -- -- --</span>
<span class="go">10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">60: 60 -- -- -- -- -- -- -- -- -- -- -- -- -- -- --</span>
<span class="go">70: -- -- -- -- -- -- -- --</span>
</pre></div>

<p>Each I2C device must have a unique 7-bit address, i.e. 0x00 to 0x7f. The ranges [0x00, 0x07] and [0x78, 0x7f] are reserved. The above output indicates the DAC is at address 0x60. (Rows are the value of the first hexadecimal number of the address, columns are the second.)</p>
<h3>i2cset</h3>
<p><code>i2cset</code> is a command line tool that is part of <code>i2c-tools</code> and that is used to write data to I2C devices. I can set the voltage output of the DAC to 0 as follows:</p>
<div class="code"><pre class="code literal-block"><span class="go">i2cset -y 1 0x60 0x40 0x00 0x00 i</span>
</pre></div>

<p>The arguments mean the following:</p>
<ul>
<li>
<strong>-y</strong> : Auto-confirm</li>
<li>
<strong>1</strong> : Use the device on bus 1</li>
<li>
<strong>0x60</strong> : Use the device at address <strong>0x60</strong>
</li>
<li>
<strong>0x40</strong> : This is a command byte</li>
<li>
<strong>0x00 0x00</strong> : These two data bytes specify the DAC output level</li>
<li>
<strong>i</strong> : This is the write mode. <code>i</code> means I2C block write: <a href="https://docs.kernel.org/i2c/smbus-protocol.html#i2c-block-write">https://docs.kernel.org/i2c/smbus-protocol.html#i2c-block-write</a>
</li>
</ul>
<h4>Command byte</h4>
<p>The command byte is explained on pages 23 and 25 of the <a href="https://ww1.microchip.com/downloads/en/devicedoc/22039d.pdf">MCP4725 datasheet</a>. From most-significant to least-significant bits, the bits mean:</p>
<ol>
<li>
<strong>C2</strong> : command bit</li>
<li>
<strong>C1</strong> : command bit</li>
<li>
<strong>C0</strong> : command bit </li>
<li>
<strong>X</strong> : unused</li>
<li>
<strong>X</strong> : unused</li>
<li>
<strong>PD1</strong> : Power down select</li>
<li>
<strong>PD0</strong> : Power down select</li>
<li>
<strong>X</strong> : unused</li>
</ol>
<p>According to Table 6-2 and Figure 6-2, <code>C2, C1, C0 = 0, 1, 0</code> identifies the command to write to the DAC register and NOT also to the EEPROM. In normal operation, the power down bits are 0, 0 (page 28).</p>
<p>So, to write to the DAC register, we want to send <code>0b01000000</code> which in hexadecimal is <code>0x40</code>.</p>
<h4>Data bytes to voltage</h4>
<p>The data bytes are explained in Figure 6-2 of the datasheet. The first byte contains bits 11-4, and the second byte bits 3-0 in the most-significant bits:</p>
<p><code>D11 D10 D9 D8   D7 D6 D5 D4 | D3 D2 D1 D0  X X X X</code></p>
<p>12-bits are used because this is a 12-bit DAC. The mapping between bytes and voltage is:</p>
<table>
<thead><tr>
<th>Data bytes, hex</th>
<th>Data bytes, decimal</th>
<th>Voltage</th>
</tr></thead>
<tbody>
<tr>
<td>0x00 0x00</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0xFF 0xF0</td>
<td>65520</td>
<td>V_max</td>
</tr>
</tbody>
</table>
<p>where V_max is the voltage supplied to the chip's Vcc pin (3.3V in my case). The output step size is \( \Delta V = V_{max} / 4096 \) or about 0.8 mV.</p>
<h2>Control via Python</h2>
<p>This is modified from <a href="https://learn.sparkfun.com/tutorials/raspberry-pi-spi-and-i2c-tutorial/all">Sparkfun's tutorial</a> and uses the smbus Python bindings. Be aware that the tutorial example has a bug in how it prepares the list of bytes to send to the DAC.</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span> <span class="nn">smbus</span>


<span class="n">OUTPUT_MAX</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4095</span>
<span class="n">V_MAX</span> <span class="o">=</span> <span class="mf">3.3</span>


<span class="k">def</span> <span class="nf">send</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">channel</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device_address</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mh">0x60</span><span class="p">,</span> <span class="n">command_byte</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mh">0x40</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">output</span> <span class="o">&gt;</span> <span class="mf">0.0</span> <span class="ow">and</span> <span class="n">output</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">"Output voltage must be expressed as fraction of the maximum in the range [0.0, 1.0]"</span>

    <span class="n">bus</span> <span class="o">=</span> <span class="n">smbus</span><span class="o">.</span><span class="n">SMBus</span><span class="p">(</span><span class="n">channel</span><span class="p">)</span>

    <span class="n">output_bytes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">output</span> <span class="o">*</span> <span class="n">OUTPUT_MAX</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xfff</span>
    <span class="n">data_byte_0</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_bytes</span> <span class="o">&amp;</span> <span class="mh">0xff0</span><span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="mi">4</span>  <span class="c1"># First data byte</span>
    <span class="n">data_bytes</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">data_byte_0</span><span class="p">,</span> <span class="p">(</span><span class="n">output_bytes</span> <span class="o">&amp;</span> <span class="mh">0xf</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="mi">4</span><span class="p">]</span>  <span class="c1"># Second data byte</span>

    <span class="n">bus</span><span class="o">.</span><span class="n">write_i2c_block_data</span><span class="p">(</span><span class="n">device_address</span><span class="p">,</span> <span class="n">command_byte</span><span class="p">,</span> <span class="n">data_bytes</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">output</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.42</span>
    <span class="n">send</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Estimated output: </span><span class="si">{</span><span class="n">output</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">V_MAX</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

<h2>Misc.</h2>
<h3>Basic Calculator <code>bc</code>
</h3>
<p>This is a command line calculator and can be used for hexadecimal, binary, and decimal conversions. Install with <code>apt install bc</code>.</p>
<div class="code"><pre class="code literal-block"><span class="gp"># </span>Convert<span class="w"> </span>0x40<span class="w"> </span>to<span class="w"> </span>binary
<span class="go">echo "ibase=16; obase=2; 40" | bc</span>

<span class="gp"># </span>Convert<span class="w"> </span>0x40<span class="w"> </span>to<span class="w"> </span>decimal
<span class="go">echo "ibase=16; 40" | bc</span>
</pre></div>

<p><strong>Note that hexadecimal values must be uppercase, e.g. 0xC7, not 0xc7!</strong></p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/persist-internet-connection-sharing-after-reboot/" class="u-url">Persist Internet Connection Sharing after Reboot</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/persist-internet-connection-sharing-after-reboot/" rel="bookmark">
            <time class="published dt-published" datetime="2024-09-17T09:52:27+02:00" itemprop="datePublished" title="2024-09-17 09:52">2024-09-17 09:52</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/persist-internet-connection-sharing-after-reboot/#disqus_thread" data-disqus-identifier="cache/posts/persist-internet-connection-sharing-after-reboot.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p><a href="posts/internet-connection-sharing-for-raspberry-pi-setups/">In my previous post</a> I wrote about how to use Microsoft's Internet Connection Sharing to share an internet connection on a Windows machine with a Raspberry Pi. Unfortunately, I learned that the ICS service settings do not persist after the Windows machine reboots, and as a result the ICS connection is lost.</p>
<p>The fix is explained in <a href="https://learn.microsoft.com/en-us/troubleshoot/windows-client/networking/ics-not-work-after-computer-or-service-restart">this Microsoft Learn page</a>.</p>
<p>To fix the issue, add a key in the Windows registry with the following information:</p>
<ul>
<li>Path: HKEY_LOCAL_MACHINE\Software\Microsoft\Windows\CurrentVersion\SharedAccess</li>
<li>Type: DWORD</li>
<li>Setting: EnableRebootPersistConnection</li>
<li>Value: 1</li>
</ul>
<p>I then had to reset the shared connection by unchecking and rechecking the boxes in the <code>Sharing</code> tab of the internet connection as explained previously. After a reboot, I confirmed that I could connect to the Pi without manually re-enabling ICS.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/internet-connection-sharing-for-raspberry-pi-setups/" class="u-url">Internet Connection Sharing for Raspberry Pi Setups</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/internet-connection-sharing-for-raspberry-pi-setups/" rel="bookmark">
            <time class="published dt-published" datetime="2024-09-13T15:26:11+02:00" itemprop="datePublished" title="2024-09-13 15:26">2024-09-13 15:26</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/internet-connection-sharing-for-raspberry-pi-setups/#disqus_thread" data-disqus-identifier="cache/posts/internet-connection-sharing-for-raspberry-pi-setups.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>Today I decided to set up an old Raspberry Pi 3B+ for a task in the lab. After burning the latest Raspberry Pi OS Lite image on the SD card, I booted it up and was faced with the unfortunately common problem of network access. It would have taken days to get IT to register the Pi's MAC address on our system, and I did not want to wait that long.</p>
<p>Luckily, I had a spare network crossover cable and an extra ethernet interface on my Windows work laptop, so I plugged the Pi directly into the laptop and enabled Microsoft Internet Connection Sharing (ICS) between the network connection through which I was connected to the internet and the connection to the Pi. In my specific example:</p>
<ol>
<li>Press the Windows key and navigate to <code>View network connections</code>
</li>
<li>Right click on my internet connection (<code>Ethernet 2</code> in my case), select <code>Properties...</code>, and then the <code>Sharing</code> tab.</li>
<li>Check <code>Allow other network users to connect...</code> and in the <code>Home networking connection:</code> dropdown, select the connection corresponding to the Pi (<code>Ethernet</code> in my case).</li>
<li>Check <code>Allow other network users to control...</code>. I'm not sure whether this is necessary.</li>
</ol>
<p>Click OK and restart the Pi if it's already connected. Once it restarts, it should now have internet access through the laptop.</p>
<p>Next I wanted to connect with SSH to the Pi from my laptop and I needed to know the Pi's IP address. Luckily, ICS uses the <code>mshome.net</code> domain name for the network, and the Raspberry Pi by default has the hostname <code>raspberrypi</code>. So getting the IP is as easy running the <code>ping raspberrypi.mshome.net</code> command in Powershell.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/the-mono16-format-and-flir-cameras/" class="u-url">The Mono16 Format and Flir Cameras</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/the-mono16-format-and-flir-cameras/" rel="bookmark">
            <time class="published dt-published" datetime="2024-08-27T14:15:36+02:00" itemprop="datePublished" title="2024-08-27 14:15">2024-08-27 14:15</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/the-mono16-format-and-flir-cameras/#disqus_thread" data-disqus-identifier="cache/posts/the-mono16-format-and-flir-cameras.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <p>For a long time I had found the Mono16 image format of Flir's cameras a bit strange. In the lab I have several Flir cameras with 12-bit ADC's, but the images they output in Mono16 would span a range from 0 to around 65535. How does the camera map a 12-bit number to a 16-bit number?</p>
<p>If you search for the Mono16 format you will find that it's a padded format. This means that, in the 12-bit ADC example, 4 bits in each pixel are always 0, and the remaining 12 bits represent the pixel's value. But this should mean that we should get pixel values only between 0 and 2^12 - 1, or 4095. So how is it that we can saturate one of these cameras with values near 65535?</p>
<p>Today it occurred to me that Flir's Mono16 format might not use all the values in the range [0, 65535]. This is indeed the case, as I show below with an image stack that I acquired from one of these cameras:</p>
<div class="code"><pre class="code literal-block"><span class="o">&gt;&gt;&gt;</span> <span class="n">sorted_unique_pixels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">sorted_unique_pixels</span><span class="p">))</span>
<span class="n">array</span><span class="p">([</span> <span class="mi">16</span><span class="p">,</span>  <span class="mi">32</span><span class="p">,</span>  <span class="mi">48</span><span class="p">,</span>  <span class="mi">64</span><span class="p">,</span>  <span class="mi">96</span><span class="p">,</span> <span class="mi">144</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint16</span><span class="p">)</span>
</pre></div>

<p>This prints all the possible, unique differences between the sorted and flattened pixel values in my particular image stack. Notice how they are all multiples of 16?</p>
<p>Let's look also at the sorted array of unique values itself:</p>
<div class="code"><pre class="code literal-block"><span class="o">&gt;&gt;&gt;</span> <span class="n">sorted_unique_pixels</span>
<span class="n">array</span><span class="p">([</span> <span class="mi">5808</span><span class="p">,</span>  <span class="mi">5824</span><span class="p">,</span>  <span class="mi">5856</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">57312</span><span class="p">,</span> <span class="mi">57328</span><span class="p">,</span> <span class="mi">57472</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint16</span><span class="p">)</span>
</pre></div>

<p>There are more than a million pixels in this array, yet they all take values that are integer multiples of 16.</p>
<p>It looks like Flir's Mono16 format rescales the camera's output onto the interval [0, 65535] by introducing "gaps" between the numbers equal to 2^16 - 2^N where N is the bit-depth of the camera's ADC.</p>
<p>But wait just a moment. Above I said that 4 bits in the Mono16 are zero, but I assumed that these were the most significant bits. If the least significant bits are the zero padding, then the allowed pixel values would be, for example, 
<code>0000 0000 = 0</code>, <code>0001 0000 = 16</code>, <code>0010 0000 = 32</code>, <code>0011 0000 = 48</code>, etc. (Here I ignored the first 8 bits for clarity.)</p>
<p>So it appears that Flir is indeed padding the 12-bit ADC data with 0's in its Mono16 format. But, somewhat counter-intuitively, <em>it is the four least significant bits that are the zero padding.</em> I say this is counter-intuitive because I have another camera that pads the most significant bits, so that the maximum pixel value is really 2^N - 1, with N being the ADC's bit-depth.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="posts/literature-review-an-optical-technique-for-remote-focusing-in-microscopy/" class="u-url">Literature Review: An Optical Technique for Remote Focusing in Microscopy</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                Kyle M. Douglass
            </span></p>
            <p class="dateline">
            <a href="posts/literature-review-an-optical-technique-for-remote-focusing-in-microscopy/" rel="bookmark">
            <time class="published dt-published" datetime="2024-05-30T13:43:39+02:00" itemprop="datePublished" title="2024-05-30 13:43">2024-05-30 13:43</time></a>
            </p>
                <p class="commentline">
    
    <a href="posts/literature-review-an-optical-technique-for-remote-focusing-in-microscopy/#disqus_thread" data-disqus-identifier="cache/posts/literature-review-an-optical-technique-for-remote-focusing-in-microscopy.html">Comments</a>


        </p>
</div>
    </header><div class="e-content entry-content">
    <h4>Citation</h4>
<p><a href="https://doi.org/10.1016/j.optcom.2007.10.007">E.J. Botcherby, R. Juškaitis, M.J. Booth, T. Wilson, "An optical technique for remote focusing in microscopy," Optics Communications, Volume 281, Issue 4, 2008, Pages 880-887</a></p>
<h4>Abstract</h4>
<blockquote>
<p>We describe the theory of a new method of optical refocusing that is particularly relevant for confocal and multiphoton microscopy systems. This method avoids the spherical aberration that is common to other optical refocusing systems. We show that aberration-free refocusing can be achieved over an axial scan range of 70 μm for a 1.4 NA objective lens. As refocusing is implemented remotely from the specimen, this method enables high axial scan speeds without mechanical interference between the objective lens and the specimen.</p>
</blockquote>
<h2>Reasons for this Review</h2>
<p>I am interested in this paper for two reasons:</p>
<ol>
<li>Recent advances in light sheet microscopy have made the theory of remote focusing more relevant than in the past.</li>
<li>The paper presents a simplified theory of imaging by a high numerical aperture (NA) objective that is useful for understanding image formation in microscopes without resorting to the usual (and more complicated) Richards and Wolf description.</li>
</ol>
<h2>Problem Addressed by the Paper</h2>
<p>The introduction lays out the reasons for this paper in a straightforward manner:</p>
<ul>
<li>The primary bottleneck in 3D microscopy is axial scanning of the sample (what the authors call <strong>refocusing</strong>).</li>
<li>Due to fundamental optics, refocusing a high resolution microscope involves varying the objective/sample distance, i.e. the image plane must remain fixed.</li>
<li>It would be desirable to develop a <strong>simple</strong> mechanism whereby the objective or sample need not move to achieve refocusing in such microscopes without introducing unwanted aberrations.<ul>
<li>This is because samples are becoming more complex (think embryos, organoids, etc.).</li>
<li>Adaptive optics to fix these aberrations would introduce too much complexity into the setup. (More on this later.)</li>
</ul>
</li>
</ul>
<h2>Theory of 3D Imaging in Microscopes</h2>
<p>The theory of 3D imaging is introduced by first considering a perfect imaging system with an object space refractive index of \( n_1 \) and an image space refractive index of \( n_2 \). Such a system transforms all the rays emanating from any point in the 3D object space to converge to a single point in the 3D image space. An image formed by such a system is known as a <strong>stigmatic</strong> image. Unfortunately, Maxwell, followed by Born and Wolf, showed that such a system is only possible if the magnification is the same in all directions and with magnitude</p>
<p>$$ \left| M \right| = \frac{n_1}{n_2} $$</p>
<p>This also implies that conjugate rays must have the same angle with respect to the optical axis.</p>
<p>$$ \gamma_2 = \pm \gamma_1 $$</p>
<p>Any system that does not meet these criteria is not a perfect imaging system. However, there exist some conditions whereby the system can create a perfect image if their requirements are satisfied. Under these conditions, a perfect image will be created only for objects of limited extent in the object space. The two conditions that are relevant for microscopy are</p>
<ol>
<li>the sine condition, and</li>
<li>the Herschel condition.</li>
</ol>
<p>Under the sine condition, points in a plane transverse to the optical axis are imaged perfectly onto the image plane; points that lie at some axial distance from the object plane suffer from spherical aberration and their images are not stigmatic. In some sense, the Herschel condition is the opposite: on-axis points are imaged stigmatically regardless of their axial position, but off-axis points suffer from aberrations.</p>
<p>The authors note the important fact that most microscope objectives are designed to satisfy the sine condition. As a result, the image plane must remain fixed so that aberration-free refocusing can only be achieved by varying the sample-objective distance. In the authors' words:</p>
<blockquote>
<p>...it is possible to see why commercial microscopes, operating under the sine condition refocus by changing the distance between the specimen and objective, as any attempt to detect images away from the optimal image plane will lead to a degradation by spherical aberration.</p>
</blockquote>
<h3>Questions</h3>
<ol>
<li>Does an ideal imaging system need only produce stigmatic images, or must it also accurately reproduce the relative positions between any pair of points in the image space (up to a proportionality factor)? </li>
<li>What exactly are the defintions of the sine and Herschel conditions? Is it the equations relating the angles of conjugate rays? Is it based on the subset of the object space that is imaged stigmatically? Or, as we'll see in the next section, are they defined by the mapping of ray heights between principal surfaces? The authors present a few attributes of each condition, but I'm not certain which attributes serve as the definitions and which are consequences of their assumptions being true.</li>
</ol>
<h2>The General Pupil Function</h2>
<p>I really liked this section. The authors present a model of a high NA microscope objective that is based on its principal surfaces. They then use a mix of scalar wave theory and ray tracing to explain why the sine condition produces stigmatic images for points near the axis in the focal plane of the objective. I think the value in this model is that it is much more approachable than the electromagnetic Richards and Wolf model for aplanatic systems.</p>
<p>To recall, the principal planes in paraxial optics are used to abstract away the details of a lens system. Refraction effectively occurs at these planes, and the focal length is measured relative to them. In non-paraxial systems, the principal planes actually become curved surfaces. Interestingly, most of the famous optics texts, such as Born and Wolf, are somewhat quiet about this fact, but it can be found in papers such as <a href="https://opg.optica.org/opn/abstract.cfm?uri=opn-9-2-56">Mansuripur, Optics and Photonics News, 9, 56-60 (1998)</a>.</p>
<p>So a high NA objective is modeled as a pair of principal surfaces:</p>
<ol>
<li>The first is a sphere centered on the axis with a radius of curvature equal to the focal distance</li>
<li>The second is a plane perpendicular to the axis, and they refer to it as the pupil plane</li>
</ol>
<p>Another important thing to note is that these surfaces <strong>are not</strong> the usual reference spheres centered about object and image points and located in the entrance/exit pupils. I think the authors are right to use principal surfaces because many modern objectives are object-space telecentric, which places the entrance pupil at infinity. In this case the concept of a reference sphere sitting in the entrance pupil becomes a bit murky and I do not know whether it's applicable.</p>
<p>In any case, the authors compute the path length differences between points in the object space in this system and use the sine and Herschel conditions to map the rays from the object to the image space principal surfaces. (Each condition results in a different mapping.) Under the approximation that the extent of the object is small, the equations for the path length differences demonstrate what was stated in the previous section: that the sine condition leads to spherical aberration for points that do not lie in the focal plane of the objective. In fact, the phase profile of the wave (the authors weave between ray and wave optics) exiting the second principal plane is expanded as:</p>
<p>$$ znk \left[ 1 - \frac{\rho^2 \sin^2 \alpha}{2} + \frac{\rho^4 \sin^4 \alpha}{8} + \cdots \right] $$</p>
<p>For \( z = 0 \), i.e. the object is in the focal plane, all the terms disappear and we get a flat exit wave. When \( z \neq 0 \):</p>
<blockquote>
<p>Focussing the tube lens is accurately described by the quadratic term, as it operates in the paraxial regime. Unfortunately the higher order terms which represent spherical aberrations cannot be focussed by the tube lens and consequently there is a breakdown of stigmatic imaging for these points.</p>
</blockquote>
<p>In other words, <strong>under the sine condition, object points that are outside the focal plane produce curved, non-spherical wavefronts that cannot be focussed to a single point by a tube lens.</strong></p>
<p>If, however, another lens in a reversed orientation was placed so that the curved wavefront from the objective was input into it, it would form a stigmatic image in its image space. This suggests a method for remote focussing.</p>
<h3>Questions</h3>
<ol>
<li>Is the second principal surface flat because the image is formed at infinity by a high NA, infinity-corrected objective? What would its radius of curvature be in a finite conjugate objective?</li>
<li>Is the authors' pupil plane coplanar with the objective's exit pupil? Probably not; I think they're referring to the plane in which we find the objective's pupil function, which is somewhat standard (and confusing) nomenclature in microscopy.</li>
</ol>
<h2>A Technique for Remote Focusing</h2>
<p>We arrive now at the crux of the paper. The authors suggest a setup for remote focusing that is free (within limits) of the spherical aberration that is introduced by objectives that satisfy the sine condition. Effectively they image the pupil from one objective onto the other with a 4f system. This ensures that the aberrated wavefront from the first objective is "unaberrated" by the second objective. Then, another microscope images the focal region of the second objective. 3D scanning is achieved by moving the objective of the second microscope (often called O3 in light sheet microscopes).</p>
<p>There are a few important points:</p>
<ul>
<li>A 4f system needs to be used between the first (O1) and second (O2) objectives to relay the pupil because it faithfully maps the wavefront without adding any additional phase distortion.</li>
<li>On a related note, you can't use tube lenses in the 4f system that are not afocal with the objective. These so-called widefield tube lenses do not share a focal plane with the objective. The objective's pupil must be in the front focal plane of the 4f system.</li>
<li>The "perfect" imaging system of O1/4f system/O2 will have an isotropic magnification of \( n1 / n2 \). This satisfies Maxwell's requirement for 3D stigmatic imaging.</li>
<li>This approach will not work well for objectives that require specific tube lenses for aberration correction. (Sorry Zeiss.)</li>
<li>You will not lose resolution as long as the second objective has a higher <em>angular aperture</em> (not numerical aperture). You can, for example, use a NA 1.4 oil objective for O1 and a NA 0.95 dry objective for O2 because the O2 object space is in air, whereas the O1 object space is in oil with \( n \approx 1.5 \). From the definition of numerical aperture, the sine of the limiting angle of O1 must necessarily be smaller than the air objective.</li>
</ul>
<p>At this point I found it amusing that the authors cited "complexity" as a reason for why their approach is superior to adaptive optics in the introduction of this paper.</p>
<h3>Questions</h3>
<ol>
<li>The authors suggest a different approach where a mirror is placed after O2 so that it also serves as O3 and use a beam splitter to direct the light leaving O2 onto a camera. Why don't light sheet microscopes use this setup? Is it because of a loss of photons due to the beam splitter?</li>
</ol>
<h2>Range of Operation</h2>
<p>The equation for the path length difference between points in object space depends on the assumption of small object distances. This assumption places a limit on the range of validity of this approach. To quantify this limit, the authors computed the Strehl ratio of the phase of the wavefront in the pupil. Honestly, the calculations of this section look tedious. In the end, and after "some routine but rather protracted calculations, a simple result emerges." The simple result looks kind of ugly, depending, among other things on the sine to the eigth power of the aperture angle. It looks like the approach is valid for distances of several tens of microns on both sides of the focal plane of O1, which is in fact quite useful for many biological samples.</p>
<p>Ironically, the authors decide at this point that adaptive optics, the approach to remote focusing that is too complex, probably isn't that bad after all. It can be used to extend the range of validity of the authors' approach by correcting the higher order terms that are dropped in the binomial expansion for the optical path difference.</p>
<h2>Summary</h2>
<p>The authors go on to experimentally verify the approach in a rather unremarkable experiment of taking z-stacks of beads in two different setups. The PSF in their approach is much less aberrated than a normal widefield microscope over an axial range of about \( \pm 40 \mu m \).</p>
<p>Overall I quite like the paper because of its simplified theoretical model and clear explantion of the sine condition. I would argue, though, that the approach is not necessarily less complex than some of the alternatives that they rule out in the introduction. Admittedly, arguments over complexity are usually subjective and this doesn't necessarily mean the paper is of low quality. Given that many light sheet approaches are now based on this method, the paper serves as a good theoretical grounding into why remote focusing works and, in some cases, may be necessary.</p>
    </div>
    </article>
</div>

        <ul class="pager postindexpager clearfix">
<li class="next"><a href="index-1.html" rel="next">Older posts</a></li>
        </ul>
<script>var disqus_shortname="kyle-m-douglass";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
        </script><!--End of body content--><footer id="footer">
            Contents © 2025         <a href="mailto:kyle.m.douglass@gmail.com">Kyle M. Douglass</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
<a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
<img alt="Creative Commons License BY-NC-SA" style="border-width:0; margin-bottom:12px;" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a>
            
            
        </footer>
</div>
</div>


        <script src="assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
