<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kyle M. Douglass (Posts about optics)</title><link>https://kylemdouglass.com/</link><description></description><atom:link href="https://kylemdouglass.com/categories/optics.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:kyle.m.douglass@gmail.com"&gt;Kyle M. Douglass&lt;/a&gt; 
&lt;a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;
&lt;img alt="Creative Commons License BY-NC-SA"
style="border-width:0; margin-bottom:12px;"
src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"&gt;&lt;/a&gt;</copyright><lastBuildDate>Thu, 05 Jun 2025 09:27:30 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>3D Sequential Optical System Layouts</title><link>https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;I am working on a new feature in my &lt;a href="https://github.com/kmdouglass/cherry"&gt;ray tracer&lt;/a&gt; that will allow users to lay out sequential optical systems in 3D. This is forcing me to think carefully about 3D rigid body transformations in a level of detail that I have never before considered.&lt;/p&gt;
&lt;p&gt;In this post I walk through the mathematics for modeling a pair of flat mirrors that are oriented at different angles. Strictly speaking, the layout can be represented more easily in 2D, but I will treat the problem as if it were the more general 3D case. Emphasis will be placed on specifying rotations in an intuitive manner, which will mean rotations about the optical axis, rather than about a fixed axis in a global reference frame.&lt;/p&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;The problem that I will consider is depicted as follows:&lt;/p&gt;
&lt;p&gt;&lt;img alt="A system of two flat mirrors whose optical axis forms the figure Z." src="https://kylemdouglass.com/images/sequential-layout-problem-statement.png"&gt;&lt;/p&gt;
&lt;p&gt;The system consists of two flat mirrors whose optical axis forms a "figure Z." The normal of the first mirror is at 30 degrees to the axis, and likewise for the second. The optical axis emerges from the second mirror parallel to the first.&lt;/p&gt;
&lt;p&gt;The questions are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How do I construct the system without requiring the user to specify the absolute coordinates of the mirror surfaces?&lt;/li&gt;
&lt;li&gt;How do I represent the local coordinate reference frames for each mirror surface?&lt;/li&gt;
&lt;li&gt;How do I handle transformations between frames?&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Ray Tracing Review&lt;/h3&gt;
&lt;p&gt;As a quick review, the ray tracing algorithm that I implemented was described by Spencer and Murty&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;. It is loosely follows this pseudo-code:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;each&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;surface&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;system&lt;/span&gt;:
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;each&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;ray&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;ray&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;bundle&lt;/span&gt;:
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;transform&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;ray&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;coordinates&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;by&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;rotating&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;reference&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;frame&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;into&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;local&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;surface&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;frame&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;find&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;ray&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;surface&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;intersection&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;point&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;propagate&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;ray&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;to&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;intersection&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;point&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;perform&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;bounds&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;checking&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;against&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;surface&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;redirect&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;ray&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;according&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;to&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;laws&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;refraction&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;or&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;reflection&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;transform&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;ray&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;coordinates&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;by&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;rotating&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;reference&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;frame&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;back&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;into&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;global&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;frame&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Rotations are performed using 3x3 rotation matrices. Ray/surface intersections are found numerically using the Newton-Raphson method, even for spherical surfaces&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. I computed the expressions for the surface sag and normal vectors for conics and flat surfaces by hand and hard-coded them as functions of the intersection point in the local surface reference frame to avoid having to compute them on-the-fly.&lt;/p&gt;
&lt;p&gt;Looking at the ray trace algorithm, I see three things that are relevant to this discussion:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There are both global and local reference frames&lt;/li&gt;
&lt;li&gt;Surfaces are iterated over sequentially&lt;/li&gt;
&lt;li&gt;There are rotations, one at the beginning of each loop iteration and one at the end&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's explore each one individually, starting with the global and local reference frames.&lt;/p&gt;
&lt;h2&gt;Reference Frames&lt;/h2&gt;
&lt;p&gt;I will use only right-handed reference frames where positive rotations are in the counterclockwise direction.&lt;/p&gt;
&lt;h3&gt;The Global Reference Frame&lt;/h3&gt;
&lt;p&gt;The global reference frame \( \mathbf{G} \) remains fixed. Sometimes it's called the world frame. I denote the coordinate axes of the global frame using \( x \), \( y \), and \( z \).&lt;/p&gt;
&lt;p&gt;&lt;img alt="The global reference frame." src="https://kylemdouglass.com/images/sequential-layout-global-reference-frame.png"&gt;&lt;/p&gt;
&lt;p&gt;By convention, I put its origin at the first non-object surface; this would be at the first mirror in the system of two mirrors I described above&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;. I also establish the convention that the optical axis between the object and the first surface is parallel to the global z-axis.&lt;/p&gt;
&lt;p&gt;The global frame is important because the orthonormal vectors defining the local and cursor frames (to be explained later) are expressed relative to it.&lt;/p&gt;
&lt;h3&gt;Local Reference Frames&lt;/h3&gt;
&lt;p&gt;Each surface \( i \) has a local reference frame \( \mathbf{L}_i \) whose origin lies at the vertex of the surface. Its coordinate axes are denoted \( x_i^{\prime} \), \( y_i^{\prime} \), and \( z_i^{\prime} \). For flat surfaces, I set the \( z_i^{\prime} \) axis perpendicular to the surface.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The local reference frames of the two mirrors." src="https://kylemdouglass.com/images/sequential-layout-local-reference-frames.png"&gt;&lt;/p&gt;
&lt;p&gt;Notice that the \( x^{\prime} \) axes flip directions when going from mirror 1 to mirror 2. This is done to preserve the right-handedness of the reference frames. More about this will be explained in the next section.&lt;/p&gt;
&lt;h2&gt;Sequential System Models&lt;/h2&gt;
&lt;p&gt;Ray tracing programs for optical design are often divided into two categories: sequential and nonsequential. In sequential ray tracers, rays are traced from one surface to another in the sequence for which they are defined. This means that a ray could pass right through a surface if it is not the next surface in the model sequence.&lt;/p&gt;
&lt;p&gt;Nonsequential ray tracers do not take account of the order in which surfaces are defined. Rays are fired into the world and the intersect whatever the closest object is on their path. Illumination optics often use nonsequential ray tracing, as do rendering engines for cinema.&lt;/p&gt;
&lt;p&gt;My ray tracer is a sequential ray tracer because sequential ray tracing is easier to implement and can be applied to nearly all the use cases that I encounter in the lab.&lt;/p&gt;
&lt;h3&gt;3D Layouts of Sequential Surfaces&lt;/h3&gt;
&lt;p&gt;One possibility to layout sequential surfaces in 3D is to specify the coordinates and orientations of each surface relative to the global frame. This is how one adds surfaces in 3D in the open source Python library &lt;a href="https://github.com/HarrisonKramer/optiland"&gt;Optiland&lt;/a&gt;, for example. In practice, I found that I need to have a piece of paper by my side to work out the positions of each surface independently. This option provides maximum flexibility in surface placement.&lt;/p&gt;
&lt;p&gt;The other possibility that I considered is to leverage the fact that the surfaces are an ordered sequence, and position them in 3D space along the optical axis. The axis can reflect from reflecting surfaces using the law of reflection. Furthermore, any tilt or decenter could be specified relative to this axis. I ultimately chose this solution because I felt that it better matches my mental model of sequential optical systems. It also seems to follow more closely what I do in the lab when I build a system, i.e. add components along an axis that bends through 3D space.&lt;/p&gt;
&lt;h3&gt;The Cursor&lt;/h3&gt;
&lt;p&gt;I created the idea of the cursor to position sequential surfaces in 3D space. A cursor has a 3D position, \( \vec{ t } \left( s \right) \) that is parameterized over the track length \( s \). \( s \) is negative for the object surface, \( s = 0 \) at the first non-object surface, and achieves its greatest value at the final image plane.&lt;/p&gt;
&lt;p&gt;In addition, the cursor has a reference frame attached to it that I denote \( \mathbf{C} \left( s \right) \). The axes of the cursor frame are \( r \), \( u \)  and \( f \), which stand for right, up, and forward, respectively. This nearly matches the &lt;a href="https://dev.epicgames.com/documentation/en-us/uefn/forwardrightup-coordinate-system-in-unreal-editor-for-fortnite"&gt;FRU&lt;/a&gt; coordinate system in game engines such as Unreal, except I take the forward direction to represent the optical axis because I would say that this convention is universal in optical design.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The cursor frames at three different positions along the optical axis." src="https://kylemdouglass.com/images/sequential-layout-cursor-frames.png"&gt;&lt;/p&gt;
&lt;p&gt;Above I show the cursor frame at three different positions along the optical axis \( s_1 &amp;lt; 0 &amp;lt; s_2 &amp;lt; s_3 \). Refracting surfaces will not change the orientation of the cursor frame, but reflecting surfaces will.&lt;/p&gt;
&lt;p&gt;Finally, when \( s \) is exactly equal to a reflecting surface position, I take the orientation of the cursor frame to be the one &lt;strong&gt;before&lt;/strong&gt; reflection. An infinitesimal distance later, the frame reorients by reflecting about the surface normal at the vertex of the surface in its local frame.&lt;/p&gt;
&lt;h4&gt;Convention for Maintaining Right Handedness upon Reflection&lt;/h4&gt;
&lt;p&gt;There is an ambiguity that arises in the cursor frame upon reflection that is best illustrated in the example below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ambiguity in the cursor frame upon reflection." src="https://kylemdouglass.com/images/sequential-layout-ambiguity.png"&gt;&lt;/p&gt;
&lt;p&gt;In panel a, the cursor is incident upon a mirror with its frame's forward direction antiparallel to the mirror's normal vector. There are two equally valid choices when defining the cursor frame after reflection. In panel b, the cursor frame is rotated about the up direction, whereas in panel c it is rotated about the right direction. This means that there is no fundamentally correct way to position the cursor frame after reflection. We must choose a convention and stick with it.&lt;/p&gt;
&lt;p&gt;Reflections of the cursor frame are handled in two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reflect the frame&lt;/li&gt;
&lt;li&gt;Adjust the results to maintain right handedness and address the ambiguity illustrated above&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The vector law of reflection is used to compute the new \( \hat{ r } \), \( \hat{ u } \), and \( \hat{ f } \) unit vectors for any general angle of incidence of the cursor frame upon a reflecting surface:&lt;/p&gt;
&lt;p&gt;$$\begin{equation}
\hat{ f }^{\prime} = \hat{ f } - 2 \left( \hat{ f } \cdot \hat{ n } \right) \hat{ n }
\end{equation}$$&lt;/p&gt;
&lt;p&gt;where \( \hat{ n } \) is the surface's unit normal vector. The same applies for the right and up unit vectors.&lt;/p&gt;
&lt;p&gt;After reflection, I perform a check for right handedness. By convention, I maintain the direction of the up unit vector because many optical systems are laid out in 2D and their elements are rotated about this direction. This convention means that the right unit vector must be flipped:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;cross&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;right&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;up&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;·&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;forward&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;:
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nv"&gt;right&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nv"&gt;right&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The cross product between the right and up directions must point in the forward direction if the system is right handed. "Pointing in the forward direction" means that the dot product of the result with the forward unit vector must be greater than zero. The conditonal in the pseudocode above checks whether this is not indeed the case and flips the right unit vector if necessary.&lt;/p&gt;
&lt;h2&gt;Transformations between Reference Frames&lt;/h2&gt;
&lt;p&gt;There are two different transformations required by the ray trace algorithm:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;From the global frame to a surface local frame&lt;/li&gt;
&lt;li&gt;From a surface local frame to the global frame&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Because the system is laid out relative to the cursor frame, I need to chain together two rotations, one from the global to the cursor frame, and one from the cursor to the local frame.&lt;/p&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;Let's say that the second mirror has a diameter of 25.4 mm, and that the mirrors are separated by \( \| \vec{ t } \left( s_2 \right) \| = 100 \, mm \). I want to find the transformation from the global frame coordinates at a point on the bottom edge of the mirror to the local frame coordinates, which is \( y_2^{\prime} = -12.7 \, mm \). The image below illustrates the geometry that will be used for this example.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Example geometry for transforming from the global frame to the local frame via the cursor frame." src="https://kylemdouglass.com/images/sequential-layout-example-geometry.png"&gt;&lt;/p&gt;
&lt;p&gt;From relatively straightforward trigonmetry&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt; we get the global frame coordinates of both \( {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \) and the point we are trying to find, \( {}^{\mathbf{G}}\vec{ p } \). (Vectors preceded by superscripts with reference frame names indicate the coordinate system they are being referred to.)&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
 {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) = \left(
  \begin{array}{c}
    0 \\
    50 \sqrt{ 3 } \\
    -50
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
 {}^{\mathbf{G}}\vec{ p } = \left(
  \begin{array}{c}
    0 \\
    43.65 \sqrt{ 3 } \\
    -56.35
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;h4&gt;Step 1: Translate from the Global Origin to the Cursor Frame&lt;/h4&gt;
&lt;p&gt;The first step in computing \( {}^{\mathbf{ C }} \vec{ p } \) is to translate from the origin of the global frame to the position of cursor.&lt;/p&gt;
&lt;p&gt;$$ {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right)  =  (0, -6.35 \sqrt{3}, -6.35)^{ \mathrm{ T }}$$&lt;/p&gt;
&lt;h4&gt;Step 2: Rotate into the Cursor Frame&lt;/h4&gt;
&lt;p&gt;A rotation from the global frame into the cursor frame can be achieved by taking the \( \hat{ r } \), \( \hat{ u } \), and \( \hat{ f } \) unit vectors that define the cursor frame in the global coordinate system and making them the columns of a \( 3 \times 3 \) rotation matrix. At the second mirror, this matrix is:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
R_{GC} \left( \theta = 30^{ \circ } \right) = \left(
  \begin{array}{ccc}
    -1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; 1 / 2 &amp;amp; \sqrt{ 3 } / 2 \\
    0 &amp;amp; \sqrt{ 3 } / 2 &amp;amp; -1 / 2
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;If this is not clear, consider that the columns of a rotation matrix represent the basis vectors of the coordinate system after rotation, but expressed in the original (global) frame's coordinate system. Also, from the diagram above, \( \hat{ u } \left( s_2 \right) = ( 0, 1 / 2, \sqrt{ 3 } / 2)^{ \mathrm{ T }} \) and \( \hat{ f } \left( s_2 \right) = ( 0, \sqrt{ 3 } / 2, - 1 / 2)^{ \mathrm{ T }}\), which are the second and third columns of the matrix.&lt;/p&gt;
&lt;p&gt;The rotation into the cursor frame is the product between the rotation matrix and the difference \( {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \):&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
{}^{\mathbf{ C }} \vec{ p } = R_{GC} \left[ {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \right] = \left(
  \begin{array}{ccc}
    -1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; 1 / 2 &amp;amp; \sqrt{ 3 } / 2 \\
    0 &amp;amp; \sqrt{ 3 } / 2 &amp;amp; -1 / 2
  \end{array}
\right) \left(
  \begin{array}{c}
    0 \\
    -6.35 \sqrt{ 3 } \\
    -6.35
  \end{array}
\right) = \left(
  \begin{array}{c}
    0 \\
    -6.35 \sqrt{ 3 } \\
    -6.35
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;At first, I thought I had made a mistake when I did this calculation because the vector is unchanged after rotation. However, as illustrated below, you can see that the relative lengths of the projections of \( {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \) onto the \( u \) and \( f \) axes make sense.&lt;/p&gt;
&lt;p&gt;&lt;img alt="A simplified schematic showing the projection of difference vector onto the -u and -f axes." src="https://kylemdouglass.com/images/sequential-layout-example-global-to-cursor.png"&gt;&lt;/p&gt;
&lt;p&gt;As it turns out, I inadvertently chose an eigenvector of the rotation matrix as an example; any general point will in fact change its coordinates when moving from the global to the cursor frame. For example, if we try to rotate a vector that is antiparallel to the global z-axis, i.e. \( {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) = ( 0, 0, -1 )^{ \mathrm{ T } }\), then it will become&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
R_{GC} \left[ {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \right] = \left(
  \begin{array}{ccc}
    -1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; 1 / 2 &amp;amp; \sqrt{ 3 } / 2 \\
    0 &amp;amp; \sqrt{ 3 } / 2 &amp;amp; -1 / 2
  \end{array}
\right) \left(
  \begin{array}{c}
    0 \\
    0 \\
    -1
  \end{array}
\right) = \left(
  \begin{array}{c}
    0 \\
    -0.8660 \\
    -0.5
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;in the cursor frame.&lt;/p&gt;
&lt;h4&gt;Step 3: Rotate into the Surface Local Frame&lt;/h4&gt;
&lt;p&gt;For the final step, I need to compose a rotation matrix from a sequence of three rotations. To do this well, I need to be very clear about what types of rotations I am performing and their sequence.&lt;/p&gt;
&lt;h5&gt;Active vs. Passive Rotations&lt;/h5&gt;
&lt;p&gt;The difference between active and passive rotations are illustrated below for a 45 degree rotation about the right axis.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Active vs. passive rotations" src="https://kylemdouglass.com/images/sequential-layout-active-vs-passive.png"&gt;&lt;/p&gt;
&lt;p&gt;Active rotations specify the rotation of a point relative to a fixed reference frame; passive rotations specify the rotation of a reference frame, keeping the point fixed. And pay attention here: the right axis points into the screen, so a positive rotation would be clockwise when viewed from the perspective drawn above. &lt;/p&gt;
&lt;p&gt;What are the corresponding rotation matrices? Here, I found that the internet is absolutely littered with wrong answers, including on sites like Wikipedia. I even get different answers from LLMs depending on when I ask. Therefore, I am including them here as a gift to my future self.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;active&lt;/strong&gt; rotation matrices about the x (right), y (up), and z (forward) axes are:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
R_x \left( \theta \right) = \left(
  \begin{array}{ccc}
    1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; \cos \theta &amp;amp; - \sin \theta \\
    0 &amp;amp; \sin \theta &amp;amp; \cos \theta
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
R_y \left( \psi \right) = \left(
  \begin{array}{ccc}
    \cos \psi &amp;amp; 0 &amp;amp; \sin \psi \\
    0 &amp;amp; 1 &amp;amp; 0 \\
    - \sin \psi &amp;amp; 0 &amp;amp; \cos \psi
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
R_z \left( \phi \right) = \left(
  \begin{array}{ccc}
    \cos \phi &amp;amp; - \sin \phi &amp;amp; 0 \\
    \sin \phi &amp;amp; \cos \phi &amp;amp; 0 \\
    0 &amp;amp; 0 &amp;amp; 1
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;passive&lt;/strong&gt; rotation matrices about the x (right), y (up), and z (forward) axes are:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
R_x \left( \theta \right) = \left(
  \begin{array}{ccc}
    1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; \cos \theta &amp;amp; \sin \theta \\
    0 &amp;amp; - \sin \theta &amp;amp; \cos \theta
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
R_y \left( \psi \right) = \left(
  \begin{array}{ccc}
    \cos \psi &amp;amp; 0 &amp;amp; - \sin \psi \\
    0 &amp;amp; 1 &amp;amp; 0 \\
    \sin \psi &amp;amp; 0 &amp;amp; \cos \psi
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
R_z \left( \phi \right) = \left(
  \begin{array}{ccc}
    \cos \phi &amp;amp; \sin \phi &amp;amp; 0 \\
    - \sin \phi &amp;amp; \cos \phi &amp;amp; 0 \\
    0 &amp;amp; 0 &amp;amp; 1
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;Notice that all that changes between these two types of rotations is the location of a negative sign on the \( \sin \) terms.&lt;/p&gt;
&lt;p&gt;I found that a useful way to remember whether a matrix represents an active or passive rotation is as follows. Take for example the +45 degree rotation of the vector \( ( 0, 0, 1 )^{ \mathrm{ T } } \) about the right direction illustrated above. You can see that an active rotation should result in a negative \( u \) and a positive \( f \) component. This means&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
\left(
  \begin{array}{ccc}
    1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; 1 / \sqrt{ 2 } &amp;amp; - 1 / \sqrt{ 2 } \\
    0 &amp;amp; 1 / \sqrt{ 2 } &amp;amp; 1 / \sqrt{ 2 }
  \end{array}
\right) \left(
  \begin{array}{c}
    0 \\
    0 \\
    1
  \end{array}
\right) = \left(
  \begin{array}{c}
    0 \\
    - 1 / \sqrt{ 2 } \\
    1 / \sqrt{ 2 }
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;The passive rotation should result in positive values for both the \( u^{ \prime } \) and \( f^{ \prime } \) components:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
\left(
  \begin{array}{ccc}
    1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; 1 / \sqrt{ 2 } &amp;amp; 1 / \sqrt{ 2 } \\
    0 &amp;amp; - 1 / \sqrt{ 2 } &amp;amp; 1 / \sqrt{ 2 }
  \end{array}
\right) \left(
  \begin{array}{c}
    0 \\
    0 \\
    1
  \end{array}
\right) = \left(
  \begin{array}{c}
    0 \\
    1 / \sqrt{ 2 } \\
    1 / \sqrt{ 2 }
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;I can do a similar check for the other directions to verify the other matrices.&lt;/p&gt;
&lt;h5&gt;Extrinsic vs. Intrinsic Rotations&lt;/h5&gt;
&lt;p&gt;I found these easier to understand than active and passive rotations. Extrinsic rotations are rotations that are always about a fixed global reference frame. On the other hand, intrinsic rotations are about the intermediate frames that result from a single rotation. So if I rotate about the \( f \) axis, then the \( r \) and \( u \) axes will be rotated, resulting in an intermediate \( r^{ \prime }u^{ \prime }f^{ \prime } \) frame. The next rotation will be about one of these intermediate axes.&lt;/p&gt;
&lt;p&gt;The confusing thing about these two types of rotations is the order in which the rotation matrices are applied to a vector. An &lt;strong&gt;extrinsic&lt;/strong&gt; rotation of a vector \( \vec{ v } \) about \( r \), then \( u \), then \( f \) is written as:&lt;/p&gt;
&lt;p&gt;$$ R_f R_u R_r \vec{ v} $$&lt;/p&gt;
&lt;p&gt;which follows the usual commutativity rules of matrix multiplication. An &lt;strong&gt;intrinsic&lt;/strong&gt; rotation of a vector \( \vec{ v } \) about \( r \), then \( u^{ \prime } \), then \( f^{ \prime \prime } \), on the other hand is written as:&lt;/p&gt;
&lt;p&gt;$$ R_r R_u R_f \vec{ v} $$&lt;/p&gt;
&lt;p&gt;So even though the rotation about the right direction is performed first, we multiply the vector first by the rotation matrix about the \( f \) direction in the second intermediate frame.&lt;/p&gt;
&lt;p&gt;All of this might seem confusing and lead one to wonder why they would want to use intrinsic rotations, but actually they are much more intuitive than extrinsic rotations and make a lot of sense when laying out an optical system. For example, if I have a two-axis mirror mount and I rotate the mirror about the vertical axis, a horizontal rotation that follows will be about the axis in the newly rotated frame, not the global laboratory frame. In any case, a sequence of three extrinsic rotations and three intrinsic rotations through the same angles will produce the same result so long as the order of the rotation matrices is correct.&lt;/p&gt;
&lt;h5&gt;Euler Angles and Rotation Sequences&lt;/h5&gt;
&lt;p&gt;The most important thing I learned about Euler angles is that they are completely meaningless unless you also specify a rotation sequence. Additionally, the internet is full of resources about the distinction between proper and improper Euler angles. The gist of what I learned here is that proper Euler angles are really a distraction to scientists and engineers because they rely on rotation sequences in which one of the axes is used twice. More useful are what aerospace engineers sometimes refer to as the Tait-Bryan angles, which are the rotation angles associated with sequences like \( z-y^{ \prime }-x^{ \prime \prime } \) or \( x-y-z \).&lt;/p&gt;
&lt;p&gt;Now, there is one point here that is worth making and that is relevant to optical system layout: &lt;strong&gt;rotations about \( f \), the forward direction, into the local frame are best performed last in the sequence.&lt;/strong&gt; To understand why, consider a cylindrical lens with an axis parallel to the local \( z' \) direction. If we perform an intrinsic rotation about the cursor's \( f \) direction first and then try to adjust its tip or tilt, we will be doing so about axes that are rotated such that its tip and tilt become coupled with respect to the global frame. When aligning such systems, no one expects that rotation of a cylindrical lens about its axis will change the way that the tip and tilt adjustors on a lens mount work.&lt;/p&gt;
&lt;p&gt;For all these reasons, I choose an intrinsic sequence \(r - u^{ \prime } - f^{{ \prime \prime} } \) of passive rotations with Euler angles \( \theta \), \( \psi \), and \( \phi \), respectively. The corresponding rotation matrix is:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
R_{ \mathbf{ CL } } = R_r ( \theta ) R_u ( \psi ) R_f ( \phi ) = \left(
  \begin{array}{ccc}
    \cos \psi \cos \theta &amp;amp; \sin \psi \cos \theta &amp;amp; - \sin \theta \\
    \sin \phi \sin \theta \cos \psi - \sin \psi \cos \theta &amp;amp; \sin \phi \sin \psi \sin \theta + \cos \phi \cos \psi &amp;amp; \sin \psi \cos \theta \\
    \sin \phi \sin \psi + \sin \theta \cos \phi \cos \psi &amp;amp; - \sin \phi \cos \psi + \sin \psi \sin \theta \cos \phi &amp;amp; \cos \phi \cos \theta 
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;And finally, the transformation of the point on the mirror from the global to the surface local frame is:&lt;/p&gt;
&lt;p&gt;$$ {}^{ \mathbf{ L }}\vec{p} = R_{ CL }R_{ GC } \left[ {}^{\mathbf{G}}\vec{ p } - {}^{\mathbf{G}}\vec{ t } \left( s_2 \right) \right] $$&lt;/p&gt;
&lt;h5&gt;The Solution to the Example&lt;/h5&gt;
&lt;p&gt;Does this give the correct result in the above example? Well, the mirror is rotated +30 degrees about the right direction, so the cursor-to-local rotation matrix is:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
R_{CL} \left( \theta \right) = \left(
  \begin{array}{ccc}
    1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; \sqrt{ 3 } / 2 &amp;amp; 1 / 2 \\
    0 &amp;amp; - 1 / 2 &amp;amp; \sqrt{ 3 } / 2
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;From earlier, the vector representing the point in the cursor frame is \( ( 0, -6.35 \sqrt{ 3 }, -6.35 )^{ \mathrm{ T } } \). Their product gives the final answer:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
{}^{ \mathbf{ L } }\vec{ p } = \left(
  \begin{array}{ccc}
    1 &amp;amp; 0 &amp;amp; 0 \\
    0 &amp;amp; \sqrt{ 3 } / 2 &amp;amp; 1 / 2 \\
    0 &amp;amp; - 1 / 2 &amp;amp; \sqrt{ 3 } / 2
  \end{array}
\right) \left(
  \begin{array}{c}
    0 \\
    -6.35 \sqrt{ 3 } \\
    -6.35
  \end{array}
\right) = \left(
\begin{array}{c}
    0 \\
    -12.7 \\
    0
  \end{array}
\right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;This is exactly as expected, as we wanted to get a point on the bottom of the 25.4 mm diameter mirror in its local frame.&lt;/p&gt;
&lt;h4&gt;Step 4: Rotate back from the Surface Local to the Global Frame&lt;/h4&gt;
&lt;p&gt;I need to go back to the global frame at the end of each iteration for a ray trace. Fortunately, it's easy to undo a rotation because the inverse of a rotation matrix is just its transpose. I also need to swap the order of the matrices when taking the inverse, and add back the offset from the origin of the global system. This means:&lt;/p&gt;
&lt;p&gt;$${}^{\mathbf{ G } }\vec{ p } = R_{GC}^{ \mathrm{ T} } R_{CL}^{ \mathrm{ T} } {}^{\mathbf{ L } }\vec{ p } + {}^{\mathbf{ G }} \vec{ t } (s_2) $$&lt;/p&gt;
&lt;p&gt;I plugged in the numbers in Python and verified that I get the original point back.&lt;/p&gt;
&lt;p&gt;This should be all I need to know to implement 3D sequential optical system layouts in my ray tracer.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;G. H. Spencer and M. V. R. K. Murty, "General Ray-Tracing Procedure," J. Opt. Soc. Am. 52, 672-678 (1962). &lt;a href="https://doi.org/10.1364/JOSA.52.000672"&gt;https://doi.org/10.1364/JOSA.52.000672&lt;/a&gt;. &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Ray/surface intersections with spherical surfaces can be found analytically using the quadratic equation with only minor caveats considering stability issues due to floating point arithmetic. This would likely be faster than using Newton-Raphson. However, a general system contains both spherical and non-spherical surfaces, and I was concerned that checking each surface type would result in a performance hit due to branch prediction failures by the processor. I could probably have found a way around this by deciding ahead of time which algorithm to use to determine the intersection for each surface before entering the main ray tracing loop, but during initial development I decided to just use Newton-Raphson for everything because doing so resulted in very simple code. (Thanks to Andy York for telling me about the numerical instabilities when using the quadratic equation. See Chapter 7 here: &lt;a href="https://www.realtimerendering.com/raytracinggems/rtg/index.html"&gt;https://www.realtimerendering.com/raytracinggems/rtg/index.html&lt;/a&gt;.) &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;The object plane is the flat surface perpendicular to the optical axis in which the object lies. It is always at surface index 0 in my convention. &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;Since the only angles involved are \( 30^{ \circ} \) and \( 60^{ \circ } \), I used a 30-60-90 triangle of lengths 1, \( \sqrt{ 3 } \), and 2, respectively to compute the cosines and sines. &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fnref:4" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;Passive rotations result in a rotation of the coordinate axes, keeping a point fixed; active rotations rotate a point about a set of axes. &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fnref:5" title="Jump back to footnote 5 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;The cosine and sine of 45 degrees are both \( 1 / \sqrt{ 2 } \). &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/#fnref:6" title="Jump back to footnote 6 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>optics</category><category>ray tracing</category><guid>https://kylemdouglass.com/posts/3d-sequential-optical-system-layouts/</guid><pubDate>Thu, 05 Jun 2025 09:26:25 GMT</pubDate></item><item><title>An Analog LED Dimmer Circuit</title><link>https://kylemdouglass.com/posts/an-analog-led-dimmer-circuit/</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;I recently needed to build a circuit to control the brightness of a 4 W LED with a knob. I know basic electronics, and I thought this would be easy. I spoke to a few people whom I know and are knowledgable in electronics. I also asked people on Reddit. A lot of people said it would be easy.&lt;/p&gt;
&lt;p&gt;As it turns it, it wasn't easy.&lt;/p&gt;
&lt;h2&gt;The Requirements&lt;/h2&gt;
&lt;p&gt;My requirments are simple:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The brightness should be manually adjustable with a knob from OFF to nearly full ON.&lt;/li&gt;
&lt;li&gt;The LED will serve as the light source of a microscope trans-illuminator. It should work across a large range of frame acquisition rates (1 Hz to 1 kHz, or exposure times of 1 ms to 1 s).&lt;/li&gt;
&lt;li&gt;The range of brightnesses should be variable across the dynamic range of the camera, which in my case is 35,000:1, or about 90 dB.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I don't care about efficiency. I don't care about whether I can use a Raspberry Pi to control it. I don't care whether it can be turned on or off with different logic levels. I just want a knob that I can turn to make the LED brighter or dimmer.&lt;/p&gt;
&lt;p&gt;In spite of the insistence of several people that I communicated with on the internet, I decided that the second requirement would preclude using pulse width modulation (PWM) to dim the LED. Even when I could convince others that PWM almost always causes aliasing at high frame rates, they tried to find obscure work arounds so I could still use PWM. I really do appreciate all the feedback I got. But I also learned that PWM is the hammer of the electronics world that makes everything look like a nail&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/an-analog-led-dimmer-circuit/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2&gt;The Circuit&lt;/h2&gt;
&lt;p&gt;I reached out to a friend of mine who's a wizard at analog electronics&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/an-analog-led-dimmer-circuit/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. He suggested to use a MOSFET and to vary the gate-source voltage to control the current through the LED.&lt;/p&gt;
&lt;p&gt;After a lot of thinking and reading, I arrived at &lt;a href="https://tinyurl.com/2cnw45fy"&gt;the following circuit&lt;/a&gt;:&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="https://kylemdouglass.com/images/led-dimmer-circuit-v0.png"&gt;
&lt;/figure&gt;

&lt;p&gt;The LED is an Osram Oslon star LED (LST1-01F05-4070-01) with a maximum current of 1.3 A and a maximum forward voltage of 3.2 V. The MOSFET is an IRF510, whose gate-source threshold voltage is about 3 V.&lt;/p&gt;
&lt;p&gt;Here's a brief explanation of what each component does:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Voltage source&lt;/strong&gt; : This is just a 12 V wall wart.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;200 nF capacitor&lt;/strong&gt; : This smooths out any fluctuations from the wall wart.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;50 kOhm potentiometer&lt;/strong&gt; : The "knob." Turning it will vary the gate-source voltage of the MOSFET, which controls how much current flows through the LED.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;50 kOhm resistor&lt;/strong&gt; : This along with the potentiometer forms a voltage divider to keep the minimum voltage at the MOSFET gate close to where the LED turns on. Without it, you need to rotate the potentiometer almost half of its full range for the LED to turn on.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;300 nF capacitor&lt;/strong&gt; : A debounce capacitor that smooths out the mechanical irregularities of the pot when it turns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IRF510 MOSFET&lt;/strong&gt; : Basically a valve that I can vary continuously to control the LED current by setting the voltage at the gate.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LED&lt;/strong&gt; : So pretty.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;10 Ohm resistor&lt;/strong&gt; : This limits the current through the resistor. I calculated its value by dividing the maximum supply voltage minus the maximum forward voltage drop across the LED by the maximum current, then rounded up for safety.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ R = \frac{V}{I} = \frac{\left( 12 \, V - 3.2 \, V \right) }{1.3 \, A} = 6.8 \Omega $$&lt;/p&gt;
&lt;p&gt;The resistor also has to handle a large power dissipation at the maximum current:&lt;/p&gt;
&lt;p&gt;$$ P = I^2 R = \left(1.3 \, A \right)^2 \left( 10 \Omega \right) = 16.9 W $$&lt;/p&gt;
&lt;p&gt;I decided instead to keep the current to less than 1 Amp so that I could use a 10 Watt resistor that I had.&lt;/p&gt;
&lt;p&gt;Power dissipation is also why we don't just use a potentiometer to control the LED current: my pots were only rated up to about 50 mW, whereas I expected that the MOSFET would have handle loads on the order of Watts due to the high current.&lt;/p&gt;
&lt;h2&gt;What I Learned&lt;/h2&gt;
&lt;h3&gt;I really need to study MOSFETS&lt;/h3&gt;
&lt;p&gt;I still don't really know how to solve circuits with MOSFETs. I arrived at the above circuit largely by trial-and-error on a prototype and by performing naive calculations on the voltage divider that turned out to not be entirely correct. I also expected that the LED would turn on once I passed the MOSFET's gate-source voltage threshold, but this turned out to be off by about 2 or 3 V.&lt;/p&gt;
&lt;h3&gt;MOSFETs suffer from second order effects&lt;/h3&gt;
&lt;p&gt;There is currently a hysteresis in the gate-ground voltage at when the LED turns on and when it turns off by about half a Volt. According to a helpful person on Reddit, this is likely due to a change in both the LEDs forward voltage and the MOSFET's threshold voltage with temperature once the current starts flowing. A possible fix is to swap the order of the LED and the MOSFET so that only the MOSFET will contribute to the hysteresis.&lt;/p&gt;
&lt;h3&gt;You can always complicate things to make them better&lt;/h3&gt;
&lt;p&gt;The same person on Reddit above also suggested making the circuit robust to temperature variations by adding an opamp to control the MOSFET gate voltage. It would compensate for temperature changes by comparing the potentiometer value to the 10 Ohm resistor in a closed feedback loop.&lt;/p&gt;
&lt;h3&gt;Electronics is an art&lt;/h3&gt;
&lt;p&gt;Yes, electronics is a science, but I would argue that having to mentally juggle second order effects and the fact that experts seem to make an initial design "by feel" are signatures of an art.&lt;/p&gt;
&lt;p&gt;It also struck me how nearly every step of the process forced me to take a detour to address something I hadn't at first considered, such as current limits in the wires and large variations in the MOSFET specs.&lt;/p&gt;
&lt;p&gt;The next time I need to do something like this, I will expect the problem to take longer to solve than I first anticipate.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;To my non-native English speaking readers: I mean that people try to use PWM to solve problems where it's not appropriate. &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/an-analog-led-dimmer-circuit/#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;And if you play electric guitar, be sure to check out his handmade effects pedals: &lt;a href="https://www.volumeandpower.com/"&gt;https://www.volumeandpower.com/&lt;/a&gt;. &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/an-analog-led-dimmer-circuit/#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>electronics</category><category>optics</category><guid>https://kylemdouglass.com/posts/an-analog-led-dimmer-circuit/</guid><pubDate>Fri, 17 Jan 2025 08:08:27 GMT</pubDate></item><item><title>Coordinate Systems for Modeling Microscope Objectives</title><link>https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;p&gt;A common model for infinity corrected microscope objectives is that of an aplanatic and telecentric optical system. In many developments of this model, emphasis is placed upon the calculation of the electric field near the focus. However, this has the effect that the definition of the coordinate systems and geometry are conflated with the determination of the fields. In addition, making the model amenable to computation often occurs as an afterthought.&lt;/p&gt;
&lt;p&gt;In this post I will explore the geometry of an aplanatic system for modeling high NA objectives with an emphasis on computational implementations. My approach follows Novotny and Hecht&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; and Herrera and Quinto-Su&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2&gt;The Model Components&lt;/h2&gt;
&lt;p&gt;The model system is illustrated below:&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="https://kylemdouglass.com/images/aplanatic-telecentric-system.png"&gt;
  &lt;figcaption&gt;A high NA, infinity corrected microscope objective as an aplanatic and telecentric optical system.
&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In this model, we abstract over the details of the objective by representing it as four surfaces:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A back focal plane containing an aperture stop&lt;/li&gt;
&lt;li&gt;A back principal plane, \( P \)&lt;/li&gt;
&lt;li&gt;A front principal surface, \( P' \)&lt;/li&gt;
&lt;li&gt;A front focal plane&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The space to the left of the back principal plane is called the infinity space. The space to the right of the front principal surface is called the sample space.&lt;/p&gt;
&lt;p&gt;We let the infinity space refractive index \( n_1 = 1 \) because it is in air. The refractive index \( n_2 \) is the refractive index of the immersion medium.&lt;/p&gt;
&lt;p&gt;The unit vectors \( \mathbf{n} \) are not used in this discussion; they are relevant for computing the fields.&lt;/p&gt;
&lt;h3&gt;Assumptions&lt;/h3&gt;
&lt;p&gt;We make one assumption: the system obeys the sine condition. The meaning of this will be explained later.&lt;/p&gt;
&lt;p&gt;An aplanatic system is one that obeys the sine condition.&lt;/p&gt;
&lt;p&gt;We will not assume the intensity law to conserve energy because it is only necessary when computing the electric field near the focus.&lt;/p&gt;
&lt;h3&gt;The Aperture Stop and Back Focal Plane&lt;/h3&gt;
&lt;p&gt;The aperture stop (AS) of an optical system is the element that limits the angle of the marginal ray.&lt;/p&gt;
&lt;p&gt;The system is telecentric because the aperture stop is located in the back focal plane (BFP). We can shape the focal field by spatially modulating any of the amplitude, phase, or polarization of the incident light in a plane conjugate to the BFP.&lt;/p&gt;
&lt;h3&gt;The Back Principal Plane&lt;/h3&gt;
&lt;p&gt;This is the plane in infinity space at which rays appear to refract. It is a plane because rays coming from a point in the front focal plane all emerge into the infinity space in the same direction.&lt;/p&gt;
&lt;p&gt;Strictly speaking, focus field calculations require us to propagate the field from the AS to the back principal plane before computing the Debye diffraction integral, but this step is often omitted&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;. The assumptions of paraxial optics should hold here.&lt;/p&gt;
&lt;h3&gt;The Front Principal Surface&lt;/h3&gt;
&lt;p&gt;The front principal surface is the surface at which rays appear to refract in the sample space. It is a surface because&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;this is a non-paraxial system, and&lt;/li&gt;
&lt;li&gt;we assumed the sine condition.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The sine condition states that refraction of a ray coming from an on-axis point in the front focal plane occurs on a spherical cap centered upon the focal point. The distance from the optical axis of the point of intersection of the ray with the surface is proportional to the sine of the angle that the ray makes with the axis.&lt;/p&gt;
&lt;p&gt;The principal surface is in the far field of the electric field coming from the focal region. For this reason, we can represent a point on this surface as representing a single ray or a plane wave&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3&gt;The Front Focal Plane&lt;/h3&gt;
&lt;p&gt;This plane is located a distance \( n_2 f \) from the principal surface&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;. It is not at a distance \( f \) from this surface. This is a result of imaging in an immersion medium.&lt;/p&gt;
&lt;h2&gt;Geometry and Coordinate Systems&lt;/h2&gt;
&lt;h3&gt;The Aperture Stop Radius&lt;/h3&gt;
&lt;p&gt;The aperture stop radius \( R \) corresponds to the distance from the axis to the point where the marginal ray intersects the front prinicpal surface. In the sample space, the marginal ray travels at an angle \( \theta_{max} \) with respect to the axis.&lt;/p&gt;
&lt;p&gt;Under the sine condition, this height is&lt;/p&gt;
&lt;p&gt;$$ R = n_2 f \sin{ \theta_{max} } = f \, \text{NA} $$&lt;/p&gt;
&lt;p&gt;The right-most expression uses the definition of the numerical aperture \( \text{NA} \equiv n \sin{ \theta_{max} } \).&lt;/p&gt;
&lt;p&gt;Compare this result to the oft-cited expression for the entrance pupil diameter of an objective lens: \( D = 2 f \, \text{NA} \). They are the same. This makes sense because an entrance pupil is either&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;an image of an aperture stop, or&lt;/li&gt;
&lt;li&gt;a physical stop.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;The Back Principal Plane&lt;/h3&gt;
&lt;p&gt;There are two independent coordinate systems in the back principal plane:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the spatial coordinate system defining the far field positions \( \left( x_{\infty} , y_{\infty} \right) \), and&lt;/li&gt;
&lt;li&gt;the coordinate system of the angular spectrum of plane waves \( \left( k_x, k_y \right) \).&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;The Far Field Coordinate System&lt;/h4&gt;
&lt;p&gt;The far field coordinate system may be written in Cartesian form as \( \left( x_{\infty} , y_{\infty} \right) \). It also has a cylindrical representation as&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
\rho &amp;amp;=&amp;amp; \sqrt{x_{\infty}^2 + y_{\infty}^2} \\
\phi &amp;amp;=&amp;amp; \arctan \left( \frac{y_{\infty}}{x_{\infty}} \right)
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;The cylindrical representation appears to be preferred in textbook developments of the model. The Cartesian representation is likely preferred for computational models because it works naturally with two-dimensional arrays of numbers, and because beam shaping elements such as spatial light modulators are rectangular arrays of pixels&lt;sup id="fnref2:2"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h4&gt;The Angular Spectrum Coordinate System&lt;/h4&gt;
&lt;p&gt;Each point in the angular spectrum coordinate system represents a plane wave in the sample space that is traveling at an angle \( \theta \) to the axis according to:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
k_x &amp;amp;=&amp;amp; k \sin \theta \cos \phi \\
k_y &amp;amp;=&amp;amp; k \sin \theta \sin \phi \\
k_z &amp;amp;=&amp;amp; k \cos \theta
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;where \( k = 2 \pi n_2 / \lambda = n_2 k_0 \).&lt;/p&gt;
&lt;p&gt;Along the y-axis ( \( x_{\infty} = 0 \) ), the maximum value of \( k_y \) is \(n_2 k_0 \sin \theta_{max} = k_0 \, \text{NA} \).&lt;/p&gt;
&lt;p&gt;Substitute in the expression \( \text{NA} = R / f \) and we get \(k_{y, max} = k_0 R / f\). But \( R = y_{\infty, max} \). This (and similar reasoning for the x-axis) implies that:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
k_x &amp;amp;=&amp;amp; k_0 x_{\infty} / f \\
k_y &amp;amp;=&amp;amp; k_0 y_{\infty} / f
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;The above equations link the angular spectrum coordinate system to the far field coordinate system. They are no longer independent once \( f \) and \( \lambda \) are specified.&lt;/p&gt;
&lt;h2&gt;Numerical Meshes&lt;/h2&gt;
&lt;p&gt;There are four free parameters for defining the coordinate systems of the numerical meshes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The numerical aperture, \( \text{NA} \)&lt;/li&gt;
&lt;li&gt;The wavelength, \( \lambda \)&lt;/li&gt;
&lt;li&gt;The focal length, \( f \)&lt;/li&gt;
&lt;li&gt;The linear mesh size, \( L \)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below is a figure that illustrates the construction of the meshes. Both the far field and angular spectrum coordinate systems are represented by a \( L \times L \) array. \( L = 16 \) in the figure below. In general the value of \( L \) should be a power of 2 to help ensure the efficiency of the Fast Fourier Transform (FFT). By considering only powers of 2, we need only consider arrays of even size as well.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="https://kylemdouglass.com/images/pupil-function-simulation-mesh.png"&gt;
  &lt;figcaption&gt;A numeric mesh representing the far field and angular spectrum coordinate systems of a microscope objective. Fields are sampled at the center of each mesh pixel.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The fields are defined on a region of circular support that is centered on this array. The radius of the domain of the far field coordinate system is \( f \text{NA} \); the radius of the domain of the angular spectrum coordinate system is \( k_0 \text{NA} \).&lt;/p&gt;
&lt;p&gt;The boxes that are bound by the gray lines indicate the location of each field sample. The \( \left( x_{\infty} , y_{\infty} \right) \) and the \( \left( k_x, k_y \right) \) coordinate systems are sampled at the center of each gray box. The origin is therefore not sampled, which will help avoid division by zero errors when the fields are eventually computed.&lt;/p&gt;
&lt;p&gt;The figure suggests that we could create only one mesh and scale it by either \( f \text{NA} \) or \( k_0 \text{NA} \) depending on which coordinate system we are working with. The normalized coordinates become \( \left( x_{\infty} / \left( f \text{NA} \right), y_{\infty} / \left( f \text{NA} \right) \right) \) and \( \left( k_x / \left( k_0 \text{NA} \right), k_y / \left( k_0 \text{NA} \right) \right) \).&lt;/p&gt;
&lt;h3&gt;1D Mesh Example&lt;/h3&gt;
&lt;p&gt;As an example, let \( L = 16 \). To four decimal places, the normalized coordinates are \( -1.0000, -0.8667, \ldots, -0.0667, 0.0667, \ldots, 0.8667, 1.0000 \).&lt;/p&gt;
&lt;p&gt;The spacing between array elements is \( 2 / \left( L - 1 \right) = 0.1333 \). Note that 0 is not included in the 1D mesh as it goes from -0.0667 to 0.0667.&lt;/p&gt;
&lt;p&gt;A 2D mesh is easily constructed from the 1D mesh using tools such as NumPy's &lt;a href="https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html"&gt;meshgrid&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Back Principal Plane Mesh Spacings&lt;/h3&gt;
&lt;p&gt;In the x-direction, the mesh spacing of the far field coordinate system is&lt;/p&gt;
&lt;p&gt;$$ \Delta x_{\infty} = 2 R / \left( L - 1 \right) = 2 f \text{NA} / \left( L - 1 \right) $$&lt;/p&gt;
&lt;p&gt;In the \( k_x \)-direction, the mesh spacing of the angular spectrum coordinate system is&lt;/p&gt;
&lt;p&gt;$$ \Delta k_x = 2 k_{max} / \left( L - 1 \right) = 2 k_0 \text{NA} / \left( L - 1 \right) $$&lt;/p&gt;
&lt;p&gt;Note the symmetry between these two expressions. One scales with \( f \text{NA} \) and the other \( k_0 \text{NA} \). Recall that these are free parameters of the model.&lt;/p&gt;
&lt;h3&gt;Sample Space Mesh Spacing&lt;/h3&gt;
&lt;p&gt;It is interesting to compute the spacing between mesh elements \( \Delta x \) in the sample space when the fields are eventually computed.&lt;/p&gt;
&lt;p&gt;The sampling angular frequency in the sample space is \( k_S = 2 \pi / \Delta x \).&lt;/p&gt;
&lt;p&gt;The Nyquist-Shannon sampling theory states that the maximum informative angular frequency is \( k_{max} = k_S / 2 \).&lt;/p&gt;
&lt;p&gt;From the previous section, we know that \( k_{max} = \left(L - 1 \right) \Delta k_x / 2 \), and that \( \Delta k_x = 2 k_0 \text{NA} / \left( L - 1 \right) \).&lt;/p&gt;
&lt;p&gt;Combining all the previous expressions and simplifying, we get:&lt;/p&gt;
&lt;p&gt;$$\begin{eqnarray}
k_S &amp;amp;=&amp;amp; 2 k_{max} \\
2 \pi / \Delta x &amp;amp;=&amp;amp; \left(L - 1 \right) \Delta k_x \\
2 \pi / \Delta x &amp;amp;=&amp;amp; \left(L - 1 \right) \left[ 2 k_0 \text{NA} / \left( L - 1 \right) \right] \\
2 \pi / \Delta x &amp;amp;=&amp;amp; \left(L - 1 \right) \left[ 2 \left(2 \pi / \lambda \right) \text{NA} / \left( L - 1 \right) \right]
\end{eqnarray}$$&lt;/p&gt;
&lt;p&gt;Solving the above expression for \( \Delta x \), we arrive at&lt;/p&gt;
&lt;p&gt;$$ \Delta x = \frac{\lambda}{2 \text{NA}} $$&lt;/p&gt;
&lt;p&gt;which is of course the Abbe diffraction limit.&lt;/p&gt;
&lt;h3&gt;Effect of not Sampling the Origin&lt;/h3&gt;
&lt;p&gt;Herrera and Quinto-Su&lt;sup id="fnref3:2"&gt;&lt;a class="footnote-ref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; point out that an error will be introduced if we naively apply the FFT to compute the field components in the \( \left( k_x, k_y \right) \) coordinate system because the origin is not sampled, whereas the FFT assumes that we sample the zero frequency component. The effect is that the result of the FFT has a constant phase error that accounts for a half-pixel shift in each direction of the mesh.&lt;/p&gt;
&lt;p&gt;Consider again the 1D mesh example with \(L = 16 \): \( -1.0000, -0.8667, \ldots, -0.0667, 0.0667, \ldots, 0.8667, 1.0000 \)&lt;/p&gt;
&lt;p&gt;In Python and other languages that index arrays starting at 0, the origin is located at \(L / 2 - 0.5 \), i.e. halfway between the samples at index 7 and 8. A lateral shift in Fourier space is equivalent to a phase shift in real space:&lt;/p&gt;
&lt;p&gt;$$ \phi_{shift} \left(X, Y \right) =  -j 2 \pi \frac{0.5}{L} X - j 2 \pi \frac{0.5}{L} Y $$&lt;/p&gt;
&lt;p&gt;where \( X \) and \( Y \) are normalized coordinates.&lt;/p&gt;
&lt;p&gt;At this point, I am uncertain whether the phasor with the above argument needs to be multiplied or divided with the result of the FFT because 1. there are a few typos in the signs for the coordinate system bounds in the manuscript of Herrera and Quinto-Su, and 2. the correction was developed for use in MATLAB, which indexes arrays starting at 1. Once the fields are computed, it would be easy to verify the correct sign of the phase terms following the procedure outlined in Figure 3 of Herrera and Quinto-Su's manuscript.&lt;/p&gt;
&lt;h3&gt;Structure of the Algorithm&lt;/h3&gt;
&lt;p&gt;The algorithm to compute the focus fields will proceed as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(optional) Propgate the inputs fields from the AS to the back principal plane using paraxial wave propagation&lt;/li&gt;
&lt;li&gt;Input the sampled fields in the back principal plane in the \( \left( x_{\infty}, y_{\infty} \right) \) coordinate system&lt;/li&gt;
&lt;li&gt;Transform the fields to the \( \left( k_x, k_y \right) \) coordinate system&lt;/li&gt;
&lt;li&gt;Compute the fields in the \( \left(x, y, z \right) \) coordinate system using the FFT&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Additional Remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Zero padding the mesh will increase the sample space resolution beyond the Abbe limit, but since the fields remain zero outside of the support, no new information is added.&lt;/li&gt;
&lt;li&gt;On the other hand, zero padding might be required when computing fields going from the sample space to the back principal plane to faithfully reproduce any evanescent components.&lt;/li&gt;
&lt;li&gt;Separating the coordinate system and mesh construction from the calculation of the fields reveals that the two assumptions of the model belong separately to each part. The sine condition is used in the construction of the coordinate systems, whereas energy conservation is used when computing the fields.&lt;/li&gt;
&lt;li&gt;This post did not explain how to compute the fields.&lt;/li&gt;
&lt;li&gt;Herrera and Quinto-Su (and possibly also Novotny and Hecht) appear to use an "effective" focal length which can be obtained by multiplying the one that I use by the sample space refractive index. I prefer my formulation because it is consistent with geometric optics and the well-known expression for the diameter of an objective's entrance pupil. When the fields are calculated, however, I do not yet know whether the arguments of the phasors of the Debye integral will require modification.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Lukas Novotny and Bert Hecht, "Principles of Nano-Optics," Cambridge University Press (2006). &lt;a href="https://doi.org/10.1017/CBO9780511813535"&gt;https://doi.org/10.1017/CBO9780511813535&lt;/a&gt; &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fnref2:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Isael Herrera and Pedro A. Quinto-Su, "Simple computer program to calculate arbitrary tightly focused (propagating and evanescent) vector light fields," arXiv:2211.06725 (2022). &lt;a href="https://doi.org/10.48550/arXiv.2211.06725"&gt;https://doi.org/10.48550/arXiv.2211.06725&lt;/a&gt; &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fnref2:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fnref3:2" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Marcel Leutenegger, Ramachandra Rao, Rainer A. Leitgeb, and Theo Lasser, "Fast focus field calculations," Opt. Express 14, 11277-11291 (2006). &lt;a href="https://doi.org/10.1364/OE.14.011277"&gt;https://doi.org/10.1364/OE.14.011277&lt;/a&gt; &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:3" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;Sun-Uk Hwang and Yong-Gu Lee, "Simulation of an oil immersion objective lens: A simplified ray-optics model considering Abbe’s sine condition," Opt. Express 16, 21170-21183 (2008). &lt;a href="https://doi.org/10.1364/OE.16.021170"&gt;https://doi.org/10.1364/OE.16.021170&lt;/a&gt; &lt;a class="footnote-backref" href="https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/#fnref:4" title="Jump back to footnote 4 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><category>microscopy</category><category>optics</category><guid>https://kylemdouglass.com/posts/coordinate-systems-for-modeling-microscope-objectives/</guid><pubDate>Thu, 21 Nov 2024 09:52:48 GMT</pubDate></item><item><title>Literature Review: An Optical Technique for Remote Focusing in Microscopy</title><link>https://kylemdouglass.com/posts/literature-review-an-optical-technique-for-remote-focusing-in-microscopy/</link><dc:creator>Kyle M. Douglass</dc:creator><description>&lt;h4&gt;Citation&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://doi.org/10.1016/j.optcom.2007.10.007"&gt;E.J. Botcherby, R. Juškaitis, M.J. Booth, T. Wilson, "An optical technique for remote focusing in microscopy," Optics Communications, Volume 281, Issue 4, 2008, Pages 880-887&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;We describe the theory of a new method of optical refocusing that is particularly relevant for confocal and multiphoton microscopy systems. This method avoids the spherical aberration that is common to other optical refocusing systems. We show that aberration-free refocusing can be achieved over an axial scan range of 70 μm for a 1.4 NA objective lens. As refocusing is implemented remotely from the specimen, this method enables high axial scan speeds without mechanical interference between the objective lens and the specimen.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Reasons for this Review&lt;/h2&gt;
&lt;p&gt;I am interested in this paper for two reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Recent advances in light sheet microscopy have made the theory of remote focusing more relevant than in the past.&lt;/li&gt;
&lt;li&gt;The paper presents a simplified theory of imaging by a high numerical aperture (NA) objective that is useful for understanding image formation in microscopes without resorting to the usual (and more complicated) Richards and Wolf description.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Problem Addressed by the Paper&lt;/h2&gt;
&lt;p&gt;The introduction lays out the reasons for this paper in a straightforward manner:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The primary bottleneck in 3D microscopy is axial scanning of the sample (what the authors call &lt;strong&gt;refocusing&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Due to fundamental optics, refocusing a high resolution microscope involves varying the objective/sample distance, i.e. the image plane must remain fixed.&lt;/li&gt;
&lt;li&gt;It would be desirable to develop a &lt;strong&gt;simple&lt;/strong&gt; mechanism whereby the objective or sample need not move to achieve refocusing in such microscopes without introducing unwanted aberrations.&lt;ul&gt;
&lt;li&gt;This is because samples are becoming more complex (think embryos, organoids, etc.).&lt;/li&gt;
&lt;li&gt;Adaptive optics to fix these aberrations would introduce too much complexity into the setup. (More on this later.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Theory of 3D Imaging in Microscopes&lt;/h2&gt;
&lt;p&gt;The theory of 3D imaging is introduced by first considering a perfect imaging system with an object space refractive index of \( n_1 \) and an image space refractive index of \( n_2 \). Such a system transforms all the rays emanating from any point in the 3D object space to converge to a single point in the 3D image space. An image formed by such a system is known as a &lt;strong&gt;stigmatic&lt;/strong&gt; image. Unfortunately, Maxwell, followed by Born and Wolf, showed that such a system is only possible if the magnification is the same in all directions and with magnitude&lt;/p&gt;
&lt;p&gt;$$ \left| M \right| = \frac{n_1}{n_2} $$&lt;/p&gt;
&lt;p&gt;This also implies that conjugate rays must have the same angle with respect to the optical axis.&lt;/p&gt;
&lt;p&gt;$$ \gamma_2 = \pm \gamma_1 $$&lt;/p&gt;
&lt;p&gt;Any system that does not meet these criteria is not a perfect imaging system. However, there exist some conditions whereby the system can create a perfect image if their requirements are satisfied. Under these conditions, a perfect image will be created only for objects of limited extent in the object space. The two conditions that are relevant for microscopy are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the sine condition, and&lt;/li&gt;
&lt;li&gt;the Herschel condition.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under the sine condition, points in a plane transverse to the optical axis are imaged perfectly onto the image plane; points that lie at some axial distance from the object plane suffer from spherical aberration and their images are not stigmatic. In some sense, the Herschel condition is the opposite: on-axis points are imaged stigmatically regardless of their axial position, but off-axis points suffer from aberrations.&lt;/p&gt;
&lt;p&gt;The authors note the important fact that most microscope objectives are designed to satisfy the sine condition. As a result, the image plane must remain fixed so that aberration-free refocusing can only be achieved by varying the sample-objective distance. In the authors' words:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;...it is possible to see why commercial microscopes, operating under the sine condition refocus by changing the distance between the specimen and objective, as any attempt to detect images away from the optimal image plane will lead to a degradation by spherical aberration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Questions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Does an ideal imaging system need only produce stigmatic images, or must it also accurately reproduce the relative positions between any pair of points in the image space (up to a proportionality factor)? &lt;/li&gt;
&lt;li&gt;What exactly are the defintions of the sine and Herschel conditions? Is it the equations relating the angles of conjugate rays? Is it based on the subset of the object space that is imaged stigmatically? Or, as we'll see in the next section, are they defined by the mapping of ray heights between principal surfaces? The authors present a few attributes of each condition, but I'm not certain which attributes serve as the definitions and which are consequences of their assumptions being true.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;The General Pupil Function&lt;/h2&gt;
&lt;p&gt;I really liked this section. The authors present a model of a high NA microscope objective that is based on its principal surfaces. They then use a mix of scalar wave theory and ray tracing to explain why the sine condition produces stigmatic images for points near the axis in the focal plane of the objective. I think the value in this model is that it is much more approachable than the electromagnetic Richards and Wolf model for aplanatic systems.&lt;/p&gt;
&lt;p&gt;To recall, the principal planes in paraxial optics are used to abstract away the details of a lens system. Refraction effectively occurs at these planes, and the focal length is measured relative to them. In non-paraxial systems, the principal planes actually become curved surfaces. Interestingly, most of the famous optics texts, such as Born and Wolf, are somewhat quiet about this fact, but it can be found in papers such as &lt;a href="https://opg.optica.org/opn/abstract.cfm?uri=opn-9-2-56"&gt;Mansuripur, Optics and Photonics News, 9, 56-60 (1998)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So a high NA objective is modeled as a pair of principal surfaces:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The first is a sphere centered on the axis with a radius of curvature equal to the focal distance&lt;/li&gt;
&lt;li&gt;The second is a plane perpendicular to the axis, and they refer to it as the pupil plane&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another important thing to note is that these surfaces &lt;strong&gt;are not&lt;/strong&gt; the usual reference spheres centered about object and image points and located in the entrance/exit pupils. I think the authors are right to use principal surfaces because many modern objectives are object-space telecentric, which places the entrance pupil at infinity. In this case the concept of a reference sphere sitting in the entrance pupil becomes a bit murky and I do not know whether it's applicable.&lt;/p&gt;
&lt;p&gt;In any case, the authors compute the path length differences between points in the object space in this system and use the sine and Herschel conditions to map the rays from the object to the image space principal surfaces. (Each condition results in a different mapping.) Under the approximation that the extent of the object is small, the equations for the path length differences demonstrate what was stated in the previous section: that the sine condition leads to spherical aberration for points that do not lie in the focal plane of the objective. In fact, the phase profile of the wave (the authors weave between ray and wave optics) exiting the second principal plane is expanded as:&lt;/p&gt;
&lt;p&gt;$$ znk \left[ 1 - \frac{\rho^2 \sin^2 \alpha}{2} + \frac{\rho^4 \sin^4 \alpha}{8} + \cdots \right] $$&lt;/p&gt;
&lt;p&gt;For \( z = 0 \), i.e. the object is in the focal plane, all the terms disappear and we get a flat exit wave. When \( z \neq 0 \):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Focussing the tube lens is accurately described by the quadratic term, as it operates in the paraxial regime. Unfortunately the higher order terms which represent spherical aberrations cannot be focussed by the tube lens and consequently there is a breakdown of stigmatic imaging for these points.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, &lt;strong&gt;under the sine condition, object points that are outside the focal plane produce curved, non-spherical wavefronts that cannot be focussed to a single point by a tube lens.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If, however, another lens in a reversed orientation was placed so that the curved wavefront from the objective was input into it, it would form a stigmatic image in its image space. This suggests a method for remote focussing.&lt;/p&gt;
&lt;h3&gt;Questions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Is the second principal surface flat because the image is formed at infinity by a high NA, infinity-corrected objective? What would its radius of curvature be in a finite conjugate objective?&lt;/li&gt;
&lt;li&gt;Is the authors' pupil plane coplanar with the objective's exit pupil? Probably not; I think they're referring to the plane in which we find the objective's pupil function, which is somewhat standard (and confusing) nomenclature in microscopy.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;A Technique for Remote Focusing&lt;/h2&gt;
&lt;p&gt;We arrive now at the crux of the paper. The authors suggest a setup for remote focusing that is free (within limits) of the spherical aberration that is introduced by objectives that satisfy the sine condition. Effectively they image the pupil from one objective onto the other with a 4f system. This ensures that the aberrated wavefront from the first objective is "unaberrated" by the second objective. Then, another microscope images the focal region of the second objective. 3D scanning is achieved by moving the objective of the second microscope (often called O3 in light sheet microscopes).&lt;/p&gt;
&lt;p&gt;There are a few important points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A 4f system needs to be used between the first (O1) and second (O2) objectives to relay the pupil because it faithfully maps the wavefront without adding any additional phase distortion.&lt;/li&gt;
&lt;li&gt;On a related note, you can't use tube lenses in the 4f system that are not afocal with the objective. These so-called widefield tube lenses do not share a focal plane with the objective. The objective's pupil must be in the front focal plane of the 4f system.&lt;/li&gt;
&lt;li&gt;The "perfect" imaging system of O1/4f system/O2 will have an isotropic magnification of \( n1 / n2 \). This satisfies Maxwell's requirement for 3D stigmatic imaging.&lt;/li&gt;
&lt;li&gt;This approach will not work well for objectives that require specific tube lenses for aberration correction. (Sorry Zeiss.)&lt;/li&gt;
&lt;li&gt;You will not lose resolution as long as the second objective has a higher &lt;em&gt;angular aperture&lt;/em&gt; (not numerical aperture). You can, for example, use a NA 1.4 oil objective for O1 and a NA 0.95 dry objective for O2 because the O2 object space is in air, whereas the O1 object space is in oil with \( n \approx 1.5 \). From the definition of numerical aperture, the sine of the limiting angle of O1 must necessarily be smaller than the air objective.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point I found it amusing that the authors cited "complexity" as a reason for why their approach is superior to adaptive optics in the introduction of this paper.&lt;/p&gt;
&lt;h3&gt;Questions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The authors suggest a different approach where a mirror is placed after O2 so that it also serves as O3 and use a beam splitter to direct the light leaving O2 onto a camera. Why don't light sheet microscopes use this setup? Is it because of a loss of photons due to the beam splitter?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Range of Operation&lt;/h2&gt;
&lt;p&gt;The equation for the path length difference between points in object space depends on the assumption of small object distances. This assumption places a limit on the range of validity of this approach. To quantify this limit, the authors computed the Strehl ratio of the phase of the wavefront in the pupil. Honestly, the calculations of this section look tedious. In the end, and after "some routine but rather protracted calculations, a simple result emerges." The simple result looks kind of ugly, depending, among other things on the sine to the eigth power of the aperture angle. It looks like the approach is valid for distances of several tens of microns on both sides of the focal plane of O1, which is in fact quite useful for many biological samples.&lt;/p&gt;
&lt;p&gt;Ironically, the authors decide at this point that adaptive optics, the approach to remote focusing that is too complex, probably isn't that bad after all. It can be used to extend the range of validity of the authors' approach by correcting the higher order terms that are dropped in the binomial expansion for the optical path difference.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;The authors go on to experimentally verify the approach in a rather unremarkable experiment of taking z-stacks of beads in two different setups. The PSF in their approach is much less aberrated than a normal widefield microscope over an axial range of about \( \pm 40 \mu m \).&lt;/p&gt;
&lt;p&gt;Overall I quite like the paper because of its simplified theoretical model and clear explantion of the sine condition. I would argue, though, that the approach is not necessarily less complex than some of the alternatives that they rule out in the introduction. Admittedly, arguments over complexity are usually subjective and this doesn't necessarily mean the paper is of low quality. Given that many light sheet approaches are now based on this method, the paper serves as a good theoretical grounding into why remote focusing works and, in some cases, may be necessary.&lt;/p&gt;</description><category>microscopy</category><category>optics</category><guid>https://kylemdouglass.com/posts/literature-review-an-optical-technique-for-remote-focusing-in-microscopy/</guid><pubDate>Thu, 30 May 2024 11:43:39 GMT</pubDate></item></channel></rss>